{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# Load sentiment analysis model and tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define file paths\n",
    "file_path = \"Batch_one/filtered_steemit_2024-03-11.csv\"  # Update with actual file\n",
    "output_path = \"Batch_one/sentiment_results.csv\"\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Removes URLs, non-alphabetic characters, and extra spaces.\"\"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "#Code to name the files and file path here. \n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Get sentiment score and label from the model.\"\"\"\n",
    "    # Truncate text to the maximum sequence length\n",
    "    # The problem was here, passing the whole tokenized output\n",
    "    # Now we only pass the text part\n",
    "    result = sentiment_pipeline(text, truncation=True, max_length=512, return_all_scores=False)[0]\n",
    "    score = int(result['label'][0])  # Extract numeric rating\n",
    "    return score, result['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, output_path, first_chunk=True):\n",
    "    \"\"\"Process a chunk of data and save results.\"\"\"\n",
    "    chunk['processed_text'] = chunk['text'].apply(process_text)\n",
    "    chunk[['sentiment_score', 'sentiment_category']] = chunk['processed_text'].apply(lambda x: pd.Series(analyze_sentiment(x)))\n",
    "\n",
    "    # Save the chunk to the output file\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_path, mode='w', header=True, index=False)\n",
    "    else:\n",
    "        chunk.to_csv(output_path, mode='a', header=False, index=False)\n",
    "\n",
    "    # Release memory and collect garbage\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Read the CSV in chunks and process each chunk\n",
    "chunk_size = 1  # Reducing to 1 rows per chunk for better memory management\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    process_chunk(chunk, output_path, first_chunk)\n",
    "    first_chunk = False  # After first chunk, set to False to append to file\n",
    "\n",
    "print(f\"Sentiment analysis completed. Results saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Set device (0 = GPU, -1 = CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Load the zero-shot classification model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "\n",
    "# Load tokenizer to handle truncation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define candidate topics\n",
    "candidate_labels = [\n",
    "    \"politics\", \"sports\", \"technology\", \"health\", \"entertainment\", \"business\",\n",
    "    \"education\", \"food\", \"travel\", \"climate\", \"relationships\", \"science\",\n",
    "    \"religion\", \"economy\", \"crime\", \"military\", \"fashion\", \"music\", \"movies\", \"personal\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def classify_text(text):\n",
    "    \"\"\"Classifies text into a topic while handling memory issues.\"\"\"\n",
    "    # Tokenize and truncate text to fit within the model limit\n",
    "    inputs = tokenizer(text, truncation=True, max_length=500, return_tensors=\"pt\")\n",
    "\n",
    "    # Convert tokenized text back to string (ensuring it remains truncated)\n",
    "    truncated_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    # Perform classification on truncated text\n",
    "    result = classifier(truncated_text, candidate_labels, multi_label=True)\n",
    "\n",
    "    # Get the most relevant category\n",
    "    best_category = result[\"labels\"][0]\n",
    "\n",
    "    return best_category\n",
    "\n",
    "# Process rows in chunks\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Processes a chunk of data and releases memory.\"\"\"\n",
    "    chunk[\"category\"] = chunk[\"text\"].apply(classify_text)\n",
    "\n",
    "    # Release GPU memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return chunk\n",
    "\n",
    "# File paths\n",
    "input_file = \"/content/sentiment_results.csv\"\n",
    "output_file = \"sentiment_results_with_category.csv\"\n",
    "\n",
    "# Process file in chunks to manage memory\n",
    "chunk_size = 1  # Process one row at a time to prevent memory overflow\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    processed_chunk = process_chunk(chunk)\n",
    "\n",
    "    # Save to CSV\n",
    "    if first_chunk:\n",
    "        processed_chunk.to_csv(output_file, mode=\"w\", index=False)\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        processed_chunk.to_csv(output_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    # Additional memory cleanup\n",
    "    del processed_chunk\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Processing completed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load KeyBERT model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Load Tokenizer to check max token limit\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_tokens = bert_tokenizer.model_max_length  # Get the max token limit\n",
    "\n",
    "# Load sentiment results\n",
    "input_file = \"/content/sentiment_results_with_category.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Filter rows where sentiment score is 5\n",
    "df = df[df['sentiment_score'] == 5]\n",
    "\n",
    "# Initialize TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "vectorizer.fit(df['processed_text'].dropna())  # Fit on non-null texts\n",
    "\n",
    "# Function to extract keywords dynamically\n",
    "def extract_keywords(text, category):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Truncate text to max token length\n",
    "    tokenized_text = bert_tokenizer.tokenize(text)\n",
    "    truncated_text = bert_tokenizer.convert_tokens_to_string(tokenized_text[:max_tokens])\n",
    "\n",
    "    # Use KeyBERT with category as context\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        truncated_text,\n",
    "        keyphrase_ngram_range=(1, 3),\n",
    "        stop_words='english',\n",
    "        top_n=3,\n",
    "        use_mmr=True,\n",
    "        diversity=0.7\n",
    "    )\n",
    "\n",
    "    # Extract top keywords\n",
    "    extracted_keywords = [kw[0] for kw in keywords]\n",
    "    return \", \".join(extracted_keywords)\n",
    "\n",
    "# Apply keyword extraction\n",
    "df['keywords'] = df.apply(lambda row: extract_keywords(row['processed_text'], row['category']), axis=1)\n",
    "\n",
    "# Select relevant columns for clustering\n",
    "df_output = df[['timestamp', 'keywords', 'category']]\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"topics_for_clustering.csv\"\n",
    "df_output.to_csv(output_file, index=False)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Keyword extraction completed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/topics_for_clustering.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "keywords = df['keywords'].astype(str)\n",
    "categories = df['category']\n",
    "\n",
    "# Convert keywords to numerical features\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(keywords)\n",
    "\n",
    "# Use sentence embeddings for better clustering\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_embeddings = model.encode(keywords, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Reduce dimensionality for clustering\n",
    "X_reduced = umap.UMAP(n_components=10, random_state=42).fit_transform(X_embeddings)\n",
    "\n",
    "# Optimize HDBSCAN parameters\n",
    "def find_best_cluster_size(X, min_size_range=[3, 10, 20, 50]):\n",
    "    best_score = -1\n",
    "    best_size = None\n",
    "    best_labels = None\n",
    "\n",
    "    for min_size in min_size_range:\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_size, min_samples=3, metric='euclidean', cluster_selection_method='eom')\n",
    "        labels = clusterer.fit_predict(X)\n",
    "\n",
    "        if len(set(labels)) > 1:  # Ensure we have multiple clusters\n",
    "            score = silhouette_score(X, labels)\n",
    "            if score > best_score and score >= 0.65:\n",
    "                best_score = score\n",
    "                best_size = min_size\n",
    "                best_labels = labels\n",
    "\n",
    "    return best_size, best_labels\n",
    "\n",
    "# Find optimal min_cluster_size\n",
    "best_size, best_labels = find_best_cluster_size(X_reduced)\n",
    "print(f\"Best min_cluster_size: {best_size}\")\n",
    "\n",
    "df['cluster'] = best_labels\n",
    "\n",
    "# Save clustered results\n",
    "df.to_csv(\"clustered_topics.csv\", index=False)\n",
    "\n",
    "# Visualize Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=df['cluster'], palette='viridis', alpha=0.7)\n",
    "plt.title(\"HDBSCAN Clustering of Topics\")\n",
    "plt.show()\n",
    "\n",
    "gc.collect()\n",
    "print(\"Clustering completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load clustered data\n",
    "file_path = \"/content/clustered_topics.csv\"  # Update this if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Clean the keywords to ensure they are readable English words\n",
    "def clean_keywords(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', str(text))  # Keep only English words\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['cleaned_keywords'] = df['keywords'].apply(clean_keywords)\n",
    "\n",
    "# Step 2: Identify representative words/phrases per cluster\n",
    "def get_top_keywords(keywords, top_n=5):\n",
    "    words = \" \".join(keywords).split()\n",
    "    word_counts = Counter(words)\n",
    "    return \" \".join([word for word, _ in word_counts.most_common(top_n)])\n",
    "\n",
    "cluster_topics = df.groupby(\"cluster\")[\"cleaned_keywords\"].apply(lambda x: get_top_keywords(x, top_n=5)).reset_index()\n",
    "cluster_topics.columns = [\"cluster\", \"representative_keywords\"]\n",
    "\n",
    "# Step 3: Merge with categories for analysis\n",
    "final_df = df.merge(cluster_topics, on=\"cluster\", how=\"left\")\n",
    "\n",
    "# Save results\n",
    "output_path = \"final_topics.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Representative topics extracted and saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
