{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a66caf8-22cc-422a-a322-ba78c32fa1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process: 263\n",
      "Processing chunk 1 of 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 50/50 [14:47<00:00, 17.74s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2 of 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 50/50 [13:28<00:00, 16.17s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3 of 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 50/50 [15:17<00:00, 18.35s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4 of 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 50/50 [18:10<00:00, 21.82s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 5 of 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 50/50 [16:58<00:00, 20.38s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 6 of 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [06:38<00:00, 30.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Word Frequencies:\n",
      "         Word  Frequency\n",
      "0     doubler      23435\n",
      "1        jpeg      22543\n",
      "2  upvotebank      21099\n",
      "3    rewarded      21077\n",
      "4  aggregator      20987\n",
      "5     minimum      19927\n",
      "6        roll      19755\n",
      "7        paid      19030\n",
      "8   excellent      18926\n",
      "9       tried      18890\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Define input and output folders\n",
    "batch_one_folder = \"Batch_one\"\n",
    "output_file = \"word_frequencies.json\"\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# POS tags to consider\n",
    "VALID_POS_TAGS = {\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"}\n",
    "\n",
    "# Thresholds\n",
    "magic_threshold_min = 5\n",
    "magic_threshold_max = 100\n",
    "\n",
    "# Chunking parameters\n",
    "num_files_per_chunk = 50  # Process 50 files at a time\n",
    "\n",
    "\n",
    "def extract_valid_words(text):\n",
    "    \"\"\"Extract words from text using spaCy, keeping only certain POS tags.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc if token.pos_ in VALID_POS_TAGS and token.is_alpha]\n",
    "\n",
    "def compute_frequencies(file):\n",
    "    \"\"\"Compute word frequencies from a single file.\"\"\"\n",
    "    filepath = os.path.join(batch_one_folder, file)\n",
    "    local_frequencies = {}\n",
    "    try:\n",
    "        for chunk in pd.read_csv(filepath, on_bad_lines='skip', engine=\"c\", usecols=[\"concatenated_text\"], chunksize=100000):\n",
    "            for text in chunk[\"concatenated_text\"].dropna():\n",
    "                for word in extract_valid_words(text):\n",
    "                    local_frequencies[word] = local_frequencies.get(word, 0) + 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "    return local_frequencies\n",
    "\n",
    "def process_files_in_chunks(files):\n",
    "    \"\"\"Process files in chunks to optimize performance.\"\"\"\n",
    "    f_total = {}\n",
    "    for i in range(0, len(files), num_files_per_chunk):\n",
    "        chunk_files = files[i:i + num_files_per_chunk]\n",
    "        print(f\"Processing chunk {i // num_files_per_chunk + 1} of {len(files) // num_files_per_chunk + 1}...\")\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=32) as executor:\n",
    "            results = list(tqdm(executor.map(compute_frequencies, chunk_files), total=len(chunk_files), desc=\"Processing\"))\n",
    "        \n",
    "        for local_freq in results:\n",
    "            for word, count in local_freq.items():\n",
    "                if magic_threshold_min <= count <= magic_threshold_max:\n",
    "                    f_total[word] = f_total.get(word, 0) + count\n",
    "    \n",
    "    return f_total\n",
    "\n",
    "# Get list of files\n",
    "filtered_files = [f for f in os.listdir(batch_one_folder) if f.endswith(\".csv\")]\n",
    "print(f\"Total files to process: {len(filtered_files)}\")\n",
    "\n",
    "# Process files in chunks\n",
    "word_frequencies = process_files_in_chunks(filtered_files)\n",
    "\n",
    "# Save word frequencies to a JSON file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_frequencies, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Display a sample of the most frequent words\n",
    "sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "sample_df = pd.DataFrame(sorted_words, columns=[\"Word\", \"Frequency\"])\n",
    "print(\"\\nSample Word Frequencies:\")\n",
    "print(sample_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03b03c-745e-4db6-a3ab-8180368342cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
