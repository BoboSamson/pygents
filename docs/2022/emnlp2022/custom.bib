@inproceedings{1,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L{ukasz} and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{2,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{3,
  author    = {Linas Vepstas and
               Ben Goertzel},
  title     = {Learning Language from a Large (Unannotated) Corpus},
  journal   = {Computing Research Repository},
  volume    = {arXiv:1401.3372},
  year      = {2014},
  url       = {http://arxiv.org/abs/1401.3372},
  eprinttype = {arXiv},
  eprint    = {1401.3372},
  timestamp = {Fri, 27 Dec 2019 21:15:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VepstasG14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{4,  author={Kolonin, Anton},  booktitle={2015 International Conference on Biomedical Engineering and Computational Technologies (SIBIRCON)},   title={Automatic text classification and property extraction applications in medicine},   year={2015},  volume={},  number={},  pages={133-137},  doi={10.1109/SIBIRCON.2015.7361868}}

@InProceedings{5,
author="Glushchenko, Alex
and Suarez, Andres
and Kolonin, Anton
and Goertzel, Ben
and Castillo, Claudia
and Leung, Man Hin
and Baskov, Oleg",
editor="Ikl{\'e}, Matthew
and Franz, Arthur
and Rzepka, Rafal
and Goertzel, Ben",
title="Unsupervised Language Learning in {OpenCog}",
booktitle="Artificial General Intelligence",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="109--118",
abstract="We discuss technology capable to learn language without supervision. While the entire goal may be too ambitious and not achievable to full extent, we explore how far we can advance grammar learning. We present the current approach employed in the open source OpenCog Artificial Intelligence Platform, describe the cognitive pipeline being constructed and present some intermediate results.",
isbn="978-3-319-97676-1"
}

@InProceedings{6,
author="Glushchenko, Alex
and Suarez, Andres
and Kolonin, Anton
and Goertzel, Ben
and Baskov, Oleg",
editor="Hammer, Patrick
and Agrawal, Pulin
and Goertzel, Ben
and Ikl{\'e}, Matthew",
title="Programmatic {Link} {Grammar} Induction for Unsupervised Language Learning",
booktitle="Artificial General Intelligence",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="111--120",
abstract="Although natural (i.e. human) languages do not seem to follow a strictly formal grammar, their structure analysis and generation can be approximated by one. Having such a grammar is an important tool for programmatic language understanding. Due to the huge number of natural languages and their variations, processing tools that rely on human intervention are available only for the most popular ones. We explore the problem of unsupervisedly inducing a formal grammar for any language, using the {Link} {Grammar} paradigm, from unannotated parses also obtained without supervision from an input corpus. The details of our state-of-the-art grammar induction technology and its evaluation techniques are described, as well as preliminary results of its application on both synthetic and real world text-corpora.",
isbn="978-3-030-27005-6"
}

@article{7,
  author    = {Deniz Yuret},
  title     = {Discovery of Linguistic Relations Using Lexical Attraction},
  journal   = {Computing Research Repository},
  volume    = {arXiv:cmp-lg/9805009},
  year      = {1998},
  url       = {http://arxiv.org/abs/cmp-lg/9805009},
  timestamp = {Mon, 13 Aug 2018 16:47:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/cmp-lg-9805009.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{8,
  author={Ramesh, Vignav and Kolonin, Anton},
  booktitle={2020 Science and Artificial Intelligence conference (S.A.I.ence)}, 
  title={Interpretable Natural Language Segmentation Based on {Link} {Grammar}}, 
  year={2020},
  volume={},
  number={},
  pages={25-32},
  doi={10.1109/S.A.I.ence50533.2020.9303220}}
  
@article{9,
  author    = {Vignav Ramesh and
               Anton Kolonin},
  title     = {Natural Language Generation Using {Link} {Grammar} for General Conversational
               Intelligence},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2105.00830},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.00830},
  eprinttype = {arXiv},
  eprint    = {2105.00830},
  timestamp = {Wed, 12 May 2021 15:54:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-00830.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10,
author="Ramesh, Vignav
and Kolonin, Anton",
editor="Goertzel, Ben
and Ikl{\'e}, Matthew
and Potapov, Alexey",
title="Unsupervised Context-Driven Question Answering Based on {Link} {Grammar}",
booktitle="Artificial General Intelligence",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="210--220",
abstract="While general conversational intelligence (GCI) can be considered one of the core aspects of artificial general intelligence (AGI), there currently exists minimal overlap between the disciplines of AGI and natural language processing (NLP). Only a few AGI architectures can comprehend and generate natural language, and most NLP systems rely either on hardcoded, specialized rules and frameworks that cannot generalize to the various complex domains of human language or on heavily trained deep neural network models that cannot be interpreted, controlled, or made sense of. In this paper, we propose an interpretable ``Contextual Generator'' architecture for question answering (QA), built as an extension of the recently published ``Generator'' algorithm for sentence generation, that produces grammatically valid answers to queries structured as lists of seed words. We demonstrate the potential for this architecture to perform automated, closed-domain QA by detailing results on queries from SingularityNET's ``small world'' POC-English corpus and from the Stanford Question Answering Dataset. Overall, our work may bring a greater degree of GCI to proto-AGI NLP pipelines. The proposed QA architecture is open-source and can be found on GitHub under the MIT License at https://github.com/aigents/aigents-java-nlp.",
isbn="978-3-030-93758-4"
}

@inproceedings{11,
  title={A Hybrid Approach to Cross-Linguistic Tokenization: Morphology with Statistics},
  author={Logan Kearsley},
  year={2016}
}

@InProceedings{12,
  title={An Unsupervised Machine Learning Approach to Segmentation of Clinician-Entered Free Text},
  author={Jesse O. Wrenn and Peter D. Stetson and Stephen B. Johnson},
  booktitle={Proceedings of the AMIA Annual Symposium},
  year={2007},
  pages={
          811-5
        }
}

@Article{13,
author={Friston, Karl},
title={The free-energy principle: a unified brain theory?},
journal={Nature Reviews Neuroscience},
year={2010},
month={Feb},
day={01},
volume={11},
number={2},
pages={127-138},
abstract={Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
issn={1471-0048},
doi={10.1038/nrn2787},
url={https://doi.org/10.1038/nrn2787}
}

@InProceedings{14,
author="Kolonin, Anton",
editor="Goertzel, Ben
and Ikl{\'e}, Matthew
and Potapov, Alexey",
title="Neuro-Symbolic Architecture for Experiential Learning in Discrete and Functional Environments",
booktitle="Artificial General Intelligence",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="106--115",
abstract="The paper presents a ``horizontal neuro-symbolic integration'' approach for artificial general intelligence along with elementary representation-agnostic cognitive architecture and explores its usability under the experiential learning framework for reinforcement learning problem powered by ``global feedback''.",
isbn="978-3-030-93758-4"
}

@article{15,
  doi = {10.48550/ARXIV.2203.13573},
  
  url = {https://arxiv.org/abs/2203.13573},
  
  author = {Gopalakrishnan, Anand and Irie, Kazuki and Schmidhuber, Jürgen and van Steenkiste, Sjoerd},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Learning of Temporal Abstractions with Slot-based Transformers},
  
  journal   = {Computing Research Repository},
  volume    = {arXiv:2203.13573},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@Article{16,
author={Sun, Ching Chu
and Hendrix, Peter
and Ma, Jianqiang
and Baayen, Rolf Harald},
title={Chinese {Lexical} {Database} ({CLD})},
journal={Behavior Research Methods},
year={2018},
month={Dec},
day={01},
volume={50},
number={6},
pages={2606-2629},
abstract={We present the Chinese Lexical Database (CLD): a large-scale lexical database for simplified Chinese. The CLD provides a wealth of lexical information for 3913 one-character words, 34,233 two-character words, 7143 three-character words, and 3355 four-character words, and is publicly available through http://www.chineselexicaldatabase.com. For each of the 48,644 words in the CLD, we provide a wide range of categorical predictors, as well as an extensive set of frequency measures, complexity measures, neighborhood density measures, orthography-phonology consistency measures, and information-theoretic measures. We evaluate the explanatory power of the lexical variables in the CLD in the context of experimental data through analyses of lexical decision latencies for one-character, two-character, three-character and four-character words, as well as word naming latencies for one-character and two-character words. The results of these analyses are discussed.},
issn={1554-3528},
doi={10.3758/s13428-018-1038-3},
url={https://doi.org/10.3758/s13428-018-1038-3}
}

@article{17,
  doi = {10.48550/ARXIV.2202.12710},
  
  url = {https://arxiv.org/abs/2202.12710},
  
  author = {Vityaev, Evgenii and Kolonin, Anton and Kurpatov, Andrey and Molchanov, Artem},
  
  keywords = {Neurons and Cognition (q-bio.NC), Artificial Intelligence (cs.AI), FOS: Biological sciences, FOS: Biological sciences, FOS: Computer and information sciences, FOS: Computer and information sciences, 68T01},
  
  title = {Brain Principles Programming},
  
  journal   = {Computing Research Repository},
  volume    = {arXiv:2202.12710},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{18,
  doi = {10.48550/ARXIV.1809.08390},
  
  url = {https://arxiv.org/abs/1809.08390},
  
  author = {Jiang, Junfeng and Li, Jiahao},
  
  keywords = {Computational Finance (q-fin.CP), Computation and Language (cs.CL), FOS: Economics and business, FOS: Economics and business, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Constructing Financial Sentimental Factors in {Chinese} Market Using Natural Language Processing},
  
  journal   = {Computing Research Repository},
  volume    = {arXiv:1809.08390},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{19,
  author    = {Jesse Dodge and
               Suchin Gururangan and
               Dallas Card and
               Roy Schwartz and
               Noah A. Smith},
  title     = {Show Your Work: Improved Reporting of Experimental Results},
  journal   = {Computing Research Repository},
  volume    = {arXiv:1909.03004},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.03004},
  eprinttype = {arXiv},
  eprint    = {1909.03004},
  timestamp = {Wed, 23 Dec 2020 10:13:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-03004.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{20,
  title={Statistical parsing and unambiguous word representation in {OpenCog}’s Unsupervised Language Learning project},
  author={Claudia Castillo-Domenech and
Andres Suarez-Madrigal},
  year={2018},
  url       = {https://hdl.handle.net/20.500.12380/256408}
}