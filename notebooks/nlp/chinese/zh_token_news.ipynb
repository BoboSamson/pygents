{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b97af6",
   "metadata": {},
   "source": [
    "# Chinese (Simplified) Tokenization Model - Experiments - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfa4a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from importlib import reload  # Python 3.4+\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * \n",
    "\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a886e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.vengaglobal.com/blog/simplified-traditional-chinese-mandarin-cantonese/\n",
    "\n",
    "# Target Market  Written      Spoken\n",
    "# -------------------------------------\n",
    "# China          Simplified   Mandarin\n",
    "# Singapore      Simplified   Mandarin\n",
    "# Taiwan         Traditional  Mandarin\n",
    "# Hong Kong      Traditional  Cantonese\n",
    "\n",
    "# Lexicon:\n",
    "# http://www.chineselexicaldatabase.com/download.php - used below\n",
    "# Sun, C. C., Hendrix, P., Ma, J.Q. & Baayen, R. H. (2018). Chinese Lexical Database (CLD): A large-scale lexical database for simplified Mandarin Chinese. Behavior Research Methods, https://doi.org/10.3758/s13428-018-1038-3.\n",
    "\n",
    "# Corpora:\n",
    "# https://www.openslr.org/38/ - test-audio corpus, not relevant\n",
    "# https://github.com/CLUEbenchmark/CLUECorpus2020/ - email request sent\n",
    "# https://github.com/brightmart/nlp_chinese_corpus - nearly same as above downloaded, used further\n",
    "\n",
    "# TODO:\n",
    "# https://metatext.io/datasets/nlp-chinese-corpus - paper with word segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0942142",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../nlp/corpora/Chinese/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523eaa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_clue_json2text(path,filename):\n",
    "    with open(os.path.join(path,filename+'.json')) as file:\n",
    "        with open(os.path.join(path,filename+'.txt'), 'w') as fout:\n",
    "            while True:\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                j = json.loads(line)\n",
    "                #print('title',j['title'])\n",
    "                #print('desc',j['desc'])\n",
    "                #print('content',j['content'])\n",
    "                fout.write(j['title'])\n",
    "                fout.write('\\n')    \n",
    "                fout.write(j['desc'])\n",
    "                fout.write('\\n')    \n",
    "                fout.write(j['content'])\n",
    "                fout.write('\\n')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd8f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this once!\n",
    "#zh_clue_json2text(path,'clue/new2016zh/news2016zh_valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a4778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this once!\n",
    "#zh_clue_json2text(path,'clue/new2016zh/news2016zh_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989ca52",
   "metadata": {},
   "source": [
    "## Load and explore full models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22921a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lb/1m7gbdp17h578qq48pbbtxf40000gn/T/ipykernel_91711/3614049384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#zh_valid_grams = FreedomTokenizer(max_n=3,mode='grams',debug=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clue/new2016zh/news2016zh_valid.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "zh_valid_chars = FreedomTokenizer(max_n=3,mode='chars',debug=False)\n",
    "#zh_valid_grams = FreedomTokenizer(max_n=3,mode='grams',debug=False)\n",
    "\n",
    "with open(join(path, 'clue/new2016zh/news2016zh_valid.txt'),errors='ignore') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        cnt += 1\n",
    "        if (cnt % 1000) == 0:\n",
    "            print(cnt)\n",
    "        zh_valid_chars.train([line])\n",
    "        #zh_valid_grams.train([line])\n",
    "\n",
    "zh_valid_chars.store('data/models/zh_valid_chars_3a')\n",
    "#zh_valid_grams.store('data/models/zh_valid_grams_3a')\n",
    "\n",
    "print(zh_valid_chars.count_params())\n",
    "# 143,129,564 (max_n=3)\n",
    "\n",
    "#print(zh_valid_grams.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e58af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zh_valid_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247c44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bce681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "249859247\n"
     ]
    }
   ],
   "source": [
    "zh_train_chars = FreedomTokenizer(max_n=2,mode='chars',debug=False)\n",
    "\n",
    "with open(join(path, 'clue/new2016zh/news2016zh_train.txt'),errors='ignore') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        cnt += 1\n",
    "        if (cnt % 100000) == 0:\n",
    "            print(cnt)\n",
    "        zh_train_chars.train([line])\n",
    "        #zh_valid_grams.train([line])\n",
    "\n",
    "zh_train_chars.store('data/models/zh_train_chars_2a')\n",
    "\n",
    "print(zh_train_chars.count_params())\n",
    "# 249,859,247 (max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafff1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e01371f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akolonin/Documents/aigents/pygents/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (3,4,8,9,12,13,17,18,22,23,28,29,111,112,127,128) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "zh_lexicon_tokenizer = LexiconIndexedTokenizer(lexicon = list(pd.read_csv(os.path.join(path,'lexicon/chineselexicaldatabase2.1.txt'))['Word']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3ae11cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['苹果树', '和', '梨树', '开花', '，', '雾气', '飘过', '河面', '。']\n",
      "0.84 ['苹果树', '和', '梨树', '开花', '，', '雾气', '飘', '过', '河面', '。']\n",
      "\n",
      "[1] 0.02 0.5 ['苹果', '树', '和', '梨树', '开', '花', '，', '雾气飘', '过', '河面', '。']\n",
      "[1] 0.05 0.82 ['苹果树', '和', '梨树', '开花', '，', '雾气飘过', '河面', '。']\n",
      "[1] 0.1 0.63 ['苹果树', '和', '梨树开花', '，', '雾气飘过', '河面', '。']\n",
      "[1] 0.2 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1] 0.3 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1] 0.4 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1] 0.5 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.02 0.5 ['苹果', '树', '和', '梨树', '开', '花', '，', '雾气飘', '过', '河面', '。']\n",
      "[1, 2] 0.05 0.82 ['苹果树', '和', '梨树', '开花', '，', '雾气飘过', '河面', '。']\n",
      "[1, 2] 0.1 0.63 ['苹果树', '和', '梨树开花', '，', '雾气飘过', '河面', '。']\n",
      "[1, 2] 0.2 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.3 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.4 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.5 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.02 0.5 ['苹果', '树', '和', '梨树', '开', '花', '，', '雾气飘', '过', '河面', '。']\n",
      "[2] 0.05 0.82 ['苹果树', '和', '梨树', '开花', '，', '雾气飘过', '河面', '。']\n",
      "[2] 0.1 0.63 ['苹果树', '和', '梨树开花', '，', '雾气飘过', '河面', '。']\n",
      "[2] 0.2 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.3 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.4 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.5 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n"
     ]
    }
   ],
   "source": [
    "#'Цвели яблони и груши, над рекой плыл туман.'\n",
    "text = '苹果树和梨树开花，雾气飘过河面。'\n",
    "                               \n",
    "expected = [r[0] for r in jieba.tokenize(text)]\n",
    "print(expected)\n",
    "\n",
    "tokens = zh_lexicon_tokenizer.tokenize(text)\n",
    "print(round(calc_f1(expected,tokens),2),tokens)\n",
    "    \n",
    "print()\n",
    "for nlist in [[1],[1,2],[2]]:\n",
    "    for threshold in [0.02,0.05,0.1,0.2,0.3,0.4,0.5]: \n",
    "        tokens = tokenize_with_opposite_metrics(zh_train_chars.model,text,'ddf-','ddf+',[1],threshold=threshold)\n",
    "        print(nlist,threshold,round(calc_f1(expected,tokens),2),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d130b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://magichub.com/datasets/chinese-english-parallel-corpus-finance/\n",
    "test_df = pd.read_csv(os.path.join(path,'magicdata/zh_en_ru_100/CORPUS_ZH_EN_RU.txt'),delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20ced8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "然后医疗保险呢？就是我们家，不论是大人啊还是小孩都会去买一个保险\n",
      "0.84 ['然后', '医疗', '保险', '呢', '？', '就是', '我们', '家', '，', '不论是', '大人', '啊', '还是', '小孩', '都会', '去', '买', '一个', '保险']\n",
      "当他们买了保险的，按照保险合同的话，是要赔三十万的\n",
      "0.8 ['当', '他们', '买', '了', '保险', '的', '，', '按照', '保险', '合同', '的话', '，', '是', '要', '赔', '三', '十', '万', '的']\n",
      "需要意识到买了一个保险的重要性\n",
      "1.0 ['需要', '意识', '到', '买', '了', '一个', '保险', '的', '重要性']\n",
      "其实这种现象是真的很普遍，因为比如说你买一个人身意外险你那个你买的越多你那个保额就越多\n",
      "0.85 ['其实', '这', '种', '现象', '是', '真的', '很', '普遍', '，', '因为', '比如说', '你', '买', '一个', '人身', '意外', '险', '你', '那个', '你', '买', '的', '越', '多', '你', '那个', '保', '额', '就', '越', '多']\n",
      "这代父母真的很有必要去买一个保险\n",
      "0.86 ['这', '代', '父母', '真的', '很', '有', '必要', '去', '买', '一个', '保险']\n",
      "0.87\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tokenizer(texts,tokenizer):\n",
    "    avg_f1 = 0\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        print(text)\n",
    "        expected = [r[0] for r in jieba.tokenize(text)]\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        f1 = calc_f1(expected,tokens)\n",
    "        print(round(f1,2),tokens)\n",
    "        avg_f1 += f1\n",
    "        count += 1\n",
    "    print(round(avg_f1/count,2))\n",
    "\n",
    "evaluate_tokenizer(list(test_df[:5]['zh']),zh_lexicon_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ffd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e741e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "然后医疗保险呢？就是我们家，不论是大人啊还是小孩都会去买一个保险\n",
      "['然后', '医疗保险', '呢', '？', '就是', '我们', '家', '，', '不论是', '大人', '啊', '还是', '小孩', '都', '会', '去', '买', '一个', '保险']\n",
      "['然后医疗保险呢？就是我们家', '，', '不论是大人啊还是小孩都会去买一个保险']\n",
      "0.09\n",
      "当他们买了保险的，按照保险合同的话，是要赔三十万的\n",
      "['当', '他们', '买', '了', '保险', '的', '，', '按照', '保险合同', '的话', '，', '是', '要', '赔', '三十万', '的']\n",
      "['当他们买了保险', '的', '，', '按照保险合同', '的', '话', '，', '是要赔三十万', '的']\n",
      "0.32\n",
      "需要意识到买了一个保险的重要性\n",
      "['需要', '意识', '到', '买', '了', '一个', '保险', '的', '重要性']\n",
      "['需要意识到买了', '一个保险', '的', '重要性']\n",
      "0.31\n",
      "其实这种现象是真的很普遍，因为比如说你买一个人身意外险你那个你买的越多你那个保额就越多\n",
      "['其实', '这种', '现象', '是', '真的', '很', '普遍', '，', '因为', '比如说', '你', '买', '一个', '人身', '意外险', '你', '那个', '你', '买', '的', '越', '多', '你', '那个', '保额', '就', '越', '多']\n",
      "['其实这种现象是真', '的', '很普遍', '，', '因为比如说你买一个人身意外险你那个你买', '的', '越多你那个保额就越多']\n",
      "0.11\n",
      "这代父母真的很有必要去买一个保险\n",
      "['这代', '父母', '真的', '很', '有', '必要', '去', '买', '一个', '保险']\n",
      "['这代父母真', '的', '很有', '必要去买', '一个保险']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i,row in test_df[:5].iterrows():\n",
    "    text = row['zh']\n",
    "    print(text)\n",
    "    expected = [r[0] for r in jieba.tokenize(text)]\n",
    "    tokens = zh_lexicon_tokenizer.tokenize(text)\n",
    "    tokens = tokenize_with_opposite_metrics(zh_train_chars.model,text,'ddf-','ddf+',[1],threshold=0.5)\n",
    "    print(expected)\n",
    "    print(tokens)\n",
    "    print(round(calc_f1(expected,tokens),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acdadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58049430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6dc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del zh_train_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e81bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ecb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305114a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ab5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
