{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0546061d",
   "metadata": {},
   "source": [
    "# Chinese (Simplified) Tokenization Model - Experiments - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfa4a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from importlib import reload  # Python 3.4+\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * \n",
    "\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a886e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.vengaglobal.com/blog/simplified-traditional-chinese-mandarin-cantonese/\n",
    "\n",
    "# Target Market  Written      Spoken\n",
    "# -------------------------------------\n",
    "# China          Simplified   Mandarin\n",
    "# Singapore      Simplified   Mandarin\n",
    "# Taiwan         Traditional  Mandarin\n",
    "# Hong Kong      Traditional  Cantonese\n",
    "\n",
    "# Lexicon:\n",
    "# http://www.chineselexicaldatabase.com/download.php - used below\n",
    "# Sun, C. C., Hendrix, P., Ma, J.Q. & Baayen, R. H. (2018). Chinese Lexical Database (CLD): A large-scale lexical database for simplified Mandarin Chinese. Behavior Research Methods, https://doi.org/10.3758/s13428-018-1038-3.\n",
    "\n",
    "# Corpora:\n",
    "# https://www.openslr.org/38/ - test-audio corpus, not relevant\n",
    "# https://github.com/CLUEbenchmark/CLUECorpus2020/ - email request sent\n",
    "# https://github.com/brightmart/nlp_chinese_corpus - nearly same as above downloaded, used further\n",
    "\n",
    "# TODO:\n",
    "# https://metatext.io/datasets/nlp-chinese-corpus - paper with word segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0942142",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../nlp/corpora/Chinese/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523eaa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_clue_json2text(path,filename):\n",
    "    with open(os.path.join(path,filename+'.json')) as file:\n",
    "        with open(os.path.join(path,filename+'.txt'), 'w') as fout:\n",
    "            while True:\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                j = json.loads(line)\n",
    "                #print('title',j['title'])\n",
    "                #print('desc',j['desc'])\n",
    "                #print('content',j['content'])\n",
    "                fout.write(j['title'])\n",
    "                fout.write('\\n')    \n",
    "                fout.write(j['desc'])\n",
    "                fout.write('\\n')    \n",
    "                fout.write(j['content'])\n",
    "                fout.write('\\n')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd8f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this once!\n",
    "#zh_clue_json2text(path,'clue/new2016zh/news2016zh_valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a4778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this once!\n",
    "#zh_clue_json2text(path,'clue/new2016zh/news2016zh_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989ca52",
   "metadata": {},
   "source": [
    "## Load and explore full models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22921a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lb/1m7gbdp17h578qq48pbbtxf40000gn/T/ipykernel_91711/3614049384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#zh_valid_grams = FreedomTokenizer(max_n=3,mode='grams',debug=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clue/new2016zh/news2016zh_valid.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "zh_valid_chars = FreedomTokenizer(max_n=3,mode='chars',debug=False)\n",
    "#zh_valid_grams = FreedomTokenizer(max_n=3,mode='grams',debug=False)\n",
    "\n",
    "with open(join(path, 'clue/new2016zh/news2016zh_valid.txt'),errors='ignore') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        cnt += 1\n",
    "        if (cnt % 1000) == 0:\n",
    "            print(cnt)\n",
    "        zh_valid_chars.train([line])\n",
    "        #zh_valid_grams.train([line])\n",
    "\n",
    "zh_valid_chars.store('data/models/zh_valid_chars_3a')\n",
    "#zh_valid_grams.store('data/models/zh_valid_grams_3a')\n",
    "\n",
    "print(zh_valid_chars.count_params())\n",
    "# 143,129,564 (max_n=3)\n",
    "\n",
    "#print(zh_valid_grams.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e58af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zh_valid_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9118f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bce681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "249859247\n"
     ]
    }
   ],
   "source": [
    "zh_train_chars = FreedomTokenizer(max_n=2,mode='chars',debug=False)\n",
    "\n",
    "with open(join(path, 'clue/new2016zh/news2016zh_train.txt'),errors='ignore') as f:\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        cnt += 1\n",
    "        if (cnt % 100000) == 0:\n",
    "            print(cnt)\n",
    "        zh_train_chars.train([line])\n",
    "        #zh_valid_grams.train([line])\n",
    "\n",
    "zh_train_chars.store('data/models/zh_train_chars_2a')\n",
    "\n",
    "print(zh_train_chars.count_params())\n",
    "# 249,859,247 (max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a8c2cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akolonin/Documents/aigents/pygents/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (3,4,8,9,12,13,17,18,22,23,28,29,111,112,127,128) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "zh_lexicon_tokenizer = LexiconIndexedTokenizer(lexicon = list(pd.read_csv(os.path.join(path,'lexicon/chineselexicaldatabase2.1.txt'))['Word']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e060b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO move out\n",
    "\n",
    "class JebaTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        Tokenizer.__init__(self,debug=False)\n",
    "    def tokenize(self,text):\n",
    "        return [r[0] for r in jieba.tokenize(text)]\n",
    "\n",
    "def evaluate_tokenizer(texts,real_tokenizer,test_tokenizer,debug=False):\n",
    "    avg_f1 = 0\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        expected = real_tokenizer.tokenize(text)\n",
    "        tokens = test_tokenizer.tokenize(text)\n",
    "        f1 = calc_f1(expected,tokens)\n",
    "        if debug:\n",
    "            print(text)\n",
    "            print(round(f1,2),tokens)\n",
    "        avg_f1 += f1\n",
    "        count += 1\n",
    "    return round(avg_f1/count,2)\n",
    "\n",
    "\n",
    "class FreedomBasedTokenizer(FreedomTokenizer):\n",
    "\n",
    "    def __init__(self, base, back, forw, nlist, threshold=0.5, debug=False):\n",
    "        FreedomTokenizer.__init__(self,debug=debug)\n",
    "        self.model = base.model\n",
    "        self.mode = base.mode\n",
    "        self.back = back\n",
    "        self.forw = forw\n",
    "        self.nlist = nlist\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        return tokenize_with_opposite_metrics(self.model,text,self.back,self.forw,self.nlist,self.threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ea0e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['苹果树', '和', '梨树', '开花', '，', '雾气', '飘过', '河面', '。']\n",
      "0.84 ['苹果树', '和', '梨树', '开花', '，', '雾气', '飘', '过', '河面', '。']\n",
      "\n",
      "[1] 0.02 0.5 ['苹果', '树', '和', '梨树', '开', '花', '，', '雾气飘', '过', '河面', '。']\n",
      "[1] 0.05 0.82 ['苹果树', '和', '梨树', '开花', '，', '雾气飘过', '河面', '。']\n",
      "[1] 0.1 0.63 ['苹果树', '和', '梨树开花', '，', '雾气飘过', '河面', '。']\n",
      "[1] 0.2 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1] 0.3 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1] 0.4 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1] 0.5 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.02 0.5 ['苹果', '树', '和', '梨树', '开', '花', '，', '雾气飘', '过', '河面', '。']\n",
      "[1, 2] 0.05 0.82 ['苹果树', '和', '梨树', '开花', '，', '雾气飘过', '河面', '。']\n",
      "[1, 2] 0.1 0.63 ['苹果树', '和', '梨树开花', '，', '雾气飘过', '河面', '。']\n",
      "[1, 2] 0.2 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.3 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.4 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[1, 2] 0.5 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.02 0.5 ['苹果', '树', '和', '梨树', '开', '花', '，', '雾气飘', '过', '河面', '。']\n",
      "[2] 0.05 0.82 ['苹果树', '和', '梨树', '开花', '，', '雾气飘过', '河面', '。']\n",
      "[2] 0.1 0.63 ['苹果树', '和', '梨树开花', '，', '雾气飘过', '河面', '。']\n",
      "[2] 0.2 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.3 0.53 ['苹果树', '和', '梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.4 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n",
      "[2] 0.5 0.31 ['苹果树和梨树开花', '，', '雾气飘过河面', '。']\n"
     ]
    }
   ],
   "source": [
    "#'Цвели яблони и груши, над рекой плыл туман.'\n",
    "text = '苹果树和梨树开花，雾气飘过河面。'\n",
    "                               \n",
    "expected = JebaTokenizer().tokenize(text) #[r[0] for r in jieba.tokenize(text)]\n",
    "print(expected)\n",
    "\n",
    "tokens = zh_lexicon_tokenizer.tokenize(text)\n",
    "print(round(calc_f1(expected,tokens),2),tokens)\n",
    "    \n",
    "print()\n",
    "for nlist in [[1],[1,2],[2]]:\n",
    "    for threshold in [0.02,0.05,0.1,0.2,0.3,0.4,0.5]: \n",
    "        #tokens = tokenize_with_opposite_metrics(zh_train_chars.model,text,'ddf-','ddf+',[1],threshold=threshold)\n",
    "        tokens = FreedomBasedTokenizer(zh_train_chars,'ddf-','ddf+',[1],threshold=threshold).tokenize(text)\n",
    "        print(nlist,threshold,round(calc_f1(expected,tokens),2),tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2fee7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://magichub.com/datasets/chinese-english-parallel-corpus-finance/\n",
    "test_df = pd.read_csv(os.path.join(path,'magicdata/zh_en_ru_100/CORPUS_ZH_EN_RU.txt'),delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d7513c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "然后医疗保险呢？就是我们家，不论是大人啊还是小孩都会去买一个保险\n",
      "0.84 ['然后', '医疗', '保险', '呢', '？', '就是', '我们', '家', '，', '不论是', '大人', '啊', '还是', '小孩', '都会', '去', '买', '一个', '保险']\n",
      "当他们买了保险的，按照保险合同的话，是要赔三十万的\n",
      "0.8 ['当', '他们', '买', '了', '保险', '的', '，', '按照', '保险', '合同', '的话', '，', '是', '要', '赔', '三', '十', '万', '的']\n",
      "需要意识到买了一个保险的重要性\n",
      "1.0 ['需要', '意识', '到', '买', '了', '一个', '保险', '的', '重要性']\n",
      "其实这种现象是真的很普遍，因为比如说你买一个人身意外险你那个你买的越多你那个保额就越多\n",
      "0.85 ['其实', '这', '种', '现象', '是', '真的', '很', '普遍', '，', '因为', '比如说', '你', '买', '一个', '人身', '意外', '险', '你', '那个', '你', '买', '的', '越', '多', '你', '那个', '保', '额', '就', '越', '多']\n",
      "这代父母真的很有必要去买一个保险\n",
      "0.86 ['这', '代', '父母', '真的', '很', '有', '必要', '去', '买', '一个', '保险']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tokenizer(list(test_df[:5]['zh']),JebaTokenizer(),zh_lexicon_tokenizer,debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aba4593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tokenizer(list(test_df['zh']),JebaTokenizer(),zh_lexicon_tokenizer,debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d24e579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.01 0.42\n",
      "[1] 0.02 0.42\n",
      "[1] 0.05 0.41\n",
      "[1] 0.1 0.4\n",
      "[1] 0.2 0.37\n",
      "[1] 0.3 0.32\n",
      "[1] 0.4 0.29\n",
      "[1] 0.5 0.28\n",
      "[1, 2] 0.01 0.42\n",
      "[1, 2] 0.02 0.42\n",
      "[1, 2] 0.05 0.41\n",
      "[1, 2] 0.1 0.4\n",
      "[1, 2] 0.2 0.37\n",
      "[1, 2] 0.3 0.32\n",
      "[1, 2] 0.4 0.29\n",
      "[1, 2] 0.5 0.28\n",
      "[2] 0.01 0.42\n",
      "[2] 0.02 0.42\n",
      "[2] 0.05 0.41\n",
      "[2] 0.1 0.4\n",
      "[2] 0.2 0.37\n",
      "[2] 0.3 0.32\n",
      "[2] 0.4 0.29\n",
      "[2] 0.5 0.28\n"
     ]
    }
   ],
   "source": [
    "for nlist in [[1],[1,2],[2]]:\n",
    "    for threshold in [0.01,0.02,0.05,0.1,0.2,0.3,0.4,0.5]: \n",
    "        tokenizer = FreedomBasedTokenizer(zh_train_chars,'ddf-','ddf+',[1],threshold=threshold)\n",
    "        avg_f1 = evaluate_tokenizer(list(test_df['zh']),JebaTokenizer(),tokenizer,debug=False)\n",
    "        print(nlist,threshold,avg_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "74eebe75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>英国大银行巴克莱没有选择政府资助，而是将自行募集65亿英镑补充其资本金。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13日首天需要停驶的，是尾号2和7的车辆。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>他被誉为当今世界上最令人瞩目的贸易理论家之一，而他在1994年对亚洲金融危机的预言，更使他在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>代表英格兰、威尔士和苏格兰各市、郡议会的英国地方政府协会（LGA）将在本周与冰岛大使会面。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>此前台湾海基会董事长江丙坤曾经表示，陈云林将在10月底、11月初访问台湾。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>美国联邦储备局批准富国银行（Wells Fargo）以120亿美元代价并购陷入财政困难的美联...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>不过苏起强调说 ： “ 马总统在卸任以前，不论他走到哪里，坐在哪里，都是中华民国总统，这不会...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>克鲁格曼的主要研究领域包括国际贸易、国际金融、货币危机与汇率变化理论。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>这些机关的存款总值超过8.42亿英镑（14.32亿美元 ） ， 部分议会更把用以支付工资的款...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>中国海关总署公布的数字显示，九月份的出口比去年同期上升了21.5 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    zh\n",
       "0                 英国大银行巴克莱没有选择政府资助，而是将自行募集65亿英镑补充其资本金。\n",
       "1                                13日首天需要停驶的，是尾号2和7的车辆。\n",
       "2    他被誉为当今世界上最令人瞩目的贸易理论家之一，而他在1994年对亚洲金融危机的预言，更使他在...\n",
       "3        代表英格兰、威尔士和苏格兰各市、郡议会的英国地方政府协会（LGA）将在本周与冰岛大使会面。\n",
       "4                此前台湾海基会董事长江丙坤曾经表示，陈云林将在10月底、11月初访问台湾。\n",
       "..                                                 ...\n",
       "322  美国联邦储备局批准富国银行（Wells Fargo）以120亿美元代价并购陷入财政困难的美联...\n",
       "323  不过苏起强调说 ： “ 马总统在卸任以前，不论他走到哪里，坐在哪里，都是中华民国总统，这不会...\n",
       "324                克鲁格曼的主要研究领域包括国际贸易、国际金融、货币危机与汇率变化理论。\n",
       "325  这些机关的存款总值超过8.42亿英镑（14.32亿美元 ） ， 部分议会更把用以支付工资的款...\n",
       "326                中国海关总署公布的数字显示，九月份的出口比去年同期上升了21.5 % \n",
       "\n",
       "[327 rows x 1 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://magichub.com/datasets/chinese-english-parallel-corpus-finance/\n",
    "news_df = pd.read_csv(os.path.join(path,'news/news.2008.zh.shuffled.deduped'),usecols=[0], names=['zh'], header=None)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2cb1819f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tokenizer(list(news_df['zh']),JebaTokenizer(),zh_lexicon_tokenizer,debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ab199b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.01 0.43\n",
      "[1] 0.02 0.43\n",
      "[1] 0.05 0.43\n",
      "[1] 0.1 0.41\n",
      "[1] 0.2 0.38\n",
      "[1] 0.3 0.35\n",
      "[1] 0.4 0.32\n",
      "[1] 0.5 0.31\n",
      "[1, 2] 0.01 0.43\n",
      "[1, 2] 0.02 0.43\n",
      "[1, 2] 0.05 0.43\n",
      "[1, 2] 0.1 0.41\n",
      "[1, 2] 0.2 0.38\n",
      "[1, 2] 0.3 0.35\n",
      "[1, 2] 0.4 0.32\n",
      "[1, 2] 0.5 0.31\n",
      "[2] 0.01 0.43\n",
      "[2] 0.02 0.43\n",
      "[2] 0.05 0.43\n",
      "[2] 0.1 0.41\n",
      "[2] 0.2 0.38\n",
      "[2] 0.3 0.35\n",
      "[2] 0.4 0.32\n",
      "[2] 0.5 0.31\n"
     ]
    }
   ],
   "source": [
    "for nlist in [[1],[1,2],[2]]:\n",
    "    for threshold in [0.01,0.02,0.05,0.1,0.2,0.3,0.4,0.5]: \n",
    "        tokenizer = FreedomBasedTokenizer(zh_train_chars,'ddf-','ddf+',[1],threshold=threshold)\n",
    "        avg_f1 = evaluate_tokenizer(list(news_df['zh']),JebaTokenizer(),tokenizer,debug=False)\n",
    "        print(nlist,threshold,avg_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO improve the above compacting model!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1e1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a40aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885fc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del zh_train_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3b93904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cdfb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77cfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9c718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
