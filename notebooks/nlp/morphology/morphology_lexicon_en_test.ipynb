{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0965ab47",
   "metadata": {},
   "source": [
    "# Exploring sybword segmentation based on BPE, DPE, morphology and \"transition freedom\" \n",
    "\n",
    "### Test coprus based on https://arxiv.org/pdf/2005.06606.pdf (with numbers removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "24b9d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "#from importlib import reload  # Python 3.4+\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "if 'pygents.token_plot' in sys.modules:\n",
    "    del sys.modules['pygents.token_plot']\n",
    "\n",
    "\n",
    "from pygents.token import *\n",
    "from pygents.text import *\n",
    "from pygents.util import *\n",
    "from pygents.plot import plot_bars, plot_dict, matrix_plot\n",
    "from pygents.token_plot import *\n",
    "\n",
    "#TODO move to model loading/setting\n",
    "def update_grand_counts(base):\n",
    "    grand_counts = {}\n",
    "    counts = base.model[0]\n",
    "    for e in counts:\n",
    "        n = len(e)\n",
    "        if not n in grand_counts:\n",
    "            grand_counts[n] = 0\n",
    "        grand_counts[n] += counts[e]\n",
    "    base.grand_counts = grand_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a53287ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, log\n",
    "\n",
    "def root_n(x,n):\n",
    "    return exp(log(x)/n)\n",
    "\n",
    "def eval_split_log_div_cnt(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 1\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        cnt /= len(t)\n",
    "        v = math.log2(cnt)\n",
    "        p = p * v\n",
    "    return p\n",
    "\n",
    "def eval_split_root(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 1.0\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        p = p * cnt * len(t)\n",
    "    if debug:\n",
    "        print(p)\n",
    "    return p if p == 0 or len(split) == 1 else root_n(p,len(split))\n",
    "    \n",
    "def eval_avg_log(base,split,extra_len_discount=False,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 0.0\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        if extra_len_discount:\n",
    "            p = p + math.log(cnt*len(t)*len(t)+1)\n",
    "        else:\n",
    "            p = p + math.log(cnt*len(t)+1)\n",
    "    if debug:\n",
    "        print(p)\n",
    "    return p / len(split)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Mutual_information\n",
    "def eval_mi_chunk(base,chunk,log=True,debug=False):\n",
    "    f = base.model[0]\n",
    "    joint_f = f[chunk]/base.grand_counts[len(chunk)] if chunk in f else 0 # P(A,B,...)\n",
    "    singles = list(chunk)\n",
    "    p = 1.0\n",
    "    for single in singles:\n",
    "        single_f = f[single] if single in f else 0\n",
    "        p *= single_f/base.grand_counts[1] # P(A)\n",
    "    if debug:\n",
    "        print(chunk,joint_f,singles,p)\n",
    "     #return math.log(joint_f/p) if log else joint_f/p # joing_f == 0 => Math Domain Error\n",
    "    return math.log(1 + joint_f/p) if log else joint_f/p # cheating...\n",
    "\n",
    "#TODO try multiplication instead!?\n",
    "\n",
    "def eval_mi_sum(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    sum_mi = 0\n",
    "    for t in split:\n",
    "        mi = eval_mi_chunk(base,t,debug=False)\n",
    "        sum_mi += mi\n",
    "    if debug:\n",
    "        print(sum_mi)\n",
    "    return sum_mi\n",
    "    \n",
    "def eval_splits(base,splits,extra_len_discount=False,debug=False):\n",
    "    emax = 0\n",
    "    best = None\n",
    "    for split in splits:\n",
    "        #e = eval_avg_log(base,split,extra_len_discount=extra_len_discount)\n",
    "        e = eval_mi_sum(base,split,debug=False)\n",
    "        if emax < e:\n",
    "            emax = e\n",
    "            best = split\n",
    "        if debug:\n",
    "            print(round(e,2),split)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a5a55345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97565\n",
      "('the', 53097401)\n",
      "97565\n"
     ]
    }
   ],
   "source": [
    "#get raw lexicon list\n",
    "en_lex = list(pd.read_csv(\"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\",sep='\\t',header=None,na_filter=False).to_records(index=False))\n",
    "print(len(en_lex))\n",
    "\n",
    "#debug raw lexicon\n",
    "print(max(en_lex,key=lambda item:item[1]))\n",
    "en_lex_dict = weightedlist2dict(en_lex,lower=True) # no case-insensitive merge\n",
    "print(len(en_lex_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c7f92861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684498\n"
     ]
    }
   ],
   "source": [
    "lex_en_base10 = FreedomTokenizer(max_n=10,mode='chars',debug=False)\n",
    "lex_en_base10.train(en_lex_dict)\n",
    "lex_en_base10.store('data/models/lex_en_counted_10')\n",
    "print(lex_en_base10.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c083a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref_tokenizer = PrefixSuffixMorphoTokenizerCached([\"./data/corpora/English/morphology/prefixes.txt\"],\n",
    "                                   [\"./data/corpora/English/morphology/suffixes.txt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "35e51d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inter', 'est', 'ing', 'ly']\n",
      "['uni', 'v', 'er', 's', 'ities']\n",
      "['anti', 'dis', 'establ', 'ish', 'ment', 'arian', 'ism']\n",
      "['dec', 'ent', 'rali', 's', 'ations']\n",
      "['c', 'ities']\n",
      "['p', 'ing']\n"
     ]
    }
   ],
   "source": [
    "texts = ['interestingly', # ['inter', 'est', 'ing', 'ly']\n",
    "         'universities',\n",
    "         'antidisestablishmentarianism', # anti-dis-establish-ment-ar-i-an-ism\n",
    "         'decentralisations',\n",
    "         'cities',\n",
    "         'ping']\n",
    "for text in texts:\n",
    "    print(en_ref_tokenizer.tokenize(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4e086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "99f6330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test coprus based on https://arxiv.org/pdf/2005.06606.pdf\n",
    "# columns: 1 manual, 2 BPE, 3 DPE \n",
    "tokenizations =[\n",
    "[['re','cogn','ise','s'],['recognises'],['recognise','s']],\n",
    "[['advocate','s'],['advocates'],['advocate','s']],\n",
    "[['euro','zone'],['eurozone'],['euro','zone']],\n",
    "[['under','line','s'],['underlines'],['underline','s']],\n",
    "[['strength','en','s'],['strengthens'],['strengthen','s']],\n",
    "[['entrepreneur','ship'],['entrepreneurship'],['entrepreneur','ship']],\n",
    "[['ac','knowledge','s'],['acknowledges'],['acknowledge','s']],\n",
    "[['wine','s'],['wines'],['wine','s']],\n",
    "[['pre','sent','ly'],['pres','ently'],['present','ly']],\n",
    "[['fill','ed'],['f','illed'],['fill','ed']],\n",
    "[['en','dorse','ment'],['endors','ement'],['endorse','ment']],\n",
    "[['bloc'],['blo','c'],['bl','oc']],\n",
    "[['crucial','ly'],['cru','cially'],['crucial','ly']],\n",
    "[['eval','u','ation','s'],['eval','uations'],['evaluation','s']],\n",
    "[['tree','s'],['tre','es'],['tr','ees']],\n",
    "[['ticket','s'],['tick','ets'],['tick','et','s']],\n",
    "[['pre','dict','able'],['predic','table'],['predict','able']],\n",
    "[['multi','lateral','ism'],['multilater','alism'],['multilateral','ism']],\n",
    "[['rat','ing','s'],['rat','ings'],['rating','s']],\n",
    "[['pre','dict','ed'],['predic','ted'],['predict','ed']],\n",
    "[['motive','s'],['mo','tives'],['motiv','es']],\n",
    "[['re','in','force','s'],['reinfor','ces'],['reinforce','s']],\n",
    "[['proto','col','s'],['pro','tocols'],['protocol','s']],\n",
    "[['progress','ive','ly'],['pro','gressively'],['progressive','ly']],\n",
    "[['skill'],['sk','ill'],['ski','ll']],\n",
    "[['prevail','s'],['preva','ils'],['prevail','s']],\n",
    "[['de','cent','ral','isation'],['decent','ralisation'],['decent','ral','isation']],\n",
    "[['stor','ed'],['sto','red'],['stor','ed']],\n",
    "[['in','fluen','za'],['influ','enz','a'],['influen','za']],\n",
    "[['margin','al','is','ed'],['margin','alised'],['marginal','ised']],\n",
    "[['stay','ing'],['sta','ying'],['stay','ing']],\n",
    "[['intensi','ty'],['intens','ity'],['intensi','ty']],\n",
    "[['re','cast'],['rec','ast'],['re','cast']],\n",
    "[['guide','line'],['guid','eline'],['guide','line']],\n",
    "[['em','bark','ed'],['emb','arked'],['embark','ed']],\n",
    "[['out','line','s'],['out','lines'],['outline','s']],\n",
    "[['scenario','s'],['scen','ari','os'],['scenario','s']],\n",
    "[['nati','ve'],['n','ative'],['na','tive']],\n",
    "[['pre','vent','at','ive'],['preven','tative'],['prevent','ative']],\n",
    "[['home','land'],['hom','eland'],['home','land']],\n",
    "[['bath','ing'],['bat','hing'],['bath','ing']],\n",
    "[['en','danger','ed'],['endang','ered'],['endanger','ed']],\n",
    "[['continent','al'],['cont','inen','tal'],['continent','al']],\n",
    "[['ten','th'],['t','enth'],['ten','th']],\n",
    "[['vulner','abil','ity'],['vul','n','era','bility'],['vul','ner','ability']],\n",
    "[['realis','ing'],['realis','ing'],['real','ising']],\n",
    "[['tight','er'],['t','ighter'],['tight','er']]\n",
    "]\n",
    "tokenizations = [[''.join(i[0]),i[0],i[1],i[2]] for i in tokenizations]\n",
    "for i in tokenizations:\n",
    "    assert len(i)==4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b4e19ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57 \t ['re', 'cogn', 'ise', 's'] \t ['re', 'cogn', 'ises']\n",
      "0 \t ['advocate', 's'] \t ['ad', 'voc', 'ates']\n",
      "0.4 \t ['euro', 'zone'] \t ['euro', 'z', 'one']\n",
      "0.33 \t ['under', 'line', 's'] \t ['under', 'l', 'ines']\n",
      "0 \t ['strength', 'en', 's'] \t ['stre', 'ngth', 'ens']\n",
      "0 \t ['entrepreneur', 'ship'] \t ['entre', 'preneur', 'sh', 'ip']\n",
      "0 \t ['ac', 'knowledge', 's'] \t ['ack', 'nowledg', 'es']\n",
      "0 \t ['wine', 's'] \t ['w', 'i', 'n', 'es']\n",
      "0.33 \t ['pre', 'sent', 'ly'] \t ['pre', 's', 'ently']\n",
      "0 \t ['fill', 'ed'] \t ['f', 'i', 'lled']\n",
      "0 \t ['en', 'dorse', 'ment'] \t ['endo', 'rsem', 'ent']\n",
      "0 \t ['bloc'] \t ['b', 'lo', 'c']\n",
      "0.33 \t ['crucial', 'ly'] \t ['cru', 'ci', 'al', 'ly']\n",
      "0 \t ['eval', 'u', 'ation', 's'] \t ['eva', 'lu', 'ations']\n",
      "0 \t ['tree', 's'] \t ['tre', 'es']\n",
      "0 \t ['ticket', 's'] \t ['ti', 'ck', 'ets']\n",
      "0.57 \t ['multi', 'lateral', 'ism'] \t ['multi', 'later', 'al', 'ism']\n",
      "0 \t ['rat', 'ing', 's'] \t ['ra', 't', 'i', 'ngs']\n",
      "0 \t ['motive', 's'] \t ['mo', 'tiv', 'es']\n",
      "0.29 \t ['re', 'in', 'force', 's'] \t ['re', 'inforc', 'es']\n",
      "0 \t ['proto', 'col', 's'] \t ['pro', 'toc', 'ols']\n",
      "0 \t ['progress', 'ive', 'ly'] \t ['pro', 'gressiv', 'ely']\n",
      "0 \t ['skill'] \t ['sk', 'i', 'll']\n",
      "0 \t ['prevail', 's'] \t ['pre', 'va', 'ils']\n",
      "0.29 \t ['de', 'cent', 'ral', 'isation'] \t ['de', 'centralis', 'ation']\n",
      "0.33 \t ['stor', 'ed'] \t ['st', 'o', 'r', 'ed']\n",
      "0.33 \t ['in', 'fluen', 'za'] \t ['in', 'flu', 'enza']\n",
      "0.5 \t ['margin', 'al', 'is', 'ed'] \t ['mar', 'ginal', 'is', 'ed']\n",
      "0.4 \t ['stay', 'ing'] \t ['sta', 'y', 'ing']\n",
      "0 \t ['intensi', 'ty'] \t ['in', 'tens', 'ity']\n",
      "0.4 \t ['re', 'cast'] \t ['re', 'c', 'ast']\n",
      "0.4 \t ['guide', 'line'] \t ['guid', 'e', 'line']\n",
      "0.33 \t ['out', 'line', 's'] \t ['out', 'l', 'ines']\n",
      "0 \t ['scenario', 's'] \t ['scen', 'ari', 'os']\n",
      "0 \t ['nati', 've'] \t ['n', 'a', 'tive']\n",
      "0.57 \t ['pre', 'vent', 'at', 'ive'] \t ['pre', 'vent', 'ative']\n",
      "0.4 \t ['bath', 'ing'] \t ['ba', 'th', 'ing']\n",
      "0.33 \t ['en', 'danger', 'ed'] \t ['en', 'dang', 'ered']\n",
      "0 \t ['continent', 'al'] \t ['con', 'tin', 'ental']\n",
      "0 \t ['ten', 'th'] \t ['t', 'e', 'nth']\n",
      "0 \t ['vulner', 'abil', 'ity'] \t ['vul', 'ner', 'ability']\n",
      "0.4 \t ['realis', 'ing'] \t ['re', 'alis', 'ing']\n",
      "0 \t ['tight', 'er'] \t ['ti', 'gh', 'ter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46, 0.05, 0.55, 0.25]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "for t in tokenizations:    \n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = tf_tokenizer.tokenize(t[0])\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    if tf_f1 < 1.0:\n",
    "        print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bce113ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |\n",
      "|---|---|---|---|---|\n",
      "| ['euro', 'zone'] | ['eu', 'rozone'] | ['eurozone'] | ['euro', 'zone'] | ['euro', 'z', 'one'] |\n",
      "| ['entrepreneur', 'ship'] | ['ent', 're', 'pre', 'neur', 'ship'] | ['entrepreneurship'] | ['entrepreneur', 'ship'] | ['entre', 'preneur', 'sh', 'ip'] |\n",
      "| ['pre', 'sent', 'ly'] | ['pre', 's', 'ent', 'ly'] | ['pres', 'ently'] | ['present', 'ly'] | ['pre', 's', 'ently'] |\n",
      "| ['bloc'] | ['bloc'] | ['blo', 'c'] | ['bl', 'oc'] | ['b', 'lo', 'c'] |\n",
      "| ['tree', 's'] | ['tr', 'ee', 's'] | ['tre', 'es'] | ['tr', 'ees'] | ['tre', 'es'] |\n",
      "| ['multi', 'lateral', 'ism'] | ['multi', 'lat', 'er', 'al', 'ism'] | ['multilater', 'alism'] | ['multilateral', 'ism'] | ['multi', 'later', 'al', 'ism'] |\n",
      "| ['motive', 's'] | ['mot', 'ive', 's'] | ['mo', 'tives'] | ['motiv', 'es'] | ['mo', 'tiv', 'es'] |\n",
      "| ['progress', 'ive', 'ly'] | ['pro', 'gr', 'ess', 'ive', 'ly'] | ['pro', 'gressively'] | ['progressive', 'ly'] | ['pro', 'gressiv', 'ely'] |\n",
      "| ['de', 'cent', 'ral', 'isation'] | ['dec', 'ent', 'r', 'al', 'isation'] | ['decent', 'ralisation'] | ['decent', 'ral', 'isation'] | ['de', 'centralis', 'ation'] |\n",
      "| ['margin', 'al', 'is', 'ed'] | ['marginali', 's', 'ed'] | ['margin', 'alised'] | ['marginal', 'ised'] | ['mar', 'ginal', 'is', 'ed'] |\n",
      "| ['re', 'cast'] | ['re', 'cast'] | ['rec', 'ast'] | ['re', 'cast'] | ['re', 'c', 'ast'] |\n",
      "| ['out', 'line', 's'] | ['out', 'l', 'ine', 's'] | ['out', 'lines'] | ['outline', 's'] | ['out', 'l', 'ines'] |\n",
      "| ['pre', 'vent', 'at', 'ive'] | ['pre', 'v', 'ent', 'ative'] | ['preven', 'tative'] | ['prevent', 'ative'] | ['pre', 'vent', 'ative'] |\n",
      "| ['en', 'danger', 'ed'] | ['end', 'an', 'g', 'er', 'ed'] | ['endang', 'ered'] | ['endanger', 'ed'] | ['en', 'dang', 'ered'] |\n",
      "| ['vulner', 'abil', 'ity'] | ['vulnerabil', 'ity'] | ['vul', 'n', 'era', 'bility'] | ['vul', 'ner', 'ability'] | ['vul', 'ner', 'ability'] |\n",
      "|**F1**|**0.46**|**0.05**|**0.55**|**0.25**|\n"
     ]
    }
   ],
   "source": [
    "print('| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |')\n",
    "print('|---|---|---|---|---|')\n",
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "i = 0\n",
    "for t in tokenizations:\n",
    "    i +=1\n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = tf_tokenizer.tokenize(t[0])\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "    if i % 3 == 0:\n",
    "        print('|',man,'|',ref,'|',bpe,'|',dpe,'|',tf,'|')\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "#print('||**',f1[0],'**|**',f1[1],'**|**',f1[2],'**|**',f1[3],'**|**')\n",
    "print('|**F1**|**{}**|**{}**|**{}**|**{}**|'.format(f1[0],f1[1],f1[2],f1[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f74cba",
   "metadata": {},
   "source": [
    "| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |\n",
    "|---|---|---|---|---|\n",
    "| ['euro', 'zone'] | ['eu', 'rozone'] | ['eurozone'] | ['euro', 'zone'] | ['euro', 'z', 'one'] |\n",
    "| ['entrepreneur', 'ship'] | ['ent', 're', 'pre', 'neur', 'ship'] | ['entrepreneurship'] | ['entrepreneur', 'ship'] | ['entre', 'preneur', 'sh', 'ip'] |\n",
    "| ['pre', 'sent', 'ly'] | ['pre', 's', 'ent', 'ly'] | ['pres', 'ently'] | ['present', 'ly'] | ['pre', 's', 'ently'] |\n",
    "| ['bloc'] | ['bloc'] | ['blo', 'c'] | ['bl', 'oc'] | ['b', 'lo', 'c'] |\n",
    "| ['tree', 's'] | ['tr', 'ee', 's'] | ['tre', 'es'] | ['tr', 'ees'] | ['tre', 'es'] |\n",
    "| ['multi', 'lateral', 'ism'] | ['multi', 'lat', 'er', 'al', 'ism'] | ['multilater', 'alism'] | ['multilateral', 'ism'] | ['multi', 'later', 'al', 'ism'] |\n",
    "| ['motive', 's'] | ['mot', 'ive', 's'] | ['mo', 'tives'] | ['motiv', 'es'] | ['mo', 'tiv', 'es'] |\n",
    "| ['progress', 'ive', 'ly'] | ['pro', 'gr', 'ess', 'ive', 'ly'] | ['pro', 'gressively'] | ['progressive', 'ly'] | ['pro', 'gressiv', 'ely'] |\n",
    "| ['de', 'cent', 'ral', 'isation'] | ['dec', 'ent', 'r', 'al', 'isation'] | ['decent', 'ralisation'] | ['decent', 'ral', 'isation'] | ['de', 'centralis', 'ation'] |\n",
    "| ['margin', 'al', 'is', 'ed'] | ['marginali', 's', 'ed'] | ['margin', 'alised'] | ['marginal', 'ised'] | ['mar', 'ginal', 'is', 'ed'] |\n",
    "| ['re', 'cast'] | ['re', 'cast'] | ['rec', 'ast'] | ['re', 'cast'] | ['re', 'c', 'ast'] |\n",
    "| ['out', 'line', 's'] | ['out', 'l', 'ine', 's'] | ['out', 'lines'] | ['outline', 's'] | ['out', 'l', 'ines'] |\n",
    "| ['pre', 'vent', 'at', 'ive'] | ['pre', 'v', 'ent', 'ative'] | ['preven', 'tative'] | ['prevent', 'ative'] | ['pre', 'vent', 'ative'] |\n",
    "| ['en', 'danger', 'ed'] | ['end', 'an', 'g', 'er', 'ed'] | ['endang', 'ered'] | ['endanger', 'ed'] | ['en', 'dang', 'ered'] |\n",
    "| ['vulner', 'abil', 'ity'] | ['vulnerabil', 'ity'] | ['vul', 'n', 'era', 'bility'] | ['vul', 'ner', 'ability'] | ['vul', 'ner', 'ability'] |\n",
    "|**F1**|**0.46**|**0.05**|**0.55**|**0.25**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9ea70481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.5 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.7 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.9 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.95 [0.46, 0.05, 0.55, 0.09]\n",
      "[2] 0.5 [0.46, 0.05, 0.55, 0.12]\n",
      "[2] 0.7 [0.46, 0.05, 0.55, 0.13]\n",
      "[2] 0.9 [0.46, 0.05, 0.55, 0.14]\n",
      "[2] 0.95 [0.46, 0.05, 0.55, 0.14]\n",
      "[3] 0.5 [0.46, 0.05, 0.55, 0.18]\n",
      "[3] 0.7 [0.46, 0.05, 0.55, 0.2]\n",
      "[3] 0.9 [0.46, 0.05, 0.55, 0.21]\n",
      "[3] 0.95 [0.46, 0.05, 0.55, 0.18]\n",
      "[4] 0.5 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.7 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.9 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.95 [0.46, 0.05, 0.55, 0.24]\n",
      "[5] 0.5 [0.46, 0.05, 0.55, 0.24]\n",
      "[5] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[5] 0.9 [0.46, 0.05, 0.55, 0.27]\n",
      "[5] 0.95 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.5 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.9 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.95 [0.46, 0.05, 0.55, 0.24]\n",
      "[7] 0.5 [0.46, 0.05, 0.55, 0.26]\n",
      "[7] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[7] 0.9 [0.46, 0.05, 0.55, 0.25]\n",
      "[7] 0.95 [0.46, 0.05, 0.55, 0.24]\n"
     ]
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "    for th in [0.5,0.7,0.9,0.95]:\n",
    "        tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "        f1 = [0,0,0,0]\n",
    "        for t in tokenizations:    \n",
    "            man = t[1] # manual\n",
    "            ref = en_ref_tokenizer.tokenize(t[0])\n",
    "            bpe = t[2]\n",
    "            dpe = t[3]\n",
    "            tf = tf_tokenizer.tokenize(t[0])\n",
    "            ref_f1 = calc_f1(man,ref)\n",
    "            bpe_f1 = calc_f1(man,bpe)\n",
    "            dpe_f1 = calc_f1(man,dpe)\n",
    "            tf_f1 = calc_f1(man,tf)\n",
    "            #if tf_f1 < 1.0:\n",
    "            #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "            f1[0] += ref_f1\n",
    "            f1[1] += bpe_f1\n",
    "            f1[2] += dpe_f1\n",
    "            f1[3] += tf_f1\n",
    "        f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "        print(n,th,f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e4604",
   "metadata": {},
   "source": [
    "### Check if limiting training set by word frequency helps to improve morho-parsing F1 (0.0005-0.001 is the best)\n",
    "\n",
    "#### Training in https://github.com/aigents/pygents/blob/main/notebooks/nlp/morphology/morphology_lexicon_en_ru.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ebb609bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/lex_en_counted_10 [0.46, 0.05, 0.55, 0.24]\n",
      "data/models/lex_en_counted_10_0005 [0.46, 0.05, 0.55, 0.28]\n",
      "data/models/lex_en_counted_10_001 [0.46, 0.05, 0.55, 0.28]\n",
      "data/models/lex_en_counted_10_005 [0.46, 0.05, 0.55, 0.21]\n",
      "data/models/lex_en_counted_10_01 [0.46, 0.05, 0.55, 0.08]\n"
     ]
    }
   ],
   "source": [
    "for model in ['data/models/lex_en_counted_10','data/models/lex_en_counted_10_0005','data/models/lex_en_counted_10_001',\n",
    "              'data/models/lex_en_counted_10_005','data/models/lex_en_counted_10_01']:\n",
    "    best_tf_f1 = 0\n",
    "    best_f1 = None\n",
    "    base = FreedomTokenizer(name=model,max_n=10,mode='chars',debug=False)\n",
    "    tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "    for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "        for th in [0.5,0.7,0.9,0.95]:\n",
    "            tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "            f1 = [0,0,0,0]\n",
    "            for t in tokenizations:    \n",
    "                man = t[1] # manual\n",
    "                ref = en_ref_tokenizer.tokenize(t[0])\n",
    "                bpe = t[2]\n",
    "                dpe = t[3]\n",
    "                tf = tf_tokenizer.tokenize(t[0])\n",
    "                ref_f1 = calc_f1(man,ref)\n",
    "                bpe_f1 = calc_f1(man,bpe)\n",
    "                dpe_f1 = calc_f1(man,dpe)\n",
    "                tf_f1 = calc_f1(man,tf)\n",
    "                #if tf_f1 < 1.0:\n",
    "                #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "                f1[0] += ref_f1\n",
    "                f1[1] += bpe_f1\n",
    "                f1[2] += dpe_f1\n",
    "                f1[3] += tf_f1\n",
    "            f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "            #print(n,th,f1)\n",
    "            if best_tf_f1 < f1[3]:\n",
    "                best_tf_f1 = f1[3]\n",
    "                best_f1 = f1\n",
    "    print(model,f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "aa80f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1257863 [0.46, 0.05, 0.55, 0.24]\n",
      "0.0001 923736 [0.46, 0.05, 0.55, 0.24]\n",
      "0.0005 899442 [0.46, 0.05, 0.55, 0.24]\n",
      "0.001 896201 [0.46, 0.05, 0.55, 0.23]\n",
      "0.005 891853 [0.46, 0.05, 0.55, 0.24]\n",
      "0.01 888733 [0.46, 0.05, 0.55, 0.25]\n",
      "0.05 863488 [0.46, 0.05, 0.55, 0.24]\n",
      "0.1 834719 [0.46, 0.05, 0.55, 0.27]\n",
      "0.5 689472 [0.46, 0.05, 0.55, 0.19]\n"
     ]
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_nocount_7',max_n=7,mode='chars',debug=False)\n",
    "for model_threshold in [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5]:\n",
    "    if model_threshold > 0:\n",
    "        model_compress_with_loss(base.model,model_threshold)\n",
    "    best_tf_f1 = 0\n",
    "    best_f1 = None\n",
    "    tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "    for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "        for th in [0.5,0.7,0.9,0.95]:\n",
    "            tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "            f1 = [0,0,0,0]\n",
    "            for t in tokenizations:    \n",
    "                man = t[1] # manual\n",
    "                ref = en_ref_tokenizer.tokenize(t[0])\n",
    "                bpe = t[2]\n",
    "                dpe = t[3]\n",
    "                tf = tf_tokenizer.tokenize(t[0])\n",
    "                ref_f1 = calc_f1(man,ref)\n",
    "                bpe_f1 = calc_f1(man,bpe)\n",
    "                dpe_f1 = calc_f1(man,dpe)\n",
    "                tf_f1 = calc_f1(man,tf)\n",
    "                #if tf_f1 < 1.0:\n",
    "                #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "                f1[0] += ref_f1\n",
    "                f1[1] += bpe_f1\n",
    "                f1[2] += dpe_f1\n",
    "                f1[3] += tf_f1\n",
    "            f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "            #print(n,th,f1)\n",
    "            if best_tf_f1 < f1[3]:\n",
    "                best_tf_f1 = f1[3]\n",
    "                best_f1 = f1\n",
    "    print(model_threshold,base.count_params(),f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ab17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3d7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "557cd245",
   "metadata": {},
   "source": [
    "## Experiment with agglomerative (BPE) parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "33153a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "572854b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3563505298,\n",
       " 2: 2819662449,\n",
       " 3: 2098120824,\n",
       " 4: 1507873050,\n",
       " 5: 1070193634,\n",
       " 6: 742502578,\n",
       " 7: 494400804,\n",
       " 8: 308690258,\n",
       " 9: 182032346,\n",
       " 10: 99581691}"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_grand_counts(base)\n",
    "base.grand_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "7c2ce1dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vulnerabil', 'it', 'y']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_prob_greedy(base,text,debug=True):\n",
    "    freqs = base.model[0]\n",
    "    chunks = list(text)\n",
    "    while True:\n",
    "        fmax = 0\n",
    "        imax = -1\n",
    "        for i in range(len(chunks)-1):\n",
    "            chunk = chunks[i] + chunks[i+1]\n",
    "            f = eval_mi_chunk(base,chunk,debug=debug)\n",
    "            #if debug:\n",
    "            #    print(chunk,f)\n",
    "            if fmax < f:\n",
    "                fmax = f\n",
    "                imax = i\n",
    "        #print(imax)\n",
    "        if imax < 0:\n",
    "            return chunks\n",
    "        chunks[imax] = chunks[imax] + chunks[imax+1]\n",
    "        #print(chunks[imax])\n",
    "        for i in range(imax+1,len(chunks)-1):\n",
    "            chunks[i] = chunks[i+1]\n",
    "        chunks.pop()\n",
    "        if debug:\n",
    "            print(chunks)\n",
    "        if len(chunks) == 3:\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "#split_prob_greedy(base,'progressively')\n",
    "split_prob_greedy(base,'vulnerability',debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "777d2ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.374889277320312\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(eval_mi_chunk(base,'ozone',debug=False))\n",
    "print(eval_mi_chunk(base,'rozone',debug=False))\n",
    "print(eval_mi_chunk(base,'zoneo',debug=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "11d6da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu 0.00031152913367751844 ['e', 'u'] 0.0034099595011751692\n",
      "ur 0.005427480160055145 ['u', 'r'] 0.0017140935175734382\n",
      "ro 0.007267251087897862 ['r', 'o'] 0.004797911738130865\n",
      "oz 3.4722241321730635e-05 ['o', 'z'] 6.87284691351548e-05\n",
      "zo 7.18103686743817e-05 ['z', 'o'] 6.87284691351548e-05\n",
      "on 0.017580458262860667 ['o', 'n'] 0.005526995454371003\n",
      "ne 0.00691722160108818 ['n', 'e'] 0.009036296657462991\n",
      "['e', 'u', 'r', 'o', 'z', 'on', 'e']\n",
      "eu 0.00031152913367751844 ['e', 'u'] 0.0034099595011751692\n",
      "ur 0.005427480160055145 ['u', 'r'] 0.0017140935175734382\n",
      "ro 0.007267251087897862 ['r', 'o'] 0.004797911738130865\n",
      "oz 3.4722241321730635e-05 ['o', 'z'] 6.87284691351548e-05\n",
      "zon 6.141019074123636e-05 ['z', 'o', 'n'] 4.971563407080034e-06\n",
      "one 0.0021090796818667864 ['o', 'n', 'e'] 0.0006904357575258553\n",
      "['e', 'u', 'r', 'o', 'zon', 'e']\n",
      "eu 0.00031152913367751844 ['e', 'u'] 0.0034099595011751692\n",
      "ur 0.005427480160055145 ['u', 'r'] 0.0017140935175734382\n",
      "ro 0.007267251087897862 ['r', 'o'] 0.004797911738130865\n",
      "ozon 2.6414690547058984e-06 ['o', 'z', 'o', 'n'] 3.7986193649587844e-07\n",
      "zone 3.627560025693144e-05 ['z', 'o', 'n', 'e'] 6.210508359185478e-07\n",
      "['e', 'u', 'r', 'o', 'zone']\n",
      "eu 0.00031152913367751844 ['e', 'u'] 0.0034099595011751692\n",
      "ur 0.005427480160055145 ['u', 'r'] 0.0017140935175734382\n",
      "ro 0.007267251087897862 ['r', 'o'] 0.004797911738130865\n",
      "ozone 3.721756393852741e-06 ['o', 'z', 'o', 'n', 'e'] 4.745259265092297e-08\n",
      "['e', 'u', 'r', 'ozone']\n",
      "eu 0.00031152913367751844 ['e', 'u'] 0.0034099595011751692\n",
      "ur 0.005427480160055145 ['u', 'r'] 0.0017140935175734382\n",
      "rozone 0 ['r', 'o', 'z', 'o', 'n', 'e'] 2.9797470956295864e-09\n",
      "['e', 'ur', 'ozone']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e', 'ur', 'ozone']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_prob_greedy(base,'eurozone',debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8cc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb6f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "eb309a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126467\n",
      "63859\n"
     ]
    }
   ],
   "source": [
    "print(base.model[0]['progress'])\n",
    "print(base.model[0]['sively'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "7e953599",
   "metadata": {},
   "outputs": [],
   "source": [
    "basem = FreedomTokenizer(name='data/models/brown_nolines_chars_7a',max_n=7,mode='chars',debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f7d5cc35",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FreedomTokenizer' object has no attribute 'grand_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lb/1m7gbdp17h578qq48pbbtxf40000gn/T/ipykernel_22701/3059315849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_prob_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'progressively'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lb/1m7gbdp17h578qq48pbbtxf40000gn/T/ipykernel_22701/3316193459.py\u001b[0m in \u001b[0;36msplit_prob_greedy\u001b[0;34m(base, text, debug)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_mi_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;31m#if debug:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m#    print(chunk,f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lb/1m7gbdp17h578qq48pbbtxf40000gn/T/ipykernel_22701/359161298.py\u001b[0m in \u001b[0;36meval_mi_chunk\u001b[0;34m(base, chunk, log, debug)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_mi_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mjoint_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrand_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# P(A,B,...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0msingles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FreedomTokenizer' object has no attribute 'grand_counts'"
     ]
    }
   ],
   "source": [
    "split_prob_greedy(basem,'progressively')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0da09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7a2541c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t ['re', 'cogn', 'ise', 's'] \t ['recogn', 'is', 'es']\n",
      "0 \t ['advocate', 's'] \t ['adv', 'oc', 'ates']\n",
      "0.4 \t ['euro', 'zone'] \t ['euro', 'z', 'one']\n",
      "0.33 \t ['under', 'line', 's'] \t ['under', 'lin', 'es']\n",
      "0 \t ['strength', 'en', 's'] \t ['streng', 'the', 'ns']\n",
      "0.4 \t ['entrepreneur', 'ship'] \t ['entrepre', 'neur', 'ship']\n",
      "0 \t ['ac', 'knowledge', 's'] \t ['ack', 'nowled', 'ges']\n",
      "0 \t ['wine', 's'] \t ['w', 'in', 'es']\n",
      "0.33 \t ['pre', 'sent', 'ly'] \t ['pres', 'ent', 'ly']\n",
      "0.4 \t ['fill', 'ed'] \t ['fi', 'll', 'ed']\n",
      "0.33 \t ['en', 'dorse', 'ment'] \t ['end', 'orse', 'ment']\n",
      "0 \t ['bloc'] \t ['b', 'lo', 'c']\n",
      "0 \t ['crucial', 'ly'] \t ['cr', 'u', 'cially']\n",
      "0 \t ['eval', 'u', 'ation', 's'] \t ['ev', 'alu', 'ations']\n",
      "0 \t ['tree', 's'] \t ['t', 're', 'es']\n",
      "0 \t ['ticket', 's'] \t ['tic', 'k', 'ets']\n",
      "0.33 \t ['pre', 'dict', 'able'] \t ['pre', 'dic', 'table']\n",
      "0.33 \t ['pre', 'dict', 'ed'] \t ['pre', 'dic', 'ted']\n",
      "0 \t ['motive', 's'] \t ['mo', 'tiv', 'es']\n",
      "0.29 \t ['re', 'in', 'force', 's'] \t ['re', 'infor', 'ces']\n",
      "0.33 \t ['proto', 'col', 's'] \t ['proto', 'co', 'ls']\n",
      "0.33 \t ['progress', 'ive', 'ly'] \t ['progres', 'sive', 'ly']\n",
      "0 \t ['skill'] \t ['s', 'k', 'ill']\n",
      "0 \t ['prevail', 's'] \t ['pre', 'va', 'ils']\n",
      "0.57 \t ['de', 'cent', 'ral', 'isation'] \t ['de', 'central', 'isation']\n",
      "0 \t ['stor', 'ed'] \t ['st', 'ore', 'd']\n",
      "0.33 \t ['in', 'fluen', 'za'] \t ['inf', 'luen', 'za']\n",
      "0.57 \t ['margin', 'al', 'is', 'ed'] \t ['margin', 'alis', 'ed']\n",
      "0.4 \t ['stay', 'ing'] \t ['st', 'ay', 'ing']\n",
      "0 \t ['intensi', 'ty'] \t ['int', 'ens', 'ity']\n",
      "0.4 \t ['re', 'cast'] \t ['re', 'ca', 'st']\n",
      "0.4 \t ['guide', 'line'] \t ['gu', 'ide', 'line']\n",
      "0 \t ['em', 'bark', 'ed'] \t ['emb', 'ar', 'ked']\n",
      "0.33 \t ['out', 'line', 's'] \t ['out', 'lin', 'es']\n",
      "0.4 \t ['scenario', 's'] \t ['scen', 'ario', 's']\n",
      "0.4 \t ['nati', 've'] \t ['n', 'ati', 've']\n",
      "0.57 \t ['pre', 'vent', 'at', 'ive'] \t ['pre', 'vent', 'ative']\n",
      "0.4 \t ['home', 'land'] \t ['ho', 'me', 'land']\n",
      "0.4 \t ['bath', 'ing'] \t ['ba', 'th', 'ing']\n",
      "0 \t ['en', 'danger', 'ed'] \t ['end', 'ang', 'ered']\n",
      "0 \t ['continent', 'al'] \t ['con', 'tin', 'ental']\n",
      "0.4 \t ['ten', 'th'] \t ['t', 'en', 'th']\n",
      "0 \t ['vulner', 'abil', 'ity'] \t ['vul', 'ner', 'ability']\n",
      "0.4 \t ['realis', 'ing'] \t ['real', 'is', 'ing']\n",
      "0 \t ['tight', 'er'] \t ['ti', 'gh', 'ter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46, 0.05, 0.55, 0.25]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "for t in tokenizations:    \n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = split_prob_greedy(base,t[0],debug=False)\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    if tf_f1 < 1.0:\n",
    "        print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "154f47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38062\n",
      "38213\n",
      "10298053\n",
      "['t', 'i', 'g', 'h', 't', 'er']\n",
      "['ti', 'g', 'h', 't', 'er']\n",
      "['ti', 'g', 'h', 'ter']\n",
      "['ti', 'gh', 'ter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ti', 'gh', 'ter']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(base.model[0]['tight'])\n",
    "print(base.model[0]['tigh'])\n",
    "print(base.model[0]['ter'])\n",
    "split_prob_greedy(base,'tighter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e1409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969179e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "896eb91a",
   "metadata": {},
   "source": [
    "## Experiment with MI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98e82680",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = FreedomTokenizer(name='data/models/brown_nolines_chars_7a',max_n=7,mode='chars',debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7213efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381728"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.model[0]['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "53cd4d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3563505298,\n",
       " 2: 2819662449,\n",
       " 3: 2098120824,\n",
       " 4: 1507873050,\n",
       " 5: 1070193634,\n",
       " 6: 742502578,\n",
       " 7: 494400804,\n",
       " 8: 308690258,\n",
       " 9: 182032346,\n",
       " 10: 99581691}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_grand_counts(base)\n",
    "base.grand_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "106d98fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.031500645976648\n",
      "-4.575587584027379\n",
      "-1.0360678050857557\n",
      "-1.3327155922944385\n",
      "0.14110262944319749\n",
      "-6.434512698571628\n"
     ]
    }
   ],
   "source": [
    "update_grand_counts(lex_en_base10)\n",
    "print(eval_mi_chunk(lex_en_base10,\"the\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"teh\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"het\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"hte\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"eth\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"eht\",debug=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6e35add0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 4.039976777585103\n",
      "adn -3.2576567133194585\n",
      "nda 0.5944074225183512\n",
      "nad -1.1139770254934929\n",
      "dan 0.13893075456724688\n",
      "dna -2.499833279100025\n"
     ]
    }
   ],
   "source": [
    "for w in ['and','adn','nda','nad','dan','dna']:\n",
    "    print(w,eval_mi_chunk(lex_en_base10,w,debug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e9f49a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05 ['lighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "5.22 ['l', 'i', 'ghter']\n",
      "1.51 ['l', 'ig', 'hter']\n",
      "5.07 ['l', 'igh', 'ter']\n",
      "6.55 ['l', 'ight', 'er']\n",
      "5.43 ['l', 'ighte', 'r']\n",
      "1.63 ['li', 'g', 'hter']\n",
      "3.49 ['li', 'gh', 'ter']\n",
      "4.64 ['li', 'ght', 'er']\n",
      "3.64 ['li', 'ghte', 'r']\n",
      "4.09 ['lig', 'h', 'ter']\n",
      "1.86 ['lig', 'ht', 'er']\n",
      "0.85 ['lig', 'hte', 'r']\n",
      "5.81 ['ligh', 't', 'er']\n",
      "4.89 ['ligh', 'te', 'r']\n",
      "7.57 ['light', 'e', 'r']\n",
      "['lighter']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r']],debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c5e87282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05 ['lighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "5.22 ['l', 'i', 'ghter']\n",
      "1.51 ['l', 'ig', 'hter']\n",
      "5.07 ['l', 'igh', 'ter']\n",
      "6.55 ['l', 'ight', 'er']\n",
      "5.43 ['l', 'ighte', 'r']\n",
      "1.63 ['li', 'g', 'hter']\n",
      "3.49 ['li', 'gh', 'ter']\n",
      "4.64 ['li', 'ght', 'er']\n",
      "3.64 ['li', 'ghte', 'r']\n",
      "4.09 ['lig', 'h', 'ter']\n",
      "1.86 ['lig', 'ht', 'er']\n",
      "0.85 ['lig', 'hte', 'r']\n",
      "5.81 ['ligh', 't', 'er']\n",
      "4.89 ['ligh', 'te', 'r']\n",
      "7.57 ['light', 'e', 'r']\n",
      "['lighter']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r']],debug=True,extra_len_discount=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "89fb8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.54 ['t', 'ighter']\n",
      "5.86 ['ti', 'ghter']\n",
      "0.82 ['tig', 'hter']\n",
      "3.25 ['tigh', 'ter']\n",
      "5.02 ['tight', 'er']\n",
      "5.06 ['tighte', 'r']\n",
      "['t', 'ighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "['light', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(base,[['t', 'ighter'],['ti', 'ghter'],['tig', 'hter'],['tigh', 'ter'],['tight', 'er'],['tighte', 'r']],debug=True))\n",
    "print(eval_splits(base,[['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r']],debug=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ebaed72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.54 ['t', 'ighter']\n",
      "5.86 ['ti', 'ghter']\n",
      "0.82 ['tig', 'hter']\n",
      "3.25 ['tigh', 'ter']\n",
      "5.02 ['tight', 'er']\n",
      "5.06 ['tighte', 'r']\n",
      "['t', 'ighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "['light', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['t', 'ighter'],['ti', 'ghter'],['tig', 'hter'],['tigh', 'ter'],['tight', 'er'],['tighte', 'r']],debug=True))\n",
    "print(eval_splits(lex_en_base10,[['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r']],debug=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "cdf02a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05 ['lighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "5.22 ['l', 'i', 'ghter']\n",
      "1.51 ['l', 'ig', 'hter']\n",
      "5.07 ['l', 'igh', 'ter']\n",
      "6.55 ['l', 'ight', 'er']\n",
      "5.43 ['l', 'ighte', 'r']\n",
      "1.63 ['li', 'g', 'hter']\n",
      "3.49 ['li', 'gh', 'ter']\n",
      "4.64 ['li', 'ght', 'er']\n",
      "3.64 ['li', 'ghte', 'r']\n",
      "4.09 ['lig', 'h', 'ter']\n",
      "1.86 ['lig', 'ht', 'er']\n",
      "0.85 ['lig', 'hte', 'r']\n",
      "5.81 ['ligh', 't', 'er']\n",
      "4.89 ['ligh', 'te', 'r']\n",
      "7.57 ['light', 'e', 'r']\n",
      "['lighter']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[\n",
    "    ['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],\n",
    "    ['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r'],\n",
    "],debug=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "bbf273d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1587747904004283e+21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10503475.162330275"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_split_root(lex_en_base10,['l', 'ig','hter'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c34f1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde63d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd6297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
