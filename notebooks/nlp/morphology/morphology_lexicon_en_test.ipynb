{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59575ea",
   "metadata": {},
   "source": [
    "# Exploring sybword segmentation based on BPE, DPE, morphology and \"transition freedom\" \n",
    "\n",
    "### Test coprus based on https://arxiv.org/pdf/2005.06606.pdf (with numbers removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "24b9d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "#from importlib import reload  # Python 3.4+\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "if 'pygents.token_plot' in sys.modules:\n",
    "    del sys.modules['pygents.token_plot']\n",
    "\n",
    "\n",
    "from pygents.token import *\n",
    "from pygents.text import *\n",
    "from pygents.util import *\n",
    "from pygents.plot import plot_bars, plot_dict, matrix_plot\n",
    "from pygents.token_plot import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a5a55345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97565\n",
      "('the', 53097401)\n",
      "97565\n"
     ]
    }
   ],
   "source": [
    "#get raw lexicon list\n",
    "en_lex = list(pd.read_csv(\"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\",sep='\\t',header=None,na_filter=False).to_records(index=False))\n",
    "print(len(en_lex))\n",
    "\n",
    "#debug raw lexicon\n",
    "print(max(en_lex,key=lambda item:item[1]))\n",
    "en_lex_dict = weightedlist2dict(en_lex,lower=True) # no case-insensitive merge\n",
    "print(len(en_lex_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6fe0a53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684498\n"
     ]
    }
   ],
   "source": [
    "lex_en_base10 = FreedomTokenizer(max_n=10,mode='chars',debug=False)\n",
    "lex_en_base10.train(en_lex_dict)\n",
    "lex_en_base10.store('data/models/lex_en_counted_10')\n",
    "print(lex_en_base10.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c083a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref_tokenizer = PrefixSuffixMorphoTokenizerCached([\"./data/corpora/English/morphology/prefixes.txt\"],\n",
    "                                   [\"./data/corpora/English/morphology/suffixes.txt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "35e51d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inter', 'est', 'ing', 'ly']\n",
      "['uni', 'v', 'er', 's', 'ities']\n",
      "['anti', 'dis', 'establ', 'ish', 'ment', 'arian', 'ism']\n",
      "['dec', 'ent', 'rali', 's', 'ations']\n",
      "['c', 'ities']\n",
      "['p', 'ing']\n"
     ]
    }
   ],
   "source": [
    "texts = ['interestingly', # ['inter', 'est', 'ing', 'ly']\n",
    "         'universities',\n",
    "         'antidisestablishmentarianism', # anti-dis-establish-ment-ar-i-an-ism\n",
    "         'decentralisations',\n",
    "         'cities',\n",
    "         'ping']\n",
    "for text in texts:\n",
    "    print(en_ref_tokenizer.tokenize(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7e0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "af23f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test coprus based on https://arxiv.org/pdf/2005.06606.pdf\n",
    "# columns: 1 manual, 2 BPE, 3 DPE \n",
    "tokenizations =[\n",
    "[['re','cogn','ise','s'],['recognises'],['recognise','s']],\n",
    "[['advocate','s'],['advocates'],['advocate','s']],\n",
    "[['euro','zone'],['eurozone'],['euro','zone']],\n",
    "[['under','line','s'],['underlines'],['underline','s']],\n",
    "[['strength','en','s'],['strengthens'],['strengthen','s']],\n",
    "[['entrepreneur','ship'],['entrepreneurship'],['entrepreneur','ship']],\n",
    "[['ac','knowledge','s'],['acknowledges'],['acknowledge','s']],\n",
    "[['wine','s'],['wines'],['wine','s']],\n",
    "[['pre','sent','ly'],['pres','ently'],['present','ly']],\n",
    "[['fill','ed'],['f','illed'],['fill','ed']],\n",
    "[['en','dorse','ment'],['endors','ement'],['endorse','ment']],\n",
    "[['bloc'],['blo','c'],['bl','oc']],\n",
    "[['crucial','ly'],['cru','cially'],['crucial','ly']],\n",
    "[['eval','u','ation','s'],['eval','uations'],['evaluation','s']],\n",
    "[['tree','s'],['tre','es'],['tr','ees']],\n",
    "[['ticket','s'],['tick','ets'],['tick','et','s']],\n",
    "[['pre','dict','able'],['predic','table'],['predict','able']],\n",
    "[['multi','lateral','ism'],['multilater','alism'],['multilateral','ism']],\n",
    "[['rat','ing','s'],['rat','ings'],['rating','s']],\n",
    "[['pre','dict','ed'],['predic','ted'],['predict','ed']],\n",
    "[['motive','s'],['mo','tives'],['motiv','es']],\n",
    "[['re','in','force','s'],['reinfor','ces'],['reinforce','s']],\n",
    "[['proto','col','s'],['pro','tocols'],['protocol','s']],\n",
    "[['progress','ive','ly'],['pro','gressively'],['progressive','ly']],\n",
    "[['skill'],['sk','ill'],['ski','ll']],\n",
    "[['prevail','s'],['preva','ils'],['prevail','s']],\n",
    "[['de','cent','ral','isation'],['decent','ralisation'],['decent','ral','isation']],\n",
    "[['stor','ed'],['sto','red'],['stor','ed']],\n",
    "[['in','fluen','za'],['influ','enz','a'],['influen','za']],\n",
    "[['margin','al','is','ed'],['margin','alised'],['marginal','ised']],\n",
    "[['stay','ing'],['sta','ying'],['stay','ing']],\n",
    "[['intensi','ty'],['intens','ity'],['intensi','ty']],\n",
    "[['re','cast'],['rec','ast'],['re','cast']],\n",
    "[['guide','line'],['guid','eline'],['guide','line']],\n",
    "[['em','bark','ed'],['emb','arked'],['embark','ed']],\n",
    "[['out','line','s'],['out','lines'],['outline','s']],\n",
    "[['scenario','s'],['scen','ari','os'],['scenario','s']],\n",
    "[['nati','ve'],['n','ative'],['na','tive']],\n",
    "[['pre','vent','at','ive'],['preven','tative'],['prevent','ative']],\n",
    "[['home','land'],['hom','eland'],['home','land']],\n",
    "[['bath','ing'],['bat','hing'],['bath','ing']],\n",
    "[['en','danger','ed'],['endang','ered'],['endanger','ed']],\n",
    "[['continent','al'],['cont','inen','tal'],['continent','al']],\n",
    "[['ten','th'],['t','enth'],['ten','th']],\n",
    "[['vulner','abil','ity'],['vul','n','era','bility'],['vul','ner','ability']],\n",
    "[['realis','ing'],['realis','ing'],['real','ising']],\n",
    "[['tight','er'],['t','ighter'],['tight','er']]\n",
    "]\n",
    "tokenizations = [[''.join(i[0]),i[0],i[1],i[2]] for i in tokenizations]\n",
    "for i in tokenizations:\n",
    "    assert len(i)==4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3500578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57 \t ['re', 'cogn', 'ise', 's'] \t ['re', 'cogn', 'ises']\n",
      "0 \t ['advocate', 's'] \t ['ad', 'voc', 'ates']\n",
      "0.4 \t ['euro', 'zone'] \t ['euro', 'z', 'one']\n",
      "0.33 \t ['under', 'line', 's'] \t ['under', 'l', 'ines']\n",
      "0 \t ['strength', 'en', 's'] \t ['stre', 'ngth', 'ens']\n",
      "0 \t ['entrepreneur', 'ship'] \t ['entre', 'preneur', 'sh', 'ip']\n",
      "0 \t ['ac', 'knowledge', 's'] \t ['ack', 'nowledg', 'es']\n",
      "0 \t ['wine', 's'] \t ['w', 'i', 'n', 'es']\n",
      "0.33 \t ['pre', 'sent', 'ly'] \t ['pre', 's', 'ently']\n",
      "0 \t ['fill', 'ed'] \t ['f', 'i', 'lled']\n",
      "0 \t ['en', 'dorse', 'ment'] \t ['endo', 'rsem', 'ent']\n",
      "0 \t ['bloc'] \t ['b', 'lo', 'c']\n",
      "0.33 \t ['crucial', 'ly'] \t ['cru', 'ci', 'al', 'ly']\n",
      "0 \t ['eval', 'u', 'ation', 's'] \t ['eva', 'lu', 'ations']\n",
      "0 \t ['tree', 's'] \t ['tre', 'es']\n",
      "0 \t ['ticket', 's'] \t ['ti', 'ck', 'ets']\n",
      "0.57 \t ['multi', 'lateral', 'ism'] \t ['multi', 'later', 'al', 'ism']\n",
      "0 \t ['rat', 'ing', 's'] \t ['ra', 't', 'i', 'ngs']\n",
      "0 \t ['motive', 's'] \t ['mo', 'tiv', 'es']\n",
      "0.29 \t ['re', 'in', 'force', 's'] \t ['re', 'inforc', 'es']\n",
      "0 \t ['proto', 'col', 's'] \t ['pro', 'toc', 'ols']\n",
      "0 \t ['progress', 'ive', 'ly'] \t ['pro', 'gressiv', 'ely']\n",
      "0 \t ['skill'] \t ['sk', 'i', 'll']\n",
      "0 \t ['prevail', 's'] \t ['pre', 'va', 'ils']\n",
      "0.29 \t ['de', 'cent', 'ral', 'isation'] \t ['de', 'centralis', 'ation']\n",
      "0.33 \t ['stor', 'ed'] \t ['st', 'o', 'r', 'ed']\n",
      "0.33 \t ['in', 'fluen', 'za'] \t ['in', 'flu', 'enza']\n",
      "0.5 \t ['margin', 'al', 'is', 'ed'] \t ['mar', 'ginal', 'is', 'ed']\n",
      "0.4 \t ['stay', 'ing'] \t ['sta', 'y', 'ing']\n",
      "0 \t ['intensi', 'ty'] \t ['in', 'tens', 'ity']\n",
      "0.4 \t ['re', 'cast'] \t ['re', 'c', 'ast']\n",
      "0.4 \t ['guide', 'line'] \t ['guid', 'e', 'line']\n",
      "0.33 \t ['out', 'line', 's'] \t ['out', 'l', 'ines']\n",
      "0 \t ['scenario', 's'] \t ['scen', 'ari', 'os']\n",
      "0 \t ['nati', 've'] \t ['n', 'a', 'tive']\n",
      "0.57 \t ['pre', 'vent', 'at', 'ive'] \t ['pre', 'vent', 'ative']\n",
      "0.4 \t ['bath', 'ing'] \t ['ba', 'th', 'ing']\n",
      "0.33 \t ['en', 'danger', 'ed'] \t ['en', 'dang', 'ered']\n",
      "0 \t ['continent', 'al'] \t ['con', 'tin', 'ental']\n",
      "0 \t ['ten', 'th'] \t ['t', 'e', 'nth']\n",
      "0 \t ['vulner', 'abil', 'ity'] \t ['vul', 'ner', 'ability']\n",
      "0.4 \t ['realis', 'ing'] \t ['re', 'alis', 'ing']\n",
      "0 \t ['tight', 'er'] \t ['ti', 'gh', 'ter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46, 0.05, 0.55, 0.25]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "for t in tokenizations:    \n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = tf_tokenizer.tokenize(t[0])\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    if tf_f1 < 1.0:\n",
    "        print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "de1fb1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |\n",
      "|---|---|---|---|---|\n",
      "| ['euro', 'zone'] | ['eu', 'rozone'] | ['eurozone'] | ['euro', 'zone'] | ['euro', 'z', 'one'] |\n",
      "| ['entrepreneur', 'ship'] | ['ent', 're', 'pre', 'neur', 'ship'] | ['entrepreneurship'] | ['entrepreneur', 'ship'] | ['entre', 'preneur', 'sh', 'ip'] |\n",
      "| ['pre', 'sent', 'ly'] | ['pre', 's', 'ent', 'ly'] | ['pres', 'ently'] | ['present', 'ly'] | ['pre', 's', 'ently'] |\n",
      "| ['bloc'] | ['bloc'] | ['blo', 'c'] | ['bl', 'oc'] | ['b', 'lo', 'c'] |\n",
      "| ['tree', 's'] | ['tr', 'ee', 's'] | ['tre', 'es'] | ['tr', 'ees'] | ['tre', 'es'] |\n",
      "| ['multi', 'lateral', 'ism'] | ['multi', 'lat', 'er', 'al', 'ism'] | ['multilater', 'alism'] | ['multilateral', 'ism'] | ['multi', 'later', 'al', 'ism'] |\n",
      "| ['motive', 's'] | ['mot', 'ive', 's'] | ['mo', 'tives'] | ['motiv', 'es'] | ['mo', 'tiv', 'es'] |\n",
      "| ['progress', 'ive', 'ly'] | ['pro', 'gr', 'ess', 'ive', 'ly'] | ['pro', 'gressively'] | ['progressive', 'ly'] | ['pro', 'gressiv', 'ely'] |\n",
      "| ['de', 'cent', 'ral', 'isation'] | ['dec', 'ent', 'r', 'al', 'isation'] | ['decent', 'ralisation'] | ['decent', 'ral', 'isation'] | ['de', 'centralis', 'ation'] |\n",
      "| ['margin', 'al', 'is', 'ed'] | ['marginali', 's', 'ed'] | ['margin', 'alised'] | ['marginal', 'ised'] | ['mar', 'ginal', 'is', 'ed'] |\n",
      "| ['re', 'cast'] | ['re', 'cast'] | ['rec', 'ast'] | ['re', 'cast'] | ['re', 'c', 'ast'] |\n",
      "| ['out', 'line', 's'] | ['out', 'l', 'ine', 's'] | ['out', 'lines'] | ['outline', 's'] | ['out', 'l', 'ines'] |\n",
      "| ['pre', 'vent', 'at', 'ive'] | ['pre', 'v', 'ent', 'ative'] | ['preven', 'tative'] | ['prevent', 'ative'] | ['pre', 'vent', 'ative'] |\n",
      "| ['en', 'danger', 'ed'] | ['end', 'an', 'g', 'er', 'ed'] | ['endang', 'ered'] | ['endanger', 'ed'] | ['en', 'dang', 'ered'] |\n",
      "| ['vulner', 'abil', 'ity'] | ['vulnerabil', 'ity'] | ['vul', 'n', 'era', 'bility'] | ['vul', 'ner', 'ability'] | ['vul', 'ner', 'ability'] |\n",
      "|**F1**|**0.46**|**0.05**|**0.55**|**0.25**|\n"
     ]
    }
   ],
   "source": [
    "print('| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |')\n",
    "print('|---|---|---|---|---|')\n",
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "i = 0\n",
    "for t in tokenizations:\n",
    "    i +=1\n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = tf_tokenizer.tokenize(t[0])\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "    if i % 3 == 0:\n",
    "        print('|',man,'|',ref,'|',bpe,'|',dpe,'|',tf,'|')\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "#print('||**',f1[0],'**|**',f1[1],'**|**',f1[2],'**|**',f1[3],'**|**')\n",
    "print('|**F1**|**{}**|**{}**|**{}**|**{}**|'.format(f1[0],f1[1],f1[2],f1[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3f90c",
   "metadata": {},
   "source": [
    "| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |\n",
    "|---|---|---|---|---|\n",
    "| ['euro', 'zone'] | ['eu', 'rozone'] | ['eurozone'] | ['euro', 'zone'] | ['euro', 'z', 'one'] |\n",
    "| ['entrepreneur', 'ship'] | ['ent', 're', 'pre', 'neur', 'ship'] | ['entrepreneurship'] | ['entrepreneur', 'ship'] | ['entre', 'preneur', 'sh', 'ip'] |\n",
    "| ['pre', 'sent', 'ly'] | ['pre', 's', 'ent', 'ly'] | ['pres', 'ently'] | ['present', 'ly'] | ['pre', 's', 'ently'] |\n",
    "| ['bloc'] | ['bloc'] | ['blo', 'c'] | ['bl', 'oc'] | ['b', 'lo', 'c'] |\n",
    "| ['tree', 's'] | ['tr', 'ee', 's'] | ['tre', 'es'] | ['tr', 'ees'] | ['tre', 'es'] |\n",
    "| ['multi', 'lateral', 'ism'] | ['multi', 'lat', 'er', 'al', 'ism'] | ['multilater', 'alism'] | ['multilateral', 'ism'] | ['multi', 'later', 'al', 'ism'] |\n",
    "| ['motive', 's'] | ['mot', 'ive', 's'] | ['mo', 'tives'] | ['motiv', 'es'] | ['mo', 'tiv', 'es'] |\n",
    "| ['progress', 'ive', 'ly'] | ['pro', 'gr', 'ess', 'ive', 'ly'] | ['pro', 'gressively'] | ['progressive', 'ly'] | ['pro', 'gressiv', 'ely'] |\n",
    "| ['de', 'cent', 'ral', 'isation'] | ['dec', 'ent', 'r', 'al', 'isation'] | ['decent', 'ralisation'] | ['decent', 'ral', 'isation'] | ['de', 'centralis', 'ation'] |\n",
    "| ['margin', 'al', 'is', 'ed'] | ['marginali', 's', 'ed'] | ['margin', 'alised'] | ['marginal', 'ised'] | ['mar', 'ginal', 'is', 'ed'] |\n",
    "| ['re', 'cast'] | ['re', 'cast'] | ['rec', 'ast'] | ['re', 'cast'] | ['re', 'c', 'ast'] |\n",
    "| ['out', 'line', 's'] | ['out', 'l', 'ine', 's'] | ['out', 'lines'] | ['outline', 's'] | ['out', 'l', 'ines'] |\n",
    "| ['pre', 'vent', 'at', 'ive'] | ['pre', 'v', 'ent', 'ative'] | ['preven', 'tative'] | ['prevent', 'ative'] | ['pre', 'vent', 'ative'] |\n",
    "| ['en', 'danger', 'ed'] | ['end', 'an', 'g', 'er', 'ed'] | ['endang', 'ered'] | ['endanger', 'ed'] | ['en', 'dang', 'ered'] |\n",
    "| ['vulner', 'abil', 'ity'] | ['vulnerabil', 'ity'] | ['vul', 'n', 'era', 'bility'] | ['vul', 'ner', 'ability'] | ['vul', 'ner', 'ability'] |\n",
    "|**F1**|**0.46**|**0.05**|**0.55**|**0.25**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "56bd1f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.5 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.7 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.9 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.95 [0.46, 0.05, 0.55, 0.09]\n",
      "[2] 0.5 [0.46, 0.05, 0.55, 0.12]\n",
      "[2] 0.7 [0.46, 0.05, 0.55, 0.13]\n",
      "[2] 0.9 [0.46, 0.05, 0.55, 0.14]\n",
      "[2] 0.95 [0.46, 0.05, 0.55, 0.14]\n",
      "[3] 0.5 [0.46, 0.05, 0.55, 0.18]\n",
      "[3] 0.7 [0.46, 0.05, 0.55, 0.2]\n",
      "[3] 0.9 [0.46, 0.05, 0.55, 0.21]\n",
      "[3] 0.95 [0.46, 0.05, 0.55, 0.18]\n",
      "[4] 0.5 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.7 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.9 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.95 [0.46, 0.05, 0.55, 0.24]\n",
      "[5] 0.5 [0.46, 0.05, 0.55, 0.24]\n",
      "[5] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[5] 0.9 [0.46, 0.05, 0.55, 0.27]\n",
      "[5] 0.95 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.5 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.9 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.95 [0.46, 0.05, 0.55, 0.24]\n",
      "[7] 0.5 [0.46, 0.05, 0.55, 0.26]\n",
      "[7] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[7] 0.9 [0.46, 0.05, 0.55, 0.25]\n",
      "[7] 0.95 [0.46, 0.05, 0.55, 0.24]\n"
     ]
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "    for th in [0.5,0.7,0.9,0.95]:\n",
    "        tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "        f1 = [0,0,0,0]\n",
    "        for t in tokenizations:    \n",
    "            man = t[1] # manual\n",
    "            ref = en_ref_tokenizer.tokenize(t[0])\n",
    "            bpe = t[2]\n",
    "            dpe = t[3]\n",
    "            tf = tf_tokenizer.tokenize(t[0])\n",
    "            ref_f1 = calc_f1(man,ref)\n",
    "            bpe_f1 = calc_f1(man,bpe)\n",
    "            dpe_f1 = calc_f1(man,dpe)\n",
    "            tf_f1 = calc_f1(man,tf)\n",
    "            #if tf_f1 < 1.0:\n",
    "            #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "            f1[0] += ref_f1\n",
    "            f1[1] += bpe_f1\n",
    "            f1[2] += dpe_f1\n",
    "            f1[3] += tf_f1\n",
    "        f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "        print(n,th,f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1b497",
   "metadata": {},
   "source": [
    "### Check if limiting training set by word frequency helps to improve morho-parsing F1 (0.0005-0.001 is the best)\n",
    "\n",
    "#### Training in https://github.com/aigents/pygents/blob/main/notebooks/nlp/morphology/morphology_lexicon_en_ru.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8e09d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/lex_en_counted_10 [0.46, 0.05, 0.55, 0.24]\n",
      "data/models/lex_en_counted_10_0005 [0.46, 0.05, 0.55, 0.28]\n",
      "data/models/lex_en_counted_10_001 [0.46, 0.05, 0.55, 0.28]\n",
      "data/models/lex_en_counted_10_005 [0.46, 0.05, 0.55, 0.21]\n",
      "data/models/lex_en_counted_10_01 [0.46, 0.05, 0.55, 0.08]\n"
     ]
    }
   ],
   "source": [
    "for model in ['data/models/lex_en_counted_10','data/models/lex_en_counted_10_0005','data/models/lex_en_counted_10_001',\n",
    "              'data/models/lex_en_counted_10_005','data/models/lex_en_counted_10_01']:\n",
    "    best_tf_f1 = 0\n",
    "    best_f1 = None\n",
    "    base = FreedomTokenizer(name=model,max_n=10,mode='chars',debug=False)\n",
    "    tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "    for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "        for th in [0.5,0.7,0.9,0.95]:\n",
    "            tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "            f1 = [0,0,0,0]\n",
    "            for t in tokenizations:    \n",
    "                man = t[1] # manual\n",
    "                ref = en_ref_tokenizer.tokenize(t[0])\n",
    "                bpe = t[2]\n",
    "                dpe = t[3]\n",
    "                tf = tf_tokenizer.tokenize(t[0])\n",
    "                ref_f1 = calc_f1(man,ref)\n",
    "                bpe_f1 = calc_f1(man,bpe)\n",
    "                dpe_f1 = calc_f1(man,dpe)\n",
    "                tf_f1 = calc_f1(man,tf)\n",
    "                #if tf_f1 < 1.0:\n",
    "                #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "                f1[0] += ref_f1\n",
    "                f1[1] += bpe_f1\n",
    "                f1[2] += dpe_f1\n",
    "                f1[3] += tf_f1\n",
    "            f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "            #print(n,th,f1)\n",
    "            if best_tf_f1 < f1[3]:\n",
    "                best_tf_f1 = f1[3]\n",
    "                best_f1 = f1\n",
    "    print(model,f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "300a0519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1257863 [0.46, 0.05, 0.55, 0.24]\n",
      "0.0001 923736 [0.46, 0.05, 0.55, 0.24]\n",
      "0.0005 899442 [0.46, 0.05, 0.55, 0.24]\n",
      "0.001 896201 [0.46, 0.05, 0.55, 0.23]\n",
      "0.005 891853 [0.46, 0.05, 0.55, 0.24]\n",
      "0.01 888733 [0.46, 0.05, 0.55, 0.25]\n",
      "0.05 863488 [0.46, 0.05, 0.55, 0.24]\n",
      "0.1 834719 [0.46, 0.05, 0.55, 0.27]\n",
      "0.5 689472 [0.46, 0.05, 0.55, 0.19]\n"
     ]
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_nocount_7',max_n=7,mode='chars',debug=False)\n",
    "for model_threshold in [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5]:\n",
    "    if model_threshold > 0:\n",
    "        model_compress_with_loss(base.model,model_threshold)\n",
    "    best_tf_f1 = 0\n",
    "    best_f1 = None\n",
    "    tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "    for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "        for th in [0.5,0.7,0.9,0.95]:\n",
    "            tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "            f1 = [0,0,0,0]\n",
    "            for t in tokenizations:    \n",
    "                man = t[1] # manual\n",
    "                ref = en_ref_tokenizer.tokenize(t[0])\n",
    "                bpe = t[2]\n",
    "                dpe = t[3]\n",
    "                tf = tf_tokenizer.tokenize(t[0])\n",
    "                ref_f1 = calc_f1(man,ref)\n",
    "                bpe_f1 = calc_f1(man,bpe)\n",
    "                dpe_f1 = calc_f1(man,dpe)\n",
    "                tf_f1 = calc_f1(man,tf)\n",
    "                #if tf_f1 < 1.0:\n",
    "                #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "                f1[0] += ref_f1\n",
    "                f1[1] += bpe_f1\n",
    "                f1[2] += dpe_f1\n",
    "                f1[3] += tf_f1\n",
    "            f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "            #print(n,th,f1)\n",
    "            if best_tf_f1 < f1[3]:\n",
    "                best_tf_f1 = f1[3]\n",
    "                best_f1 = f1\n",
    "    print(model_threshold,base.count_params(),f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83f5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11fcf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a8923d",
   "metadata": {},
   "source": [
    "## Experiment with MI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a99e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = FreedomTokenizer(name='data/models/brown_nolines_chars_7a',max_n=7,mode='chars',debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "591cda57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381728"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.model[0]['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5d402e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, log\n",
    "def root_n(x,n):\n",
    "    return exp(log(x)/n)\n",
    "\n",
    "def eval_split_log_div_cnt(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 1\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        cnt /= len(t)\n",
    "        v = math.log2(cnt)\n",
    "        p = p * v\n",
    "    return p\n",
    "\n",
    "def eval_split_root(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 1.0\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        p = p * cnt * len(t)\n",
    "    if debug:\n",
    "        print(p)\n",
    "    return p if p == 0 or len(split) == 1 else root_n(p,len(split))\n",
    "    \n",
    "def eval_avg_log(base,split,extra_len_discount=False,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 0.0\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        if extra_len_discount:\n",
    "            p = p + math.log(cnt*len(t)*len(t)+1)\n",
    "        else:\n",
    "            p = p + math.log(cnt*len(t)+1)\n",
    "    if debug:\n",
    "        print(p)\n",
    "    return p / len(split)\n",
    "    \n",
    "def eval_splits(base,splits,extra_len_discount=False,debug=True):\n",
    "    emax = 0\n",
    "    best = None\n",
    "    for split in splits:\n",
    "        e = eval_avg_log(base,split,extra_len_discount=extra_len_discount)\n",
    "        if emax < e:\n",
    "            emax = e\n",
    "            best = split\n",
    "        if debug:\n",
    "            print(round(e,2),split)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c0bb928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.04 ['lighter']\n",
      "15.39 ['l', 'ighter']\n",
      "15.4 ['li', 'ghter']\n",
      "14.1 ['lig', 'hter']\n",
      "15.94 ['ligh', 'ter']\n",
      "16.71 ['light', 'er']\n",
      "16.07 ['lighte', 'r']\n",
      "17.21 ['l', 'i', 'ghter']\n",
      "16.17 ['l', 'ig', 'hter']\n",
      "17.41 ['l', 'igh', 'ter']\n",
      "17.85 ['l', 'ight', 'er']\n",
      "17.28 ['l', 'ighte', 'r']\n",
      "16.21 ['li', 'g', 'hter']\n",
      "17.0 ['li', 'gh', 'ter']\n",
      "17.38 ['li', 'ght', 'er']\n",
      "16.88 ['li', 'ghte', 'r']\n",
      "17.08 ['lig', 'h', 'ter']\n",
      "16.45 ['lig', 'ht', 'er']\n",
      "16.0 ['lig', 'hte', 'r']\n",
      "17.6 ['ligh', 't', 'er']\n",
      "17.3 ['ligh', 'te', 'r']\n",
      "18.0 ['light', 'e', 'r']\n",
      "['light', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0f736612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.99 ['lighter']\n",
      "16.28 ['l', 'ighter']\n",
      "16.55 ['li', 'ghter']\n",
      "15.35 ['lig', 'hter']\n",
      "17.18 ['ligh', 'ter']\n",
      "17.86 ['light', 'er']\n",
      "16.97 ['lighte', 'r']\n",
      "17.75 ['l', 'i', 'ghter']\n",
      "16.86 ['l', 'ig', 'hter']\n",
      "18.14 ['l', 'igh', 'ter']\n",
      "18.54 ['l', 'ight', 'er']\n",
      "17.82 ['l', 'ighte', 'r']\n",
      "16.9 ['li', 'g', 'hter']\n",
      "17.83 ['li', 'gh', 'ter']\n",
      "18.21 ['li', 'ght', 'er']\n",
      "17.57 ['li', 'ghte', 'r']\n",
      "17.81 ['lig', 'h', 'ter']\n",
      "17.28 ['lig', 'ht', 'er']\n",
      "16.73 ['lig', 'hte', 'r']\n",
      "18.3 ['ligh', 't', 'er']\n",
      "17.99 ['ligh', 'te', 'r']\n",
      "18.53 ['light', 'e', 'r']\n",
      "['l', 'ight', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r']],extra_len_discount=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "89fb8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.34 ['t', 'ighter']\n",
      "10.28 ['ti', 'ghter']\n",
      "7.99 ['tig', 'hter']\n",
      "9.28 ['tigh', 'ter']\n",
      "9.99 ['tight', 'er']\n",
      "9.53 ['tighte', 'r']\n",
      "['t', 'ighter']\n",
      "9.93 ['l', 'ighter']\n",
      "9.96 ['li', 'ghter']\n",
      "8.8 ['lig', 'hter']\n",
      "10.65 ['ligh', 'ter']\n",
      "11.36 ['light', 'er']\n",
      "10.38 ['lighte', 'r']\n",
      "['light', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(base,[['t', 'ighter'],['ti', 'ghter'],['tig', 'hter'],['tigh', 'ter'],['tight', 'er'],['tighte', 'r']]))\n",
    "print(eval_splits(base,[['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ebaed72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.6 ['t', 'ighter']\n",
      "31.57 ['ti', 'ghter']\n",
      "26.76 ['tig', 'hter']\n",
      "29.18 ['tigh', 'ter']\n",
      "30.72 ['tight', 'er']\n",
      "30.12 ['tighte', 'r']\n",
      "['t', 'ighter']\n",
      "30.78 ['l', 'ighter']\n",
      "30.8 ['li', 'ghter']\n",
      "28.21 ['lig', 'hter']\n",
      "31.87 ['ligh', 'ter']\n",
      "33.41 ['light', 'er']\n",
      "32.14 ['lighte', 'r']\n",
      "['light', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['t', 'ighter'],['ti', 'ghter'],['tig', 'hter'],['tigh', 'ter'],['tight', 'er'],['tighte', 'r']]))\n",
    "print(eval_splits(lex_en_base10,[['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cdf02a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.08 ['lighter']\n",
      "30.78 ['l', 'ighter']\n",
      "30.8 ['li', 'ghter']\n",
      "28.21 ['lig', 'hter']\n",
      "31.87 ['ligh', 'ter']\n",
      "33.41 ['light', 'er']\n",
      "32.14 ['lighte', 'r']\n",
      "34.42 ['l', 'i', 'ghter']\n",
      "32.33 ['l', 'ig', 'hter']\n",
      "34.81 ['l', 'igh', 'ter']\n",
      "35.69 ['l', 'ight', 'er']\n",
      "34.57 ['l', 'ighte', 'r']\n",
      "32.41 ['li', 'g', 'hter']\n",
      "33.99 ['li', 'gh', 'ter']\n",
      "34.76 ['li', 'ght', 'er']\n",
      "33.76 ['li', 'ghte', 'r']\n",
      "34.16 ['lig', 'h', 'ter']\n",
      "32.9 ['lig', 'ht', 'er']\n",
      "31.99 ['lig', 'hte', 'r']\n",
      "35.2 ['ligh', 't', 'er']\n",
      "34.59 ['ligh', 'te', 'r']\n",
      "35.99 ['light', 'e', 'r']\n",
      "['light', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[\n",
    "    ['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],\n",
    "    ['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r'],\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bbf273d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1587747904004283e+21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10503475.162330275"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_split_root(lex_en_base10,['l', 'ig','hter'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7513297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92173cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da88c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7482685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
