{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fa579f",
   "metadata": {},
   "source": [
    "# Exploring sybword segmentation based on BPE, DPE, morphology and \"transition freedom\" \n",
    "\n",
    "### Test coprus based on https://arxiv.org/pdf/2005.06606.pdf (with numbers removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "24b9d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "#from importlib import reload  # Python 3.4+\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "if 'pygents.token_plot' in sys.modules:\n",
    "    del sys.modules['pygents.token_plot']\n",
    "\n",
    "\n",
    "from pygents.token import *\n",
    "from pygents.text import *\n",
    "from pygents.util import *\n",
    "from pygents.plot import plot_bars, plot_dict, matrix_plot\n",
    "from pygents.token_plot import *\n",
    "\n",
    "#TODO move to model loading/setting\n",
    "def update_grand_counts(base):\n",
    "    grand_counts = {}\n",
    "    counts = base.model[0]\n",
    "    for e in counts:\n",
    "        n = len(e)\n",
    "        if not n in grand_counts:\n",
    "            grand_counts[n] = 0\n",
    "        grand_counts[n] += counts[e]\n",
    "    base.grand_counts = grand_counts\n",
    "\n",
    "def gram_tf(base,gram):\n",
    "    m = base.model\n",
    "    tf1 = len(m[1][gram]) if gram in m[1] else 0\n",
    "    tf2 = len(m[2][gram]) if gram in m[2] else 0\n",
    "    return tf1+tf2\n",
    "\n",
    "def update_max_freedoms(base):\n",
    "    max_freedoms = {}\n",
    "    counts = base.model[0]\n",
    "    for e in counts:\n",
    "        n = len(e)\n",
    "        if not n in max_freedoms:\n",
    "            max_freedoms[n] = 0\n",
    "        tf = gram_tf(base,e)\n",
    "        if max_freedoms[n] < tf:\n",
    "            max_freedoms[n] = tf\n",
    "    base.max_freedoms = max_freedoms\n",
    "\n",
    "def chunk_tf(base,gram):\n",
    "    m = base.model\n",
    "    tf1 = len(m[1][gram]) if gram in m[1] else 0\n",
    "    tf2 = len(m[2][gram]) if gram in m[2] else 0\n",
    "    return (tf1+tf2) / base.max_freedoms[len(gram)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "2af325ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, log\n",
    "\n",
    "def root_n(x,n):\n",
    "    return exp(log(x)/n)\n",
    "\n",
    "def eval_split_log_div_cnt(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 1\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        cnt /= len(t)\n",
    "        v = math.log2(cnt)\n",
    "        p = p * v\n",
    "    return p\n",
    "\n",
    "def eval_split_root(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 1.0\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        p = p * cnt * len(t)\n",
    "    if debug:\n",
    "        print(p)\n",
    "    return p if p == 0 or len(split) == 1 else root_n(p,len(split))\n",
    "    \n",
    "def eval_avg_log(base,split,extra_len_discount=False,debug=False):\n",
    "    f = base.model[0]\n",
    "    p = 0.0\n",
    "    for t in split:\n",
    "        cnt = f[t] if t in f else 0\n",
    "        if extra_len_discount:\n",
    "            p = p + math.log(cnt*len(t)*len(t)+1)\n",
    "        else:\n",
    "            p = p + math.log(cnt*len(t)+1)\n",
    "    if debug:\n",
    "        print(p)\n",
    "    return p / len(split)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Mutual_information\n",
    "def eval_mi_chunk(base,chunk,log=True,debug=False):\n",
    "    f = base.model[0]\n",
    "    joint_f = f[chunk]/base.grand_counts[len(chunk)] if chunk in f else 0 # P(A,B,...)\n",
    "    singles = list(chunk)\n",
    "    p = 1.0\n",
    "    for single in singles:\n",
    "        single_f = f[single] if single in f else 0\n",
    "        p *= single_f/base.grand_counts[1] # P(A)\n",
    "    if debug:\n",
    "        print(chunk,joint_f,singles,p)\n",
    "    #return math.log(joint_f/p) if log else joint_f/p # joing_f == 0 => Math Domain Error\n",
    "    return joint_f * math.log(joint_f/p) if log and joint_f > 0 else joint_f/p # cheating...\n",
    "    #return math.log(joint_f/p) if log and joint_f > 0 else joint_f/p # cheating...\n",
    "\n",
    "#TODO try multiplication instead!?\n",
    "\n",
    "def eval_mi_sum(base,split,debug=False):\n",
    "    f = base.model[0]\n",
    "    sum_mi = 0\n",
    "    for t in split:\n",
    "        mi = eval_mi_chunk(base,t,debug=False)\n",
    "        sum_mi += mi\n",
    "    if debug:\n",
    "        print(sum_mi)\n",
    "    return sum_mi\n",
    "    \n",
    "def eval_splits(base,splits,extra_len_discount=False,debug=False):\n",
    "    emax = 0\n",
    "    best = None\n",
    "    for split in splits:\n",
    "        #e = eval_avg_log(base,split,extra_len_discount=extra_len_discount)\n",
    "        e = eval_mi_sum(base,split,debug=False)\n",
    "        if emax < e:\n",
    "            emax = e\n",
    "            best = split\n",
    "        if debug:\n",
    "            print(round(e,2),split)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44a819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9dc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379853b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75668c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ceb6082",
   "metadata": {},
   "source": [
    "## Baseline experiment comapring manual, prefix/suffix-based, BPE, DPE and TF-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a5a55345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97565\n",
      "('the', 53097401)\n",
      "97565\n"
     ]
    }
   ],
   "source": [
    "#get raw lexicon list\n",
    "en_lex = list(pd.read_csv(\"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\",sep='\\t',header=None,na_filter=False).to_records(index=False))\n",
    "print(len(en_lex))\n",
    "\n",
    "#debug raw lexicon\n",
    "print(max(en_lex,key=lambda item:item[1]))\n",
    "en_lex_dict = weightedlist2dict(en_lex,lower=True) # no case-insensitive merge\n",
    "print(len(en_lex_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d7a986b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684498\n"
     ]
    }
   ],
   "source": [
    "lex_en_base10 = FreedomTokenizer(max_n=10,mode='chars',debug=False)\n",
    "lex_en_base10.train(en_lex_dict)\n",
    "lex_en_base10.store('data/models/lex_en_counted_10')\n",
    "print(lex_en_base10.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c083a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref_tokenizer = PrefixSuffixMorphoTokenizerCached([\"./data/corpora/English/morphology/prefixes.txt\"],\n",
    "                                   [\"./data/corpora/English/morphology/suffixes.txt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "35e51d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inter', 'est', 'ing', 'ly']\n",
      "['uni', 'v', 'er', 's', 'ities']\n",
      "['anti', 'dis', 'establ', 'ish', 'ment', 'arian', 'ism']\n",
      "['dec', 'ent', 'rali', 's', 'ations']\n",
      "['c', 'ities']\n",
      "['p', 'ing']\n"
     ]
    }
   ],
   "source": [
    "texts = ['interestingly', # ['inter', 'est', 'ing', 'ly']\n",
    "         'universities',\n",
    "         'antidisestablishmentarianism', # anti-dis-establish-ment-ar-i-an-ism\n",
    "         'decentralisations',\n",
    "         'cities',\n",
    "         'ping']\n",
    "for text in texts:\n",
    "    print(en_ref_tokenizer.tokenize(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c520a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ad25d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test coprus based on https://arxiv.org/pdf/2005.06606.pdf\n",
    "# columns: 1 manual, 2 BPE, 3 DPE \n",
    "tokenizations =[\n",
    "[['re','cogn','ise','s'],['recognises'],['recognise','s']],\n",
    "[['advocate','s'],['advocates'],['advocate','s']],\n",
    "[['euro','zone'],['eurozone'],['euro','zone']],\n",
    "[['under','line','s'],['underlines'],['underline','s']],\n",
    "[['strength','en','s'],['strengthens'],['strengthen','s']],\n",
    "[['entrepreneur','ship'],['entrepreneurship'],['entrepreneur','ship']],\n",
    "[['ac','knowledge','s'],['acknowledges'],['acknowledge','s']],\n",
    "[['wine','s'],['wines'],['wine','s']],\n",
    "[['pre','sent','ly'],['pres','ently'],['present','ly']],\n",
    "[['fill','ed'],['f','illed'],['fill','ed']],\n",
    "[['en','dorse','ment'],['endors','ement'],['endorse','ment']],\n",
    "[['bloc'],['blo','c'],['bl','oc']],\n",
    "[['crucial','ly'],['cru','cially'],['crucial','ly']],\n",
    "[['eval','u','ation','s'],['eval','uations'],['evaluation','s']],\n",
    "[['tree','s'],['tre','es'],['tr','ees']],\n",
    "[['ticket','s'],['tick','ets'],['tick','et','s']],\n",
    "[['pre','dict','able'],['predic','table'],['predict','able']],\n",
    "[['multi','lateral','ism'],['multilater','alism'],['multilateral','ism']],\n",
    "[['rat','ing','s'],['rat','ings'],['rating','s']],\n",
    "[['pre','dict','ed'],['predic','ted'],['predict','ed']],\n",
    "[['motive','s'],['mo','tives'],['motiv','es']],\n",
    "[['re','in','force','s'],['reinfor','ces'],['reinforce','s']],\n",
    "[['proto','col','s'],['pro','tocols'],['protocol','s']],\n",
    "[['progress','ive','ly'],['pro','gressively'],['progressive','ly']],\n",
    "[['skill'],['sk','ill'],['ski','ll']],\n",
    "[['prevail','s'],['preva','ils'],['prevail','s']],\n",
    "[['de','cent','ral','isation'],['decent','ralisation'],['decent','ral','isation']],\n",
    "[['stor','ed'],['sto','red'],['stor','ed']],\n",
    "[['in','fluen','za'],['influ','enz','a'],['influen','za']],\n",
    "[['margin','al','is','ed'],['margin','alised'],['marginal','ised']],\n",
    "[['stay','ing'],['sta','ying'],['stay','ing']],\n",
    "[['intensi','ty'],['intens','ity'],['intensi','ty']],\n",
    "[['re','cast'],['rec','ast'],['re','cast']],\n",
    "[['guide','line'],['guid','eline'],['guide','line']],\n",
    "[['em','bark','ed'],['emb','arked'],['embark','ed']],\n",
    "[['out','line','s'],['out','lines'],['outline','s']],\n",
    "[['scenario','s'],['scen','ari','os'],['scenario','s']],\n",
    "[['nati','ve'],['n','ative'],['na','tive']],\n",
    "[['pre','vent','at','ive'],['preven','tative'],['prevent','ative']],\n",
    "[['home','land'],['hom','eland'],['home','land']],\n",
    "[['bath','ing'],['bat','hing'],['bath','ing']],\n",
    "[['en','danger','ed'],['endang','ered'],['endanger','ed']],\n",
    "[['continent','al'],['cont','inen','tal'],['continent','al']],\n",
    "[['ten','th'],['t','enth'],['ten','th']],\n",
    "[['vulner','abil','ity'],['vul','n','era','bility'],['vul','ner','ability']],\n",
    "[['realis','ing'],['realis','ing'],['real','ising']],\n",
    "[['tight','er'],['t','ighter'],['tight','er']]\n",
    "]\n",
    "tokenizations = [[''.join(i[0]),i[0],i[1],i[2]] for i in tokenizations]\n",
    "for i in tokenizations:\n",
    "    assert len(i)==4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b530cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57 \t ['re', 'cogn', 'ise', 's'] \t ['re', 'cogn', 'ises']\n",
      "0 \t ['advocate', 's'] \t ['ad', 'voc', 'ates']\n",
      "0.4 \t ['euro', 'zone'] \t ['euro', 'z', 'one']\n",
      "0.33 \t ['under', 'line', 's'] \t ['under', 'l', 'ines']\n",
      "0 \t ['strength', 'en', 's'] \t ['stre', 'ngth', 'ens']\n",
      "0 \t ['entrepreneur', 'ship'] \t ['entre', 'preneur', 'sh', 'ip']\n",
      "0 \t ['ac', 'knowledge', 's'] \t ['ack', 'nowledg', 'es']\n",
      "0 \t ['wine', 's'] \t ['w', 'i', 'n', 'es']\n",
      "0.33 \t ['pre', 'sent', 'ly'] \t ['pre', 's', 'ently']\n",
      "0 \t ['fill', 'ed'] \t ['f', 'i', 'lled']\n",
      "0 \t ['en', 'dorse', 'ment'] \t ['endo', 'rsem', 'ent']\n",
      "0 \t ['bloc'] \t ['b', 'lo', 'c']\n",
      "0.33 \t ['crucial', 'ly'] \t ['cru', 'ci', 'al', 'ly']\n",
      "0 \t ['eval', 'u', 'ation', 's'] \t ['eva', 'lu', 'ations']\n",
      "0 \t ['tree', 's'] \t ['tre', 'es']\n",
      "0 \t ['ticket', 's'] \t ['ti', 'ck', 'ets']\n",
      "0.57 \t ['multi', 'lateral', 'ism'] \t ['multi', 'later', 'al', 'ism']\n",
      "0 \t ['rat', 'ing', 's'] \t ['ra', 't', 'i', 'ngs']\n",
      "0 \t ['motive', 's'] \t ['mo', 'tiv', 'es']\n",
      "0.29 \t ['re', 'in', 'force', 's'] \t ['re', 'inforc', 'es']\n",
      "0 \t ['proto', 'col', 's'] \t ['pro', 'toc', 'ols']\n",
      "0 \t ['progress', 'ive', 'ly'] \t ['pro', 'gressiv', 'ely']\n",
      "0 \t ['skill'] \t ['sk', 'i', 'll']\n",
      "0 \t ['prevail', 's'] \t ['pre', 'va', 'ils']\n",
      "0.29 \t ['de', 'cent', 'ral', 'isation'] \t ['de', 'centralis', 'ation']\n",
      "0.33 \t ['stor', 'ed'] \t ['st', 'o', 'r', 'ed']\n",
      "0.33 \t ['in', 'fluen', 'za'] \t ['in', 'flu', 'enza']\n",
      "0.5 \t ['margin', 'al', 'is', 'ed'] \t ['mar', 'ginal', 'is', 'ed']\n",
      "0.4 \t ['stay', 'ing'] \t ['sta', 'y', 'ing']\n",
      "0 \t ['intensi', 'ty'] \t ['in', 'tens', 'ity']\n",
      "0.4 \t ['re', 'cast'] \t ['re', 'c', 'ast']\n",
      "0.4 \t ['guide', 'line'] \t ['guid', 'e', 'line']\n",
      "0.33 \t ['out', 'line', 's'] \t ['out', 'l', 'ines']\n",
      "0 \t ['scenario', 's'] \t ['scen', 'ari', 'os']\n",
      "0 \t ['nati', 've'] \t ['n', 'a', 'tive']\n",
      "0.57 \t ['pre', 'vent', 'at', 'ive'] \t ['pre', 'vent', 'ative']\n",
      "0.4 \t ['bath', 'ing'] \t ['ba', 'th', 'ing']\n",
      "0.33 \t ['en', 'danger', 'ed'] \t ['en', 'dang', 'ered']\n",
      "0 \t ['continent', 'al'] \t ['con', 'tin', 'ental']\n",
      "0 \t ['ten', 'th'] \t ['t', 'e', 'nth']\n",
      "0 \t ['vulner', 'abil', 'ity'] \t ['vul', 'ner', 'ability']\n",
      "0.4 \t ['realis', 'ing'] \t ['re', 'alis', 'ing']\n",
      "0 \t ['tight', 'er'] \t ['ti', 'gh', 'ter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46, 0.05, 0.55, 0.25]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "for t in tokenizations:    \n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = tf_tokenizer.tokenize(t[0])\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    if tf_f1 < 1.0:\n",
    "        print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d3b6946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |\n",
      "|---|---|---|---|---|\n",
      "| ['euro', 'zone'] | ['eu', 'rozone'] | ['eurozone'] | ['euro', 'zone'] | ['euro', 'z', 'one'] |\n",
      "| ['entrepreneur', 'ship'] | ['ent', 're', 'pre', 'neur', 'ship'] | ['entrepreneurship'] | ['entrepreneur', 'ship'] | ['entre', 'preneur', 'sh', 'ip'] |\n",
      "| ['pre', 'sent', 'ly'] | ['pre', 's', 'ent', 'ly'] | ['pres', 'ently'] | ['present', 'ly'] | ['pre', 's', 'ently'] |\n",
      "| ['bloc'] | ['bloc'] | ['blo', 'c'] | ['bl', 'oc'] | ['b', 'lo', 'c'] |\n",
      "| ['tree', 's'] | ['tr', 'ee', 's'] | ['tre', 'es'] | ['tr', 'ees'] | ['tre', 'es'] |\n",
      "| ['multi', 'lateral', 'ism'] | ['multi', 'lat', 'er', 'al', 'ism'] | ['multilater', 'alism'] | ['multilateral', 'ism'] | ['multi', 'later', 'al', 'ism'] |\n",
      "| ['motive', 's'] | ['mot', 'ive', 's'] | ['mo', 'tives'] | ['motiv', 'es'] | ['mo', 'tiv', 'es'] |\n",
      "| ['progress', 'ive', 'ly'] | ['pro', 'gr', 'ess', 'ive', 'ly'] | ['pro', 'gressively'] | ['progressive', 'ly'] | ['pro', 'gressiv', 'ely'] |\n",
      "| ['de', 'cent', 'ral', 'isation'] | ['dec', 'ent', 'r', 'al', 'isation'] | ['decent', 'ralisation'] | ['decent', 'ral', 'isation'] | ['de', 'centralis', 'ation'] |\n",
      "| ['margin', 'al', 'is', 'ed'] | ['marginali', 's', 'ed'] | ['margin', 'alised'] | ['marginal', 'ised'] | ['mar', 'ginal', 'is', 'ed'] |\n",
      "| ['re', 'cast'] | ['re', 'cast'] | ['rec', 'ast'] | ['re', 'cast'] | ['re', 'c', 'ast'] |\n",
      "| ['out', 'line', 's'] | ['out', 'l', 'ine', 's'] | ['out', 'lines'] | ['outline', 's'] | ['out', 'l', 'ines'] |\n",
      "| ['pre', 'vent', 'at', 'ive'] | ['pre', 'v', 'ent', 'ative'] | ['preven', 'tative'] | ['prevent', 'ative'] | ['pre', 'vent', 'ative'] |\n",
      "| ['en', 'danger', 'ed'] | ['end', 'an', 'g', 'er', 'ed'] | ['endang', 'ered'] | ['endanger', 'ed'] | ['en', 'dang', 'ered'] |\n",
      "| ['vulner', 'abil', 'ity'] | ['vulnerabil', 'ity'] | ['vul', 'n', 'era', 'bility'] | ['vul', 'ner', 'ability'] | ['vul', 'ner', 'ability'] |\n",
      "|**F1**|**0.46**|**0.05**|**0.55**|**0.25**|\n"
     ]
    }
   ],
   "source": [
    "print('| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |')\n",
    "print('|---|---|---|---|---|')\n",
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "i = 0\n",
    "for t in tokenizations:\n",
    "    i +=1\n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = tf_tokenizer.tokenize(t[0])\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "    if i % 3 == 0:\n",
    "        print('|',man,'|',ref,'|',bpe,'|',dpe,'|',tf,'|')\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "#print('||**',f1[0],'**|**',f1[1],'**|**',f1[2],'**|**',f1[3],'**|**')\n",
    "print('|**F1**|**{}**|**{}**|**{}**|**{}**|'.format(f1[0],f1[1],f1[2],f1[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befebe1",
   "metadata": {},
   "source": [
    "| Reference | Morphology-based | BPE | DPE | Transtion-freedom-based |\n",
    "|---|---|---|---|---|\n",
    "| ['euro', 'zone'] | ['eu', 'rozone'] | ['eurozone'] | ['euro', 'zone'] | ['euro', 'z', 'one'] |\n",
    "| ['entrepreneur', 'ship'] | ['ent', 're', 'pre', 'neur', 'ship'] | ['entrepreneurship'] | ['entrepreneur', 'ship'] | ['entre', 'preneur', 'sh', 'ip'] |\n",
    "| ['pre', 'sent', 'ly'] | ['pre', 's', 'ent', 'ly'] | ['pres', 'ently'] | ['present', 'ly'] | ['pre', 's', 'ently'] |\n",
    "| ['bloc'] | ['bloc'] | ['blo', 'c'] | ['bl', 'oc'] | ['b', 'lo', 'c'] |\n",
    "| ['tree', 's'] | ['tr', 'ee', 's'] | ['tre', 'es'] | ['tr', 'ees'] | ['tre', 'es'] |\n",
    "| ['multi', 'lateral', 'ism'] | ['multi', 'lat', 'er', 'al', 'ism'] | ['multilater', 'alism'] | ['multilateral', 'ism'] | ['multi', 'later', 'al', 'ism'] |\n",
    "| ['motive', 's'] | ['mot', 'ive', 's'] | ['mo', 'tives'] | ['motiv', 'es'] | ['mo', 'tiv', 'es'] |\n",
    "| ['progress', 'ive', 'ly'] | ['pro', 'gr', 'ess', 'ive', 'ly'] | ['pro', 'gressively'] | ['progressive', 'ly'] | ['pro', 'gressiv', 'ely'] |\n",
    "| ['de', 'cent', 'ral', 'isation'] | ['dec', 'ent', 'r', 'al', 'isation'] | ['decent', 'ralisation'] | ['decent', 'ral', 'isation'] | ['de', 'centralis', 'ation'] |\n",
    "| ['margin', 'al', 'is', 'ed'] | ['marginali', 's', 'ed'] | ['margin', 'alised'] | ['marginal', 'ised'] | ['mar', 'ginal', 'is', 'ed'] |\n",
    "| ['re', 'cast'] | ['re', 'cast'] | ['rec', 'ast'] | ['re', 'cast'] | ['re', 'c', 'ast'] |\n",
    "| ['out', 'line', 's'] | ['out', 'l', 'ine', 's'] | ['out', 'lines'] | ['outline', 's'] | ['out', 'l', 'ines'] |\n",
    "| ['pre', 'vent', 'at', 'ive'] | ['pre', 'v', 'ent', 'ative'] | ['preven', 'tative'] | ['prevent', 'ative'] | ['pre', 'vent', 'ative'] |\n",
    "| ['en', 'danger', 'ed'] | ['end', 'an', 'g', 'er', 'ed'] | ['endang', 'ered'] | ['endanger', 'ed'] | ['en', 'dang', 'ered'] |\n",
    "| ['vulner', 'abil', 'ity'] | ['vulnerabil', 'ity'] | ['vul', 'n', 'era', 'bility'] | ['vul', 'ner', 'ability'] | ['vul', 'ner', 'ability'] |\n",
    "|**F1**|**0.46**|**0.05**|**0.55**|**0.25**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f55710b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.5 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.7 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.9 [0.46, 0.05, 0.55, 0.09]\n",
      "[1] 0.95 [0.46, 0.05, 0.55, 0.09]\n",
      "[2] 0.5 [0.46, 0.05, 0.55, 0.12]\n",
      "[2] 0.7 [0.46, 0.05, 0.55, 0.13]\n",
      "[2] 0.9 [0.46, 0.05, 0.55, 0.14]\n",
      "[2] 0.95 [0.46, 0.05, 0.55, 0.14]\n",
      "[3] 0.5 [0.46, 0.05, 0.55, 0.18]\n",
      "[3] 0.7 [0.46, 0.05, 0.55, 0.2]\n",
      "[3] 0.9 [0.46, 0.05, 0.55, 0.21]\n",
      "[3] 0.95 [0.46, 0.05, 0.55, 0.18]\n",
      "[4] 0.5 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.7 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.9 [0.46, 0.05, 0.55, 0.26]\n",
      "[4] 0.95 [0.46, 0.05, 0.55, 0.24]\n",
      "[5] 0.5 [0.46, 0.05, 0.55, 0.24]\n",
      "[5] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[5] 0.9 [0.46, 0.05, 0.55, 0.27]\n",
      "[5] 0.95 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.5 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.9 [0.46, 0.05, 0.55, 0.25]\n",
      "[6] 0.95 [0.46, 0.05, 0.55, 0.24]\n",
      "[7] 0.5 [0.46, 0.05, 0.55, 0.26]\n",
      "[7] 0.7 [0.46, 0.05, 0.55, 0.25]\n",
      "[7] 0.9 [0.46, 0.05, 0.55, 0.25]\n",
      "[7] 0.95 [0.46, 0.05, 0.55, 0.24]\n"
     ]
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "    for th in [0.5,0.7,0.9,0.95]:\n",
    "        tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "        f1 = [0,0,0,0]\n",
    "        for t in tokenizations:    \n",
    "            man = t[1] # manual\n",
    "            ref = en_ref_tokenizer.tokenize(t[0])\n",
    "            bpe = t[2]\n",
    "            dpe = t[3]\n",
    "            tf = tf_tokenizer.tokenize(t[0])\n",
    "            ref_f1 = calc_f1(man,ref)\n",
    "            bpe_f1 = calc_f1(man,bpe)\n",
    "            dpe_f1 = calc_f1(man,dpe)\n",
    "            tf_f1 = calc_f1(man,tf)\n",
    "            #if tf_f1 < 1.0:\n",
    "            #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "            f1[0] += ref_f1\n",
    "            f1[1] += bpe_f1\n",
    "            f1[2] += dpe_f1\n",
    "            f1[3] += tf_f1\n",
    "        f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "        print(n,th,f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c6cbd",
   "metadata": {},
   "source": [
    "### Check if limiting training set by word frequency helps to improve morho-parsing F1 (0.0005-0.001 is the best)\n",
    "\n",
    "#### Training in https://github.com/aigents/pygents/blob/main/notebooks/nlp/morphology/morphology_lexicon_en_ru.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "708cfe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/lex_en_counted_10 [0.46, 0.05, 0.55, 0.24]\n",
      "data/models/lex_en_counted_10_0005 [0.46, 0.05, 0.55, 0.28]\n",
      "data/models/lex_en_counted_10_001 [0.46, 0.05, 0.55, 0.28]\n",
      "data/models/lex_en_counted_10_005 [0.46, 0.05, 0.55, 0.21]\n",
      "data/models/lex_en_counted_10_01 [0.46, 0.05, 0.55, 0.08]\n"
     ]
    }
   ],
   "source": [
    "for model in ['data/models/lex_en_counted_10','data/models/lex_en_counted_10_0005','data/models/lex_en_counted_10_001',\n",
    "              'data/models/lex_en_counted_10_005','data/models/lex_en_counted_10_01']:\n",
    "    best_tf_f1 = 0\n",
    "    best_f1 = None\n",
    "    base = FreedomTokenizer(name=model,max_n=10,mode='chars',debug=False)\n",
    "    tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "    for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "        for th in [0.5,0.7,0.9,0.95]:\n",
    "            tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "            f1 = [0,0,0,0]\n",
    "            for t in tokenizations:    \n",
    "                man = t[1] # manual\n",
    "                ref = en_ref_tokenizer.tokenize(t[0])\n",
    "                bpe = t[2]\n",
    "                dpe = t[3]\n",
    "                tf = tf_tokenizer.tokenize(t[0])\n",
    "                ref_f1 = calc_f1(man,ref)\n",
    "                bpe_f1 = calc_f1(man,bpe)\n",
    "                dpe_f1 = calc_f1(man,dpe)\n",
    "                tf_f1 = calc_f1(man,tf)\n",
    "                #if tf_f1 < 1.0:\n",
    "                #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "                f1[0] += ref_f1\n",
    "                f1[1] += bpe_f1\n",
    "                f1[2] += dpe_f1\n",
    "                f1[3] += tf_f1\n",
    "            f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "            #print(n,th,f1)\n",
    "            if best_tf_f1 < f1[3]:\n",
    "                best_tf_f1 = f1[3]\n",
    "                best_f1 = f1\n",
    "    print(model,f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8f0a7c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1257863 [0.46, 0.05, 0.55, 0.24]\n",
      "0.0001 923736 [0.46, 0.05, 0.55, 0.24]\n",
      "0.0005 899442 [0.46, 0.05, 0.55, 0.24]\n",
      "0.001 896201 [0.46, 0.05, 0.55, 0.23]\n",
      "0.005 891853 [0.46, 0.05, 0.55, 0.24]\n",
      "0.01 888733 [0.46, 0.05, 0.55, 0.25]\n",
      "0.05 863488 [0.46, 0.05, 0.55, 0.24]\n",
      "0.1 834719 [0.46, 0.05, 0.55, 0.27]\n",
      "0.5 689472 [0.46, 0.05, 0.55, 0.19]\n"
     ]
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_nocount_7',max_n=7,mode='chars',debug=False)\n",
    "for model_threshold in [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5]:\n",
    "    if model_threshold > 0:\n",
    "        model_compress_with_loss(base.model,model_threshold)\n",
    "    best_tf_f1 = 0\n",
    "    best_f1 = None\n",
    "    tf_tokenizer = FreedomBasedTokenizer(base,'peak-','peak+')\n",
    "    for n in [[1],[2],[3],[4],[5],[6],[7]]:\n",
    "        for th in [0.5,0.7,0.9,0.95]:\n",
    "            tf_tokenizer.set_options(nlist = n, threshold=th)\n",
    "            f1 = [0,0,0,0]\n",
    "            for t in tokenizations:    \n",
    "                man = t[1] # manual\n",
    "                ref = en_ref_tokenizer.tokenize(t[0])\n",
    "                bpe = t[2]\n",
    "                dpe = t[3]\n",
    "                tf = tf_tokenizer.tokenize(t[0])\n",
    "                ref_f1 = calc_f1(man,ref)\n",
    "                bpe_f1 = calc_f1(man,bpe)\n",
    "                dpe_f1 = calc_f1(man,dpe)\n",
    "                tf_f1 = calc_f1(man,tf)\n",
    "                #if tf_f1 < 1.0:\n",
    "                #    print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "                f1[0] += ref_f1\n",
    "                f1[1] += bpe_f1\n",
    "                f1[2] += dpe_f1\n",
    "                f1[3] += tf_f1\n",
    "            f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "            #print(n,th,f1)\n",
    "            if best_tf_f1 < f1[3]:\n",
    "                best_tf_f1 = f1[3]\n",
    "                best_f1 = f1\n",
    "    print(model_threshold,base.count_params(),f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa6f9c",
   "metadata": {},
   "source": [
    "## Experiment with agglomerative (BPE) parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "2a79f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "31cf4e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3563505298, 2: 2819662449, 3: 2098120824, 4: 1507873050, 5: 1070193634, 6: 742502578, 7: 494400804, 8: 308690258, 9: 182032346, 10: 99581691}\n",
      "{1: 52, 2: 52, 3: 49, 4: 44, 5: 37, 6: 24, 7: 19, 8: 14, 9: 12, 10: 10}\n"
     ]
    }
   ],
   "source": [
    "update_grand_counts(base)\n",
    "update_max_freedoms(base)\n",
    "print(base.grand_counts)\n",
    "print(base.max_freedoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "8f495d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025 \t ['a', 'n', 't', 'i', 'd', 'i', 's', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'a', 'n', 'i', 's', 'm']\n",
      "0.002 \t ['an', 't', 'i', 'd', 'i', 's', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'a', 'n', 'i', 's', 'm']\n",
      "0.0017 \t ['an', 't', 'i', 'd', 'i', 's', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'an', 'i', 's', 'm']\n",
      "0.0016 \t ['an', 't', 'i', 'd', 'is', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'an', 'i', 's', 'm']\n",
      "0.0014 \t ['an', 't', 'i', 'd', 'is', 'e', 's', 't', 'a', 'b', 'l', 'is', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'an', 'i', 's', 'm']\n",
      "0.0013 \t ['an', 't', 'i', 'd', 'is', 'e', 's', 't', 'a', 'b', 'l', 'is', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'an', 'is', 'm']\n",
      "0.0011 \t ['an', 'ti', 'd', 'is', 'e', 's', 't', 'a', 'b', 'l', 'is', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'an', 'is', 'm']\n",
      "0.0009 \t ['an', 'ti', 'd', 'is', 'e', 's', 't', 'a', 'b', 'l', 'is', 'h', 'm', 'e', 'n', 't', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0008 \t ['an', 'ti', 'd', 'is', 'e', 's', 't', 'a', 'b', 'l', 'is', 'h', 'me', 'n', 't', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0005 \t ['an', 'ti', 'd', 'is', 'es', 't', 'a', 'b', 'l', 'is', 'h', 'me', 'n', 't', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0005 \t ['an', 'ti', 'd', 'is', 'es', 't', 'a', 'b', 'l', 'is', 'h', 'men', 't', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0003 \t ['an', 'ti', 'd', 'is', 'es', 't', 'a', 'b', 'l', 'is', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0003 \t ['an', 'ti', 'd', 'is', 'es', 't', 'a', 'bl', 'is', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0003 \t ['an', 'ti', 'd', 'is', 'es', 't', 'abl', 'is', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0002 \t ['an', 'ti', 'd', 'is', 'est', 'abl', 'is', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0002 \t ['an', 'ti', 'dis', 'est', 'abl', 'is', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0003 \t ['an', 'ti', 'dis', 'establ', 'is', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0004 \t ['an', 'ti', 'dis', 'establis', 'h', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0001 \t ['an', 'ti', 'dis', 'establish', 'ment', 'ar', 'i', 'an', 'is', 'm']\n",
      "0.0001 \t ['an', 'ti', 'dis', 'establish', 'ment', 'ari', 'an', 'is', 'm']\n",
      "0.0 \t ['anti', 'dis', 'establish', 'ment', 'ari', 'an', 'is', 'm']\n",
      "0.0 \t ['anti', 'dis', 'establish', 'ment', 'ari', 'an', 'ism']\n",
      "0.0 \t ['anti', 'dis', 'establish', 'ment', 'ari', 'anism']\n",
      "0.0 \t ['anti', 'dis', 'establish', 'ment', 'arianism']\n",
      "0.0 \t ['antidis', 'establish', 'ment', 'arianism']\n",
      "0.0068 \t ['t', 'h', 'i', 'n', 'k', 'a', 'b', 'l', 'e']\n",
      "0.0031 \t ['th', 'i', 'n', 'k', 'a', 'b', 'l', 'e']\n",
      "0.0008 \t ['th', 'in', 'k', 'a', 'b', 'l', 'e']\n",
      "0.0007 \t ['th', 'in', 'k', 'a', 'b', 'le']\n",
      "0.0007 \t ['th', 'in', 'k', 'a', 'ble']\n",
      "0.0004 \t ['th', 'in', 'k', 'able']\n",
      "0.0002 \t ['thin', 'k', 'able']\n",
      "0.0 \t ['think', 'able']\n",
      "0.0 \t ['thinkable']\n",
      "0.0026 \t ['i', 'm', 'a', 'g', 'i', 'n', 'a', 'b', 'l', 'e']\n",
      "0.001 \t ['i', 'm', 'a', 'g', 'in', 'a', 'b', 'l', 'e']\n",
      "0.0007 \t ['i', 'ma', 'g', 'in', 'a', 'b', 'l', 'e']\n",
      "0.0005 \t ['i', 'ma', 'g', 'in', 'a', 'b', 'le']\n",
      "0.0005 \t ['i', 'ma', 'g', 'in', 'a', 'ble']\n",
      "0.0002 \t ['i', 'ma', 'g', 'in', 'able']\n",
      "0.0001 \t ['i', 'ma', 'gin', 'able']\n",
      "0.0001 \t ['ima', 'gin', 'able']\n",
      "0.0 \t ['imagin', 'able']\n",
      "0.0 \t ['imaginable']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imaginable']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_prob_greedy(base,text,debug=True):\n",
    "    freqs = base.model[0]\n",
    "    chunks = list(text)\n",
    "    while True:\n",
    "        fmax = 0\n",
    "        imax = -1\n",
    "        fsum = 0\n",
    "        for i in range(len(chunks)-1):\n",
    "            chunk = chunks[i] + chunks[i+1]\n",
    "            #f = eval_mi_chunk(base,chunk,debug=False)\n",
    "            #f = eval_mi_chunk(base,chunk,log=False,debug=False)\n",
    "            f = eval_mi_chunk(base,chunk,debug=False) / len(chunk)\n",
    "            fsum += f\n",
    "            #if debug:\n",
    "            #    print(chunk,f)\n",
    "            if fmax < f:\n",
    "                fmax = f\n",
    "                imax = i\n",
    "        fsum /= len(chunks)\n",
    "        if debug:\n",
    "            print(round(fsum,4),'\\t',chunks)\n",
    "        if imax < 0:\n",
    "            return chunks\n",
    "        chunks[imax] = chunks[imax] + chunks[imax+1]\n",
    "        #print(chunks[imax])\n",
    "        for i in range(imax+1,len(chunks)-1):\n",
    "            chunks[i] = chunks[i+1]\n",
    "        chunks.pop()\n",
    "        #if len(chunks) == 3:\n",
    "        #    break\n",
    "    return chunks\n",
    "\n",
    "#split_prob_greedy(base,'progressively',debug=True)\n",
    "#split_prob_greedy(base,'vulnerability',debug=True)\n",
    "split_prob_greedy(base,'antidisestablishmentarianism',debug=True)\n",
    "#split_prob_greedy(base,'eurozone',debug=True)\n",
    "split_prob_greedy(base,'thinkable',debug=True)\n",
    "split_prob_greedy(base,'imaginable',debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ba7bdec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002 \t ['r', 'e', 'c', 'o', 'g', 'n', 'i', 's', 'e', 's']\n",
      "0.0014 \t ['re', 'c', 'o', 'g', 'n', 'i', 's', 'e', 's']\n",
      "0.001 \t ['re', 'c', 'o', 'g', 'n', 'is', 'e', 's']\n",
      "0.0005 \t ['re', 'co', 'g', 'n', 'is', 'e', 's']\n",
      "0.0 \t ['re', 'co', 'g', 'n', 'is', 'es']\n",
      "0.0 \t ['reco', 'g', 'n', 'is', 'es']\n",
      "0.0001 \t ['recog', 'n', 'is', 'es']\n",
      "0.0001 \t ['recogn', 'is', 'es']\n",
      "0.0 \t ['recognis', 'es']\n",
      "0.0 \t ['recognises']\n",
      "0.0012 \t ['a', 'd', 'v', 'o', 'c', 'a', 't', 'e', 's']\n",
      "0.0007 \t ['a', 'd', 'v', 'o', 'c', 'at', 'e', 's']\n",
      "0.0001 \t ['a', 'd', 'v', 'o', 'c', 'at', 'es']\n",
      "0.0001 \t ['a', 'd', 'v', 'o', 'cat', 'es']\n",
      "0.0001 \t ['ad', 'v', 'o', 'cat', 'es']\n",
      "0.0 \t ['adv', 'o', 'cat', 'es']\n",
      "0.0 \t ['adv', 'ocat', 'es']\n",
      "0.0 \t ['advocat', 'es']\n",
      "0.0 \t ['advocates']\n",
      "0.0068 \t ['p', 'i', 'n', 'g']\n",
      "0.0052 \t ['p', 'in', 'g']\n",
      "0.0002 \t ['p', 'ing']\n",
      "0.0 \t ['ping']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ping']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_prob_greedy(base,'recognises',debug=True)\n",
    "split_prob_greedy(base,'advocates',debug=True)\n",
    "split_prob_greedy(base,'ping',debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "973948d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013371943305452473\n",
      "0.0008300140341759884\n",
      "0.00039701770265302295\n"
     ]
    }
   ],
   "source": [
    "print(eval_mi_chunk(base,'advocate'))\n",
    "print(eval_mi_chunk(base,'advocates'))\n",
    "print(eval_mi_chunk(base,'dvocates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80789fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043455d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197c441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "fee000d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t ['re', 'cogn', 'ise', 's'] \t ['recognises']\n",
      "0 \t ['advocate', 's'] \t ['advocates']\n",
      "0 \t ['under', 'line', 's'] \t ['underlines']\n",
      "0.4 \t ['strength', 'en', 's'] \t ['strengthen', 's']\n",
      "0 \t ['entrepreneur', 'ship'] \t ['entreprene', 'urship']\n",
      "0 \t ['ac', 'knowledge', 's'] \t ['acknowledg', 'es']\n",
      "0 \t ['wine', 's'] \t ['wines']\n",
      "0 \t ['pre', 'sent', 'ly'] \t ['presently']\n",
      "0 \t ['fill', 'ed'] \t ['filled']\n",
      "0 \t ['en', 'dorse', 'ment'] \t ['endors', 'ement']\n",
      "0 \t ['crucial', 'ly'] \t ['crucially']\n",
      "0.33 \t ['eval', 'u', 'ation', 's'] \t ['eval', 'uations']\n",
      "0 \t ['tree', 's'] \t ['trees']\n",
      "0 \t ['ticket', 's'] \t ['tickets']\n",
      "0 \t ['pre', 'dict', 'able'] \t ['predic', 'table']\n",
      "0.4 \t ['multi', 'lateral', 'ism'] \t ['multi', 'lateralism']\n",
      "0 \t ['rat', 'ing', 's'] \t ['ratings']\n",
      "0 \t ['pre', 'dict', 'ed'] \t ['predicted']\n",
      "0 \t ['motive', 's'] \t ['motives']\n",
      "0 \t ['re', 'in', 'force', 's'] \t ['reinforces']\n",
      "0 \t ['proto', 'col', 's'] \t ['protocols']\n",
      "0.4 \t ['progress', 'ive', 'ly'] \t ['progress', 'ively']\n",
      "0 \t ['prevail', 's'] \t ['prevails']\n",
      "0.33 \t ['de', 'cent', 'ral', 'isation'] \t ['decentral', 'isation']\n",
      "0 \t ['stor', 'ed'] \t ['stored']\n",
      "0 \t ['in', 'fluen', 'za'] \t ['influenza']\n",
      "0 \t ['margin', 'al', 'is', 'ed'] \t ['marginal', 'ised']\n",
      "0 \t ['stay', 'ing'] \t ['staying']\n",
      "0 \t ['intensi', 'ty'] \t ['intensity']\n",
      "0 \t ['re', 'cast'] \t ['recast']\n",
      "0 \t ['guide', 'line'] \t ['guideline']\n",
      "0 \t ['em', 'bark', 'ed'] \t ['embarked']\n",
      "0 \t ['out', 'line', 's'] \t ['outlines']\n",
      "0 \t ['scenario', 's'] \t ['scenarios']\n",
      "0 \t ['nati', 've'] \t ['native']\n",
      "0 \t ['pre', 'vent', 'at', 'ive'] \t ['prevent', 'ative']\n",
      "0 \t ['home', 'land'] \t ['homeland']\n",
      "0 \t ['bath', 'ing'] \t ['bathing']\n",
      "0 \t ['en', 'danger', 'ed'] \t ['endangered']\n",
      "0 \t ['continent', 'al'] \t ['contin', 'ental']\n",
      "0 \t ['ten', 'th'] \t ['tenth']\n",
      "0 \t ['vulner', 'abil', 'ity'] \t ['vul', 'nerability']\n",
      "0 \t ['realis', 'ing'] \t ['realising']\n",
      "0 \t ['tight', 'er'] \t ['tighter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46, 0.05, 0.55, 0.1]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = FreedomTokenizer(name='data/models/lex_en_counted_10',max_n=10,mode='chars',debug=False)\n",
    "update_grand_counts(base)\n",
    "n = [7]\n",
    "t = 0.9\n",
    "tf_tokenizer.set_options(nlist = n, threshold=t)\n",
    "f1 = [0,0,0,0]\n",
    "for t in tokenizations:    \n",
    "    man = t[1] # manual\n",
    "    ref = en_ref_tokenizer.tokenize(t[0])\n",
    "    bpe = t[2]\n",
    "    dpe = t[3]\n",
    "    tf = split_prob_greedy(base,t[0],debug=False)\n",
    "    ref_f1 = calc_f1(man,ref)\n",
    "    bpe_f1 = calc_f1(man,bpe)\n",
    "    dpe_f1 = calc_f1(man,dpe)\n",
    "    tf_f1 = calc_f1(man,tf)\n",
    "    if tf_f1 < 1.0:\n",
    "        print(round(tf_f1,2),'\\t',man,'\\t',tf)\n",
    "    f1[0] += ref_f1\n",
    "    f1[1] += bpe_f1\n",
    "    f1[2] += dpe_f1\n",
    "    f1[3] += tf_f1\n",
    "\n",
    "f1 = [round(f/len(tokenizations),2) for f in f1]\n",
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "bbf4a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38062\n",
      "38213\n",
      "10298053\n",
      "['t', 'i', 'g', 'h', 't', 'er']\n",
      "['ti', 'g', 'h', 't', 'er']\n",
      "['ti', 'g', 'h', 'ter']\n",
      "['ti', 'gh', 'ter']\n",
      "['ti', 'ghter']\n",
      "['tighter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tighter']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(base.model[0]['tight'])\n",
    "print(base.model[0]['tigh'])\n",
    "print(base.model[0]['ter'])\n",
    "split_prob_greedy(base,'tighter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e079a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f51a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a33fca",
   "metadata": {},
   "source": [
    "## Experiment with MI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "ad52f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [['lighter'],\n",
    "    ['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r'],\n",
    "    ['l', 'i','ghter'],['l', 'ig','hter'],['l', 'igh','ter'],['l', 'ight','er'],['l', 'ighte','r'],\n",
    "    ['li', 'g','hter'],['li', 'gh','ter'],['li', 'ght','er'],['li', 'ghte','r'],\n",
    "    ['lig', 'h','ter'],['lig', 'ht','er'],['lig', 'hte','r'],['ligh', 't','er'],['ligh', 'te','r'],\n",
    "    ['light', 'e','r']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "ea041c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = FreedomTokenizer(name='data/models/brown_nolines_chars_7a',max_n=7,mode='chars',debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "d72a0a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381728"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.model[0]['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "4afbdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 6006249, 2: 6006248, 3: 6006247, 4: 6006246, 5: 6006245, 6: 6006244, 7: 6006243}\n",
      "{1: 116, 2: 101, 3: 90, 4: 99, 5: 103, 6: 91, 7: 74}\n"
     ]
    }
   ],
   "source": [
    "update_grand_counts(base)\n",
    "update_max_freedoms(base)\n",
    "print(base.grand_counts)\n",
    "print(base.max_freedoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "3ccee736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13300715166856408\n",
      "-2.759587752334441e-05\n",
      "-0.0002152679738190611\n",
      "-0.0002058241005847544\n",
      "9.514040052209535e-05\n",
      "-6.047725610669241e-06\n"
     ]
    }
   ],
   "source": [
    "update_grand_counts(lex_en_base10)\n",
    "print(eval_mi_chunk(lex_en_base10,\"the\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"teh\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"het\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"hte\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"eth\",debug=False))\n",
    "print(eval_mi_chunk(lex_en_base10,\"eht\",debug=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "0f41ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 0.05096605697327427\n",
      "adn -2.7828226384747352e-05\n",
      "nda 0.00023910828949630638\n",
      "nad -8.117918821384263e-05\n",
      "dan 3.544033347205279e-05\n",
      "dna -4.55627356311221e-05\n"
     ]
    }
   ],
   "source": [
    "for w in ['and','adn','nda','nad','dan','dna']:\n",
    "    print(w,eval_mi_chunk(lex_en_base10,w,debug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "c9eb73be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['lighter']\n",
      "0.0 ['l', 'ighter']\n",
      "0.01 ['li', 'ghter']\n",
      "0.0 ['lig', 'hter']\n",
      "0.01 ['ligh', 'ter']\n",
      "0.02 ['light', 'er']\n",
      "0.0 ['lighte', 'r']\n",
      "0.0 ['l', 'i', 'ghter']\n",
      "0.0 ['l', 'ig', 'hter']\n",
      "0.01 ['l', 'igh', 'ter']\n",
      "0.03 ['l', 'ight', 'er']\n",
      "0.0 ['l', 'ighte', 'r']\n",
      "0.0 ['li', 'g', 'hter']\n",
      "0.02 ['li', 'gh', 'ter']\n",
      "0.03 ['li', 'ght', 'er']\n",
      "0.01 ['li', 'ghte', 'r']\n",
      "0.01 ['lig', 'h', 'ter']\n",
      "0.02 ['lig', 'ht', 'er']\n",
      "0.0 ['lig', 'hte', 'r']\n",
      "0.02 ['ligh', 't', 'er']\n",
      "0.0 ['ligh', 'te', 'r']\n",
      "0.0 ['light', 'e', 'r']\n",
      "['l', 'ight', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,test_set,debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "24d06ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['lighter']\n",
      "0.0 ['l', 'ighter']\n",
      "0.01 ['li', 'ghter']\n",
      "0.0 ['lig', 'hter']\n",
      "0.01 ['ligh', 'ter']\n",
      "0.02 ['light', 'er']\n",
      "0.0 ['lighte', 'r']\n",
      "0.0 ['l', 'i', 'ghter']\n",
      "0.0 ['l', 'ig', 'hter']\n",
      "0.01 ['l', 'igh', 'ter']\n",
      "0.03 ['l', 'ight', 'er']\n",
      "0.0 ['l', 'ighte', 'r']\n",
      "0.0 ['li', 'g', 'hter']\n",
      "0.02 ['li', 'gh', 'ter']\n",
      "0.03 ['li', 'ght', 'er']\n",
      "0.01 ['li', 'ghte', 'r']\n",
      "0.01 ['lig', 'h', 'ter']\n",
      "0.02 ['lig', 'ht', 'er']\n",
      "0.0 ['lig', 'hte', 'r']\n",
      "0.02 ['ligh', 't', 'er']\n",
      "0.0 ['ligh', 'te', 'r']\n",
      "0.0 ['light', 'e', 'r']\n",
      "['l', 'ight', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,test_set,debug=True,extra_len_discount=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "89fb8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.54 ['t', 'ighter']\n",
      "5.86 ['ti', 'ghter']\n",
      "0.82 ['tig', 'hter']\n",
      "3.25 ['tigh', 'ter']\n",
      "5.02 ['tight', 'er']\n",
      "5.06 ['tighte', 'r']\n",
      "['t', 'ighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "['light', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(base,[['t', 'ighter'],['ti', 'ghter'],['tig', 'hter'],['tigh', 'ter'],['tight', 'er'],['tighte', 'r']],debug=True))\n",
    "print(eval_splits(base,[['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r']],debug=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ebaed72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.54 ['t', 'ighter']\n",
      "5.86 ['ti', 'ghter']\n",
      "0.82 ['tig', 'hter']\n",
      "3.25 ['tigh', 'ter']\n",
      "5.02 ['tight', 'er']\n",
      "5.06 ['tighte', 'r']\n",
      "['t', 'ighter']\n",
      "6.54 ['l', 'ighter']\n",
      "5.92 ['li', 'ghter']\n",
      "3.1 ['lig', 'hter']\n",
      "6.76 ['ligh', 'ter']\n",
      "8.53 ['light', 'er']\n",
      "7.91 ['lighte', 'r']\n",
      "['light', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,[['t', 'ighter'],['ti', 'ghter'],['tig', 'hter'],['tigh', 'ter'],['tight', 'er'],['tighte', 'r']],debug=True))\n",
    "print(eval_splits(lex_en_base10,[['l', 'ighter'],['li', 'ghter'],['lig', 'hter'],['ligh', 'ter'],['light', 'er'],['lighte', 'r']],debug=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "cdf02a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['lighter']\n",
      "0.0 ['l', 'ighter']\n",
      "0.01 ['li', 'ghter']\n",
      "0.0 ['lig', 'hter']\n",
      "0.01 ['ligh', 'ter']\n",
      "0.02 ['light', 'er']\n",
      "0.0 ['lighte', 'r']\n",
      "0.0 ['l', 'i', 'ghter']\n",
      "0.0 ['l', 'ig', 'hter']\n",
      "0.01 ['l', 'igh', 'ter']\n",
      "0.03 ['l', 'ight', 'er']\n",
      "0.0 ['l', 'ighte', 'r']\n",
      "0.0 ['li', 'g', 'hter']\n",
      "0.02 ['li', 'gh', 'ter']\n",
      "0.03 ['li', 'ght', 'er']\n",
      "0.01 ['li', 'ghte', 'r']\n",
      "0.01 ['lig', 'h', 'ter']\n",
      "0.02 ['lig', 'ht', 'er']\n",
      "0.0 ['lig', 'hte', 'r']\n",
      "0.02 ['ligh', 't', 'er']\n",
      "0.0 ['ligh', 'te', 'r']\n",
      "0.0 ['light', 'e', 'r']\n",
      "['l', 'ight', 'er']\n"
     ]
    }
   ],
   "source": [
    "print(eval_splits(lex_en_base10,test_set,debug=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91592b7",
   "metadata": {},
   "source": [
    "## Experiment with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "114b0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_tf(base,chunks,avg = False):\n",
    "    s = sum([chunk_tf(base,c) for c in chunks])\n",
    "    return s/len(chunks) if avg else s\n",
    "\n",
    "def chunks_pr(base,chunks,avg = False):\n",
    "    s = sum([base.model[0][c]/base.grand_counts[len(c)] for c in chunks])\n",
    "    return s/len(chunks) if avg else s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "33769745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 ['lighter']\n",
      "0.43 ['l', 'ighter']\n",
      "0.39 ['li', 'ghter']\n",
      "0.24 ['lig', 'hter']\n",
      "0.46 ['ligh', 'ter']\n",
      "0.58 ['light', 'er']\n",
      "0.43 ['lighte', 'r']\n",
      "0.52 ['l', 'i', 'ghter']\n",
      "0.46 ['l', 'ig', 'hter']\n",
      "0.59 ['l', 'igh', 'ter']\n",
      "0.62 ['l', 'ight', 'er']\n",
      "0.54 ['l', 'ighte', 'r']\n",
      "0.49 ['li', 'g', 'hter']\n",
      "0.61 ['li', 'gh', 'ter']\n",
      "0.58 ['li', 'ght', 'er']\n",
      "0.49 ['li', 'ghte', 'r']\n",
      "0.59 ['lig', 'h', 'ter']\n",
      "0.49 ['lig', 'ht', 'er']\n",
      "0.4 ['lig', 'hte', 'r']\n",
      "0.59 ['ligh', 't', 'er']\n",
      "0.56 ['ligh', 'te', 'r']\n",
      "0.64 ['light', 'e', 'r']\n",
      "\n",
      "0.64 li\n",
      "0.5 ig\n",
      "0.5 gh\n",
      "0.39 ht\n",
      "0.72 te\n",
      "0.75 er\n",
      "\n",
      "0.33 lig\n",
      "0.36 igh\n",
      "0.36 ght\n",
      "0.12 hte\n",
      "0.7 ter\n",
      "\n",
      "0.21 ligh\n",
      "0.39 ight\n",
      "0.08 ghte\n",
      "0.14 hter\n",
      "\n",
      "0.4 light\n",
      "0.14 ighte\n",
      "0.14 ghter\n",
      "\n",
      "0.64 li\n",
      "0.5 ig\n",
      "0.5 gh\n",
      "0.39 ht\n",
      "0.7 ter\n",
      "\n",
      "0.33 lig\n",
      "0.5 gh\n",
      "0.39 ht\n",
      "0.7 ter\n",
      "\n",
      "0.4 light\n",
      "0.75 er\n"
     ]
    }
   ],
   "source": [
    "#brown\n",
    "for t in test_set:\n",
    "    s = chunks_tf(base,t,avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['li','ig','gh','ht','te','er']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['lig','igh','ght','hte','ter']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['ligh','ight','ghte','hter']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['light','ighte','ghter']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['li','ig','gh','ht','ter']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['lig','gh','ht','ter']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['light','er']:\n",
    "    s = chunks_tf(base,[t],avg=True)\n",
    "    print(round(s,2),t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "0ce3139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['lighter']\n",
      "0.02 ['l', 'ighter']\n",
      "0.0 ['li', 'ghter']\n",
      "0.0 ['lig', 'hter']\n",
      "0.0 ['ligh', 'ter']\n",
      "0.01 ['light', 'er']\n",
      "0.02 ['lighte', 'r']\n",
      "0.03 ['l', 'i', 'ghter']\n",
      "0.01 ['l', 'ig', 'hter']\n",
      "0.01 ['l', 'igh', 'ter']\n",
      "0.02 ['l', 'ight', 'er']\n",
      "0.03 ['l', 'ighte', 'r']\n",
      "0.01 ['li', 'g', 'hter']\n",
      "0.0 ['li', 'gh', 'ter']\n",
      "0.01 ['li', 'ght', 'er']\n",
      "0.02 ['li', 'ghte', 'r']\n",
      "0.02 ['lig', 'h', 'ter']\n",
      "0.0 ['lig', 'ht', 'er']\n",
      "0.02 ['lig', 'hte', 'r']\n",
      "0.03 ['ligh', 't', 'er']\n",
      "0.02 ['ligh', 'te', 'r']\n",
      "0.05 ['light', 'e', 'r']\n",
      "\n",
      "0.0039 li\n",
      "0.0017 ig\n",
      "0.0017 gh\n",
      "0.001 ht\n",
      "0.0072 te\n",
      "0.0127 er\n",
      "\n",
      "0.0003 lig\n",
      "0.001 igh\n",
      "0.001 ght\n",
      "0.0001 hte\n",
      "0.0021 ter\n",
      "\n",
      "0.0002 ligh\n",
      "0.0007 ight\n",
      "0.0001 ghte\n",
      "0.0 hter\n",
      "\n",
      "0.0002 light\n",
      "0.0001 ighte\n",
      "0.0 ghter\n",
      "\n",
      "0.0039 li\n",
      "0.0017 ig\n",
      "0.0017 gh\n",
      "0.001 ht\n",
      "0.0021 ter\n",
      "\n",
      "0.0003 lig\n",
      "0.0017 gh\n",
      "0.001 ht\n",
      "0.0021 ter\n",
      "\n",
      "0.0002 light\n",
      "0.0127 er\n"
     ]
    }
   ],
   "source": [
    "#brown\n",
    "for t in test_set:\n",
    "    s = chunks_pr(base,t,avg=True)\n",
    "    print(round(s,2),t)\n",
    "print()\n",
    "for t in ['li','ig','gh','ht','te','er']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n",
    "print()\n",
    "for t in ['lig','igh','ght','hte','ter']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n",
    "print()\n",
    "for t in ['ligh','ight','ghte','hter']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n",
    "print()\n",
    "for t in ['light','ighte','ghter']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n",
    "print()\n",
    "for t in ['li','ig','gh','ht','ter']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n",
    "print()\n",
    "for t in ['lig','gh','ht','ter']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n",
    "print()\n",
    "for t in ['light','er']:\n",
    "    s = chunks_pr(base,[t],avg=True)\n",
    "    print(round(s,4),t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206448a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e90c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ac765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_break(word):\n",
    "    # find most probable piece, take it out\n",
    "    # find next, do the same\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9227b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237309d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
