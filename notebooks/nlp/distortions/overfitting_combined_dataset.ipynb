{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "from pygents.aigents_api import tokenize_re\n",
    "from pygents.util import dictcount\n",
    "from pygents.aigents_api import build_ngrams\n",
    "from pygents.plot import plot_dict\n",
    "\n",
    "from pygents.aigents_api import TextMetrics\n",
    "\n",
    "def language_metrics(metrics_list):\n",
    "    metrics = {}\n",
    "    for m in metrics_list:\n",
    "        metrics[m] = './data/dict/new_model/overfitting_combined/' + m + '.txt'\n",
    "        #metrics[m] = './data/dict/' + 'en' + '/' + m + '.txt'\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient Question</th>\n",
       "      <th>Distorted part</th>\n",
       "      <th>Dominant Distortion</th>\n",
       "      <th>Secondary Distortion (Optional)l</th>\n",
       "      <th>Secondary Distortion (Optional)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm such a failure I never do anything right.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody likes me because I'm not interesting.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't try new things because I'll just mess...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My boss didn't say 'good morning' she must be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My friend didn't invite me to the party I mus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>I’m a 21 year old female. I spent most of my l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>I am 21 female and have not had any friends fo...</td>\n",
       "      <td>Now I am at university my peers around me all ...</td>\n",
       "      <td>Overgeneralization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>From the U.S.: My brother is 19 years old and ...</td>\n",
       "      <td>He claims he’s severely depressed and has outb...</td>\n",
       "      <td>Mental filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mind Reading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>From the U.S.: I am a 21 year old woman who ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>I recently moved out on my ex-roommate because...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6057 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Patient Question  \\\n",
       "0         I'm such a failure I never do anything right.   \n",
       "1          Nobody likes me because I'm not interesting.   \n",
       "2      I can't try new things because I'll just mess...   \n",
       "3      My boss didn't say 'good morning' she must be...   \n",
       "4      My friend didn't invite me to the party I mus...   \n",
       "...                                                 ...   \n",
       "6052  I’m a 21 year old female. I spent most of my l...   \n",
       "6053  I am 21 female and have not had any friends fo...   \n",
       "6054  From the U.S.: My brother is 19 years old and ...   \n",
       "6055  From the U.S.: I am a 21 year old woman who ha...   \n",
       "6056  I recently moved out on my ex-roommate because...   \n",
       "\n",
       "                                         Distorted part Dominant Distortion  \\\n",
       "0                                                   NaN          Distortion   \n",
       "1                                                   NaN          Distortion   \n",
       "2                                                   NaN          Distortion   \n",
       "3                                                   NaN          Distortion   \n",
       "4                                                   NaN          Distortion   \n",
       "...                                                 ...                 ...   \n",
       "6052                                                NaN       No Distortion   \n",
       "6053  Now I am at university my peers around me all ...  Overgeneralization   \n",
       "6054  He claims he’s severely depressed and has outb...       Mental filter   \n",
       "6055                                                NaN       No Distortion   \n",
       "6056                                                NaN       No Distortion   \n",
       "\n",
       "      Secondary Distortion (Optional)l Secondary Distortion (Optional)  \n",
       "0                                  NaN                             NaN  \n",
       "1                                  NaN                             NaN  \n",
       "2                                  NaN                             NaN  \n",
       "3                                  NaN                             NaN  \n",
       "4                                  NaN                             NaN  \n",
       "...                                ...                             ...  \n",
       "6052                               NaN                             NaN  \n",
       "6053                               NaN                             NaN  \n",
       "6054                               NaN                    Mind Reading  \n",
       "6055                               NaN                             NaN  \n",
       "6056                               NaN                             NaN  \n",
       "\n",
       "[6057 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset_file_path = \"./data/corpora/English/distortions/halilbabacan/raw_Cognitive_distortions.csv\" \n",
    "\n",
    "import kagglehub\n",
    "multiclass_dataset_path = kagglehub.dataset_download(\"sagarikashreevastava/cognitive-distortion-detetction-dataset\")\n",
    "multiclass_dataset_file_path = multiclass_dataset_path + \"/Annotated_data.csv\"\n",
    "\n",
    "df1 = pd.read_csv(binary_dataset_file_path)\n",
    "df1 = df1.rename(columns={'Text': 'Patient Question', 'Label': 'Dominant Distortion'})\n",
    "df1.insert(1, \"Distorted part\", value = np.nan)\n",
    "df1.insert(3, \"Secondary Distortion (Optional)l\", value = np.nan)\n",
    "\n",
    "df2 = pd.read_csv(multiclass_dataset_file_path) \n",
    "df2 = df2.drop('Id_Number', axis=1) # delete columnb with id \n",
    "\n",
    "df3 = pd.concat([df1, df2], ignore_index=True)\n",
    "#df3['Dominant Distortion'] = df3['Dominant Distortion'].apply(lambda x: 'Distortion' if x != 'No Distortion' else x)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_normalized_ngrams(ngram_max, df, analytics_method):\n",
    "    distortions = defaultdict(int)\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    \n",
    "    uniq_n_gram_dicts = defaultdict(lambda: defaultdict(int)) # Counts of uniq N-grams by Distortion\n",
    "    uniq_all_n_grams = defaultdict(int)  # A general dictionary for all n-grams uniq by text\n",
    "    n_gram_distortions = defaultdict(lambda: defaultdict(int)) # Counts of distortiions by N-gram\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        ground_distortion = 'Distortion' if primary_distortion != 'No Distortion' else 'No Distortion'\n",
    "\n",
    "        dictcount(distortions,ground_distortion)\n",
    "        \n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Generation and counting of n-grams (from 1 to 4)\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[ground_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "\n",
    "            uniq_n_grams = set(n_grams)\n",
    "            for uniq_n_gram in uniq_n_grams:\n",
    "                dictcount(uniq_n_gram_dicts[ground_distortion], uniq_n_gram)\n",
    "                dictcount(uniq_all_n_grams, uniq_n_gram)\n",
    "                dictcount(n_gram_distortions[uniq_n_gram],ground_distortion)\n",
    "                \n",
    "    # Normalizing distortion-specific counts by total counts\n",
    "    norm_n_gram_dicts = {}\n",
    "    for n_gram_dict in n_gram_dicts:\n",
    "        norm_n_gram_dict = {}\n",
    "        norm_n_gram_dicts[n_gram_dict] = norm_n_gram_dict\n",
    "        dic = n_gram_dicts[n_gram_dict]\n",
    "        for n_gram in dic:\n",
    "            if len(n_gram) <= ngram_max:\n",
    "                norm_n_gram_dict[n_gram] = float( dic[n_gram] ) / all_n_grams[n_gram]\n",
    "\n",
    "    # Normalize uniq counts \n",
    "    norm_uniq_n_gram_dicts = {}\n",
    "    for uniq_n_gram_dict in uniq_n_gram_dicts:\n",
    "        norm_uniq_n_gram_dict = {}\n",
    "        norm_uniq_n_gram_dicts[uniq_n_gram_dict] = norm_uniq_n_gram_dict\n",
    "        dic = uniq_n_gram_dicts[uniq_n_gram_dict]\n",
    "        nonuniq_dic = n_gram_dicts[n_gram_dict]\n",
    "        # Normalize uniq Document counts of N-grams by distortion by Documents count by Distortion\n",
    "        for n_gram in dic:\n",
    "            if len(n_gram) <= ngram_max:\n",
    "                #norm_uniq_n_gram_dict[n_gram] = float( dic[n_gram] ) * nonuniq_dic[n_gram] / distortions[uniq_n_gram_dict] / len(n_gram_distortions[n_gram]) / all_n_grams[n_gram]\n",
    "                norm_uniq_n_gram_dict[n_gram] = float( dic[n_gram] ) / distortions[uniq_n_gram_dict] / len(n_gram_distortions[n_gram])\n",
    "\n",
    "    if analytics_method == 'normalize':\n",
    "        return norm_n_gram_dicts\n",
    "    else:\n",
    "        return norm_uniq_n_gram_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_frequency(ngram_max, df):\n",
    "    print('ANALYSE FREQUENCY')\n",
    "    # Analyze the frequency of n-grams for each cognitive distortion\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        ground_distortion = 'Distortion' if primary_distortion != 'No Distortion' else 'No Distortion'\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Generation and counting of n-grams (from 1 to 4)\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            dictcount(n_gram_dicts[ground_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "\n",
    "    return n_gram_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TF_IDF(ngram_max, df):\n",
    "    print('ANALYSE TF-IDF')\n",
    "    # Analyze TF-IDF values for n-grams for each cognitive distortion\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of documents in which each n-gram appears\n",
    "\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        ground_distortion = 'Distortion' if primary_distortion != 'No Distortion' else 'No Distortion'\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Generate n-grams and update the document counters where they appear\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[ground_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # TF-IDF Calculation\n",
    "    tfidf_dicts = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), analyze the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            tf = count / sum(ngram_dict.values())  # Frequency of the n-gram in the text (TF): TF = (Number of occurrences of the given n-gram for the specific cognitive distortion) / (Total number of occurrences of all other n-grams for the same cognitive distortion)\n",
    "            idf = math.log(total_docs / (1 + doc_counts[n_gram]))  # Inverse Document Frequency (IDF): IDF = Total number of documents / Number of documents containing the given n-gram\n",
    "            tfidf_dicts[distortion][n_gram] = tf * idf  # TF-IDF\n",
    "\n",
    "    return tfidf_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TFTF_tf(ngram_max, df):\n",
    "    print('ANALYSE TFTF')\n",
    "    # Analyze TFTF_tf values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        ground_distortion = 'Distortion' if primary_distortion != 'No Distortion' else 'No Distortion'\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[ground_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[ground_distortion][n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # Calculation of the TFTF_tf\n",
    "    tftf_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # TFTF: Mutual relevance of features and text (how important a given n-gram is for describing the text, looking for specific n-grams within the text)\n",
    "            tf = count / sum(ngram_dict.values()) # Frequency of the n-gram in the text (TF): TF = (Number of times the given n-gram appears for the specific cognitive distortion) / (Number of times all other n-grams appear for the same cognitive distortion)\n",
    "            ft = doc_counts[n_gram] / total_docs # The number of texts in which the current n-gram appears / the total number of texts in the dataset\n",
    "            Ft = sum(ngram_dict.values())\n",
    "            tftf = (tf ** 2) / (ft*Ft if ft*Ft > 0 else 1)\n",
    "\n",
    "            tftf_results[distortion][n_gram] = tftf\n",
    "\n",
    "    return tftf_results\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TCTC_tc(ngram_max, df):\n",
    "    print('ANALYSE TCTC')\n",
    "    # Analyze TCTC_tc values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        ground_distortion = 'Distortion' if primary_distortion != 'No Distortion' else 'No Distortion'\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[ground_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[ground_distortion][n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # Calculation of the TCTC_tc\n",
    "    tctc_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # TCTC: Mutual relevance of categories and text (how the given n-gram is distributed across all texts of a given category)\n",
    "            ct = unique_n_gram_dicts[distortion][n_gram] # The number of texts in which the current n-gram (n_gram) is associated with the current distortion (distortion)\n",
    "            tctc = (ct ** 2) / (doc_counts[n_gram] * total_docs) # In the denominator: the number of texts containing the given n-gram, regardless of distortion * the total number of texts\n",
    "\n",
    "            tctc_results[distortion][n_gram] = tctc\n",
    "\n",
    "    return tctc_results\n",
    "\n",
    "\n",
    "\n",
    "def analyse_CFCF_cf(ngram_max, df):\n",
    "    print('ANALYSE CFCF')\n",
    "    # Analyze CFCF_cf values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        ground_distortion = 'Distortion' if primary_distortion != 'No Distortion' else 'No Distortion'\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[ground_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[ground_distortion][n_gram] += 1\n",
    "\n",
    "    # Calculation of the CFCF_cf\n",
    "    cfcf_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # CFCF: Mutual relevance of features and categories (how characteristic the given n-gram is for a particular category)\n",
    "            cf = unique_n_gram_dicts[distortion][n_gram]\n",
    "            fc = sum(unique_n_gram_dicts[distortion].values()) # The total number of texts for all n-grams associated with the given distortion\n",
    "            cfcf = (cf ** 2) / (cf * fc if cf * fc > 0 else 1)\n",
    "\n",
    "            cfcf_results[distortion][n_gram] = cfcf\n",
    "\n",
    "    return cfcf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_off_upper_ngrams(model_ngram, upper_cut_off_percentage):\n",
    "    filtered_model_ngram = {}\n",
    "    for distortion, ngram_dict in model_ngram.items():\n",
    "        # Find the maximum metric value for the current distortion\n",
    "        max_value = max(ngram_dict.values()) if ngram_dict else 0\n",
    "        threshold_value = max_value * (upper_cut_off_percentage / 100)\n",
    "\n",
    "        # Filter n-grams that meet or exceed the threshold value\n",
    "        filtered_model_ngram[distortion] = {\n",
    "            ngram: metric for ngram, metric in ngram_dict.items() if metric >= threshold_value\n",
    "        }\n",
    "    return filtered_model_ngram\n",
    "\n",
    "def create_model_files(filtered_model_ngram):\n",
    "    # Create .txt files for each distortion\n",
    "    output_dir = \"./data/dict/new_model/overfitting_combined\"\n",
    "    distortion_file_path = f\"{output_dir}/Distortion.txt\"\n",
    "    distortions_labels = []\n",
    "\n",
    "    with open(distortion_file_path, \"w\", encoding=\"utf-8\") as distortion_file:\n",
    "        distortions_labels.append(\"Distortion\")\n",
    "        for distortion, ngrams in filtered_model_ngram.items():\n",
    "            file_path = f\"{output_dir}/{distortion}.txt\"\n",
    "            distortions_labels.append(distortion)\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for ngram in ngrams.keys():\n",
    "                    f.write(\" \".join(ngram) + \"\\n\")\n",
    "                    if distortion != \"No Distortion\":\n",
    "                            distortion_file.write(\" \".join(ngram) + \"\\n\")\n",
    "    return (list(set(distortions_labels) - {'No Distortion'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_from_counts(true_positive, true_negative, false_positive, false_negative):\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    return 2 * precision * recall / (precision + recall) if precision > 0 or recall > 0 else 0 \n",
    "\n",
    "def evaluate_df_counts(df,evaluator,threshold, tm, debug=False):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    for _, row in df.iterrows():\n",
    "        # Text definition: first, check the 2nd column; if NaN, take the text from the 1st column.\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row.iloc[3] if pd.notna(row.iloc[3]) else None  # The secondary distortion from the 4th column, if it exists\n",
    "        ground_distortion = False if primary_distortion == 'No Distortion' else True\n",
    "                       \n",
    "        our_distortion = evaluator(text,threshold, tm)\n",
    "        \n",
    "        # https://en.wikipedia.org/wiki/F-score\n",
    "        if ground_distortion == True and our_distortion == True:\n",
    "            true_positive += 1\n",
    "        if ground_distortion == False and our_distortion == True:\n",
    "            false_positive += 1\n",
    "        if ground_distortion == False and our_distortion == False:\n",
    "            true_negative += 1\n",
    "        if ground_distortion == True and our_distortion == False:\n",
    "            false_negative += 1\n",
    "\n",
    "        if debug:\n",
    "            print(ground_distortion,our_distortion,text[:20],metrics)\n",
    "\n",
    "    return true_positive, true_negative, false_positive, false_negative\n",
    "\n",
    "def evaluate_df(df,evaluator,threshold,tm, debug=False):\n",
    "    true_positive, true_negative, false_positive, false_negative = evaluate_df_counts(df,evaluator,threshold,tm,debug)\n",
    "    return f1_from_counts(true_positive, true_negative, false_positive, false_negative)\n",
    "\n",
    "def evaluate_df_acc_f1(df,evaluator,threshold,tm,debug=False):\n",
    "    true_positive, true_negative, false_positive, false_negative = evaluate_df_counts(df,evaluator,threshold,tm,debug)\n",
    "    return (true_positive + true_negative) / len(df), f1_from_counts(true_positive, true_negative, false_positive, false_negative) \n",
    "\n",
    "def our_evaluator(text,threshold, tm):\n",
    "    metrics = tm.get_sentiment_words(text)\n",
    "    if metrics.get('Distortion', 0) > threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dataset(analytics_method, ngram_max, upper_cut_off_percentage, df):\n",
    "\n",
    "    print('Analytics method:', analytics_method)\n",
    "    print('N-gram max length:', ngram_max)\n",
    "    print('Upper cut-off percentage:', upper_cut_off_percentage)\n",
    "\n",
    "    if analytics_method == 'frequency':\n",
    "        model_ngram = analyse_frequency(ngram_max, df)\n",
    "    elif analytics_method == 'TF-IDF':\n",
    "        model_ngram = analyse_TF_IDF(ngram_max, df)\n",
    "    elif analytics_method == 'TFTF_tf':\n",
    "        model_ngram = analyse_TFTF_tf(ngram_max, df)\n",
    "    elif analytics_method == 'TCTC_tc':\n",
    "        model_ngram = analyse_TCTC_tc(ngram_max, df)\n",
    "    elif analytics_method == 'CFCF_cf':\n",
    "        model_ngram = analyse_CFCF_cf(ngram_max, df)\n",
    "    elif analytics_method == 'normalize' or analytics_method == 'normalize_uniq':\n",
    "        model_ngram = analyse_normalized_ngrams(ngram_max, df, analytics_method)\n",
    "\n",
    "    # Filter out values below the threshold\n",
    "    filtered_model_ngram = cut_off_upper_ngrams(model_ngram, upper_cut_off_percentage)\n",
    "\n",
    "    # Create .txt files for each distortion\n",
    "    distortions_labels = create_model_files(filtered_model_ngram)\n",
    "    tm = TextMetrics(language_metrics(distortions_labels), encoding = \"utf-8\", debug=False)\n",
    "\n",
    "    print('\\tThreshold, accuracy, F1 score:')\n",
    "    any_res_acc = {}\n",
    "    any_res = {}\n",
    "    for threshold in [0.0,0.01,0.05,0.1,0.2,0.4,0.6,0.8]:\n",
    "        acc, f1 = evaluate_df_acc_f1(df3,our_evaluator,threshold, tm)\n",
    "        any_res_acc[threshold] = acc\n",
    "        any_res[threshold] = f1\n",
    "        print('\\t', threshold, acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6902757140498597 0.8166894664842682\n",
      "\t 0.8 0.6868086511474327 0.7788270957211146\n"
     ]
    }
   ],
   "source": [
    "analyse_dataset('normalize_uniq', 1, 10, df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6920917946177976 0.8179955108812336\n",
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.7137196631996038 0.8280785246876858\n",
      "Analytics method: normalize\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6934125804853888 0.8186346322883093\n",
      "Analytics method: normalize\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6925870893181443 0.8182350644279578\n",
      "Analytics method: normalize\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "Analytics method: normalize\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6925870893181443 0.8182350644279578\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6902757140498597 0.8166894664842682\n",
      "\t 0.8 0.6868086511474327 0.7788270957211146\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.5081723625557206 0.4562876437306078\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.3298662704309064 0.06106870229007634\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6902757140498597 0.8166894664842682\n",
      "\t 0.8 0.6871388476143305 0.780034822983169\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.5081723625557206 0.4562876437306078\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.36007924715205547 0.13981358189081225\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6902757140498597 0.8166894664842682\n",
      "\t 0.8 0.6871388476143305 0.780034822983169\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.5081723625557206 0.4562876437306078\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.36007924715205547 0.13981358189081225\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6902757140498597 0.8166894664842682\n",
      "\t 0.8 0.6871388476143305 0.780034822983169\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.5081723625557206 0.4562876437306078\n",
      "Analytics method: normalize_uniq\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.36007924715205547 0.13981358189081225\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6917615981508998 0.8178003318044307\n",
      "\t 0.8 0.6891200264157173 0.8155910292821468\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6915964999174509 0.8176849502244778\n",
      "\t 0.8 0.6833415882450058 0.8063017572207635\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6904408122833086 0.816876648110167\n",
      "\t 0.8 0.6894502228826151 0.7890072910824453\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6917615981508998 0.8178003318044307\n",
      "\t 0.8 0.6889549281822684 0.8155833985904464\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6915964999174509 0.8176849502244778\n",
      "\t 0.8 0.6802047218094767 0.8056586736229557\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6904408122833086 0.816876648110167\n",
      "\t 0.8 0.6841670794122503 0.7894331315354981\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6917615981508998 0.8178003318044307\n",
      "\t 0.8 0.6887898299488195 0.8154674498286832\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6915964999174509 0.8176849502244778\n",
      "\t 0.8 0.6800396235760278 0.8055388320288983\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6904408122833086 0.816876648110167\n",
      "\t 0.8 0.6841670794122503 0.7893403810153067\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6917615981508998 0.8178003318044307\n",
      "\t 0.8 0.6887898299488195 0.8154674498286832\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6915964999174509 0.8176849502244778\n",
      "\t 0.8 0.6800396235760278 0.8055388320288983\n",
      "Analytics method: TF-IDF\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TF-IDF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6904408122833086 0.816876648110167\n",
      "\t 0.8 0.6841670794122503 0.7893403810153067\n",
      "Analytics method: frequency\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6807000165098234 0.8097205824478553\n",
      "\t 0.8 0.4888558692421991 0.4296241709653648\n",
      "Analytics method: frequency\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.687964338781575 0.8151408450704225\n",
      "\t 0.01 0.687964338781575 0.8151408450704225\n",
      "\t 0.05 0.687964338781575 0.8151408450704225\n",
      "\t 0.1 0.687964338781575 0.8151408450704225\n",
      "\t 0.2 0.687964338781575 0.8151408450704225\n",
      "\t 0.4 0.6848274723460459 0.8127513487003433\n",
      "\t 0.6 0.6427274228165759 0.7598224195338512\n",
      "\t 0.8 0.31071487535083375 0.007606370335155693\n",
      "Analytics method: frequency\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.678058444774641 0.80810864003149\n",
      "\t 0.01 0.678058444774641 0.80810864003149\n",
      "\t 0.05 0.678058444774641 0.80810864003149\n",
      "\t 0.1 0.678058444774641 0.80810864003149\n",
      "\t 0.2 0.6782235430080898 0.8081881704556638\n",
      "\t 0.4 0.6747564801056629 0.8028817290374224\n",
      "\t 0.6 0.5846128446425624 0.6343023255813954\n",
      "\t 0.8 0.30840350008254913 0.0009539709038874314\n",
      "Analytics method: frequency\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6807000165098234 0.8097205824478553\n",
      "\t 0.8 0.4888558692421991 0.4296241709653648\n",
      "Analytics method: frequency\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.687964338781575 0.8151408450704225\n",
      "\t 0.01 0.687964338781575 0.8151408450704225\n",
      "\t 0.05 0.687964338781575 0.8151408450704225\n",
      "\t 0.1 0.687964338781575 0.8151408450704225\n",
      "\t 0.2 0.687964338781575 0.8151408450704225\n",
      "\t 0.4 0.6848274723460459 0.8127513487003433\n",
      "\t 0.6 0.6427274228165759 0.7598224195338512\n",
      "\t 0.8 0.31071487535083375 0.007606370335155693\n",
      "Analytics method: frequency\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.678058444774641 0.80810864003149\n",
      "\t 0.01 0.678058444774641 0.80810864003149\n",
      "\t 0.05 0.678058444774641 0.80810864003149\n",
      "\t 0.1 0.678058444774641 0.80810864003149\n",
      "\t 0.2 0.6782235430080898 0.8081881704556638\n",
      "\t 0.4 0.6747564801056629 0.8028817290374224\n",
      "\t 0.6 0.5846128446425624 0.6343023255813954\n",
      "\t 0.8 0.30840350008254913 0.0009539709038874314\n",
      "Analytics method: frequency\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6807000165098234 0.8097205824478553\n",
      "\t 0.8 0.4888558692421991 0.4296241709653648\n",
      "Analytics method: frequency\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.687964338781575 0.8151408450704225\n",
      "\t 0.01 0.687964338781575 0.8151408450704225\n",
      "\t 0.05 0.687964338781575 0.8151408450704225\n",
      "\t 0.1 0.687964338781575 0.8151408450704225\n",
      "\t 0.2 0.687964338781575 0.8151408450704225\n",
      "\t 0.4 0.6848274723460459 0.8127513487003433\n",
      "\t 0.6 0.6427274228165759 0.7598224195338512\n",
      "\t 0.8 0.31071487535083375 0.007606370335155693\n",
      "Analytics method: frequency\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.678058444774641 0.80810864003149\n",
      "\t 0.01 0.678058444774641 0.80810864003149\n",
      "\t 0.05 0.678058444774641 0.80810864003149\n",
      "\t 0.1 0.678058444774641 0.80810864003149\n",
      "\t 0.2 0.6782235430080898 0.8081881704556638\n",
      "\t 0.4 0.6747564801056629 0.8028817290374224\n",
      "\t 0.6 0.5846128446425624 0.6343023255813954\n",
      "\t 0.8 0.30840350008254913 0.0009539709038874314\n",
      "Analytics method: frequency\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6807000165098234 0.8097205824478553\n",
      "\t 0.8 0.4888558692421991 0.4296241709653648\n",
      "Analytics method: frequency\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.687964338781575 0.8151408450704225\n",
      "\t 0.01 0.687964338781575 0.8151408450704225\n",
      "\t 0.05 0.687964338781575 0.8151408450704225\n",
      "\t 0.1 0.687964338781575 0.8151408450704225\n",
      "\t 0.2 0.687964338781575 0.8151408450704225\n",
      "\t 0.4 0.6848274723460459 0.8127513487003433\n",
      "\t 0.6 0.6427274228165759 0.7598224195338512\n",
      "\t 0.8 0.31071487535083375 0.007606370335155693\n",
      "Analytics method: frequency\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE FREQUENCY\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.678058444774641 0.80810864003149\n",
      "\t 0.01 0.678058444774641 0.80810864003149\n",
      "\t 0.05 0.678058444774641 0.80810864003149\n",
      "\t 0.1 0.678058444774641 0.80810864003149\n",
      "\t 0.2 0.6782235430080898 0.8081881704556638\n",
      "\t 0.4 0.6747564801056629 0.8028817290374224\n",
      "\t 0.6 0.5846128446425624 0.6343023255813954\n",
      "\t 0.8 0.30840350008254913 0.0009539709038874314\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6866435529139838 0.8142129992169147\n",
      "\t 0.01 0.6866435529139838 0.8142129992169147\n",
      "\t 0.05 0.6866435529139838 0.8142129992169147\n",
      "\t 0.1 0.6866435529139838 0.8142129992169147\n",
      "\t 0.2 0.6868086511474327 0.81429270680372\n",
      "\t 0.4 0.6828462935446591 0.8110925361392468\n",
      "\t 0.6 0.640911342248638 0.7391147894926232\n",
      "\t 0.8 0.30972428595014034 0.004760771244941681\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6419019316493314 0.7816809260191243\n",
      "\t 0.01 0.6419019316493314 0.7816809260191243\n",
      "\t 0.05 0.6419019316493314 0.7816809260191243\n",
      "\t 0.1 0.6419019316493314 0.7816809260191243\n",
      "\t 0.2 0.6430576192834737 0.7822320709105559\n",
      "\t 0.4 0.6400858510813934 0.773105745212323\n",
      "\t 0.6 0.4919927356777282 0.4583700052807604\n",
      "\t 0.8 0.30823840184910023 0.0004770992366412214\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.5875846128446426 0.7334044823906083\n",
      "\t 0.01 0.5875846128446426 0.7334044823906083\n",
      "\t 0.05 0.5879148093115404 0.7335610589239967\n",
      "\t 0.1 0.5905563810467228 0.7348160821214712\n",
      "\t 0.2 0.618953277199934 0.7473730297723293\n",
      "\t 0.4 0.6387650652138022 0.7106585559375826\n",
      "\t 0.6 0.35892355951791316 0.13691931540342298\n",
      "\t 0.8 0.30807330361565133 0\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6866435529139838 0.8142129992169147\n",
      "\t 0.01 0.6866435529139838 0.8142129992169147\n",
      "\t 0.05 0.6866435529139838 0.8142129992169147\n",
      "\t 0.1 0.6866435529139838 0.8142129992169147\n",
      "\t 0.2 0.6868086511474327 0.81429270680372\n",
      "\t 0.4 0.6828462935446591 0.8110925361392468\n",
      "\t 0.6 0.640911342248638 0.7391147894926232\n",
      "\t 0.8 0.30972428595014034 0.004760771244941681\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6419019316493314 0.7816809260191243\n",
      "\t 0.01 0.6419019316493314 0.7816809260191243\n",
      "\t 0.05 0.6419019316493314 0.7816809260191243\n",
      "\t 0.1 0.6419019316493314 0.7816809260191243\n",
      "\t 0.2 0.6430576192834737 0.7822320709105559\n",
      "\t 0.4 0.6400858510813934 0.773105745212323\n",
      "\t 0.6 0.4919927356777282 0.4583700052807604\n",
      "\t 0.8 0.30823840184910023 0.0004770992366412214\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.5875846128446426 0.7334044823906083\n",
      "\t 0.01 0.5875846128446426 0.7334044823906083\n",
      "\t 0.05 0.5879148093115404 0.7335610589239967\n",
      "\t 0.1 0.5905563810467228 0.7348160821214712\n",
      "\t 0.2 0.618953277199934 0.7473730297723293\n",
      "\t 0.4 0.6387650652138022 0.7106585559375826\n",
      "\t 0.6 0.35892355951791316 0.13691931540342298\n",
      "\t 0.8 0.30807330361565133 0\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6866435529139838 0.8142129992169147\n",
      "\t 0.01 0.6866435529139838 0.8142129992169147\n",
      "\t 0.05 0.6866435529139838 0.8142129992169147\n",
      "\t 0.1 0.6866435529139838 0.8142129992169147\n",
      "\t 0.2 0.6868086511474327 0.81429270680372\n",
      "\t 0.4 0.6828462935446591 0.8110925361392468\n",
      "\t 0.6 0.640911342248638 0.7391147894926232\n",
      "\t 0.8 0.30972428595014034 0.004760771244941681\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6419019316493314 0.7816809260191243\n",
      "\t 0.01 0.6419019316493314 0.7816809260191243\n",
      "\t 0.05 0.6419019316493314 0.7816809260191243\n",
      "\t 0.1 0.6419019316493314 0.7816809260191243\n",
      "\t 0.2 0.6430576192834737 0.7822320709105559\n",
      "\t 0.4 0.6400858510813934 0.773105745212323\n",
      "\t 0.6 0.4919927356777282 0.4583700052807604\n",
      "\t 0.8 0.30823840184910023 0.0004770992366412214\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.5875846128446426 0.7334044823906083\n",
      "\t 0.01 0.5875846128446426 0.7334044823906083\n",
      "\t 0.05 0.5879148093115404 0.7335610589239967\n",
      "\t 0.1 0.5905563810467228 0.7348160821214712\n",
      "\t 0.2 0.618953277199934 0.7473730297723293\n",
      "\t 0.4 0.6387650652138022 0.7106585559375826\n",
      "\t 0.6 0.35892355951791316 0.13691931540342298\n",
      "\t 0.8 0.30807330361565133 0\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6866435529139838 0.8142129992169147\n",
      "\t 0.01 0.6866435529139838 0.8142129992169147\n",
      "\t 0.05 0.6866435529139838 0.8142129992169147\n",
      "\t 0.1 0.6866435529139838 0.8142129992169147\n",
      "\t 0.2 0.6868086511474327 0.81429270680372\n",
      "\t 0.4 0.6828462935446591 0.8110925361392468\n",
      "\t 0.6 0.640911342248638 0.7391147894926232\n",
      "\t 0.8 0.30972428595014034 0.004760771244941681\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6419019316493314 0.7816809260191243\n",
      "\t 0.01 0.6419019316493314 0.7816809260191243\n",
      "\t 0.05 0.6419019316493314 0.7816809260191243\n",
      "\t 0.1 0.6419019316493314 0.7816809260191243\n",
      "\t 0.2 0.6430576192834737 0.7822320709105559\n",
      "\t 0.4 0.6400858510813934 0.773105745212323\n",
      "\t 0.6 0.4919927356777282 0.4583700052807604\n",
      "\t 0.8 0.30823840184910023 0.0004770992366412214\n",
      "Analytics method: TFTF_tf\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TFTF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.5875846128446426 0.7334044823906083\n",
      "\t 0.01 0.5875846128446426 0.7334044823906083\n",
      "\t 0.05 0.5879148093115404 0.7335610589239967\n",
      "\t 0.1 0.5905563810467228 0.7348160821214712\n",
      "\t 0.2 0.618953277199934 0.7473730297723293\n",
      "\t 0.4 0.6387650652138022 0.7106585559375826\n",
      "\t 0.6 0.35892355951791316 0.13691931540342298\n",
      "\t 0.8 0.30807330361565133 0\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6869737493808816 0.8141905135241083\n",
      "\t 0.8 0.6456991910186561 0.6871720116618075\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.689615321116064 0.8162986124682431\n",
      "\t 0.01 0.689615321116064 0.8162986124682431\n",
      "\t 0.05 0.689615321116064 0.8162986124682431\n",
      "\t 0.1 0.689615321116064 0.8162986124682431\n",
      "\t 0.2 0.689615321116064 0.8162986124682431\n",
      "\t 0.4 0.6894502228826151 0.8161110567992961\n",
      "\t 0.6 0.6752517748060096 0.8029650405689672\n",
      "\t 0.8 0.4416377744758131 0.32414068745004\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6882945352484728 0.8153725796988069\n",
      "\t 0.01 0.6882945352484728 0.8153725796988069\n",
      "\t 0.05 0.6882945352484728 0.8153725796988069\n",
      "\t 0.1 0.6882945352484728 0.8153725796988069\n",
      "\t 0.2 0.6882945352484728 0.8153725796988069\n",
      "\t 0.4 0.6858180617467393 0.8134862295403312\n",
      "\t 0.6 0.6564305761928347 0.7743194881249322\n",
      "\t 0.8 0.34389962027406307 0.09845735027223232\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6869737493808816 0.8141905135241083\n",
      "\t 0.8 0.6511474327224699 0.6939898624185373\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.689615321116064 0.8162986124682431\n",
      "\t 0.01 0.689615321116064 0.8162986124682431\n",
      "\t 0.05 0.689615321116064 0.8162986124682431\n",
      "\t 0.1 0.689615321116064 0.8162986124682431\n",
      "\t 0.2 0.689615321116064 0.8162986124682431\n",
      "\t 0.4 0.6894502228826151 0.8161110567992961\n",
      "\t 0.6 0.6752517748060096 0.8029650405689672\n",
      "\t 0.8 0.4416377744758131 0.32414068745004\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6882945352484728 0.8153725796988069\n",
      "\t 0.01 0.6882945352484728 0.8153725796988069\n",
      "\t 0.05 0.6882945352484728 0.8153725796988069\n",
      "\t 0.1 0.6882945352484728 0.8153725796988069\n",
      "\t 0.2 0.6882945352484728 0.8153725796988069\n",
      "\t 0.4 0.6858180617467393 0.8134862295403312\n",
      "\t 0.6 0.6564305761928347 0.7743194881249322\n",
      "\t 0.8 0.34389962027406307 0.09845735027223232\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6869737493808816 0.8141905135241083\n",
      "\t 0.8 0.650982334489021 0.693800695249131\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.689615321116064 0.8162986124682431\n",
      "\t 0.01 0.689615321116064 0.8162986124682431\n",
      "\t 0.05 0.689615321116064 0.8162986124682431\n",
      "\t 0.1 0.689615321116064 0.8162986124682431\n",
      "\t 0.2 0.689615321116064 0.8162986124682431\n",
      "\t 0.4 0.6894502228826151 0.8161110567992961\n",
      "\t 0.6 0.6752517748060096 0.8029650405689672\n",
      "\t 0.8 0.4416377744758131 0.32414068745004\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6882945352484728 0.8153725796988069\n",
      "\t 0.01 0.6882945352484728 0.8153725796988069\n",
      "\t 0.05 0.6882945352484728 0.8153725796988069\n",
      "\t 0.1 0.6882945352484728 0.8153725796988069\n",
      "\t 0.2 0.6882945352484728 0.8153725796988069\n",
      "\t 0.4 0.6858180617467393 0.8134862295403312\n",
      "\t 0.6 0.6564305761928347 0.7743194881249322\n",
      "\t 0.8 0.34389962027406307 0.09845735027223232\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6869737493808816 0.8141905135241083\n",
      "\t 0.8 0.650982334489021 0.693800695249131\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.689615321116064 0.8162986124682431\n",
      "\t 0.01 0.689615321116064 0.8162986124682431\n",
      "\t 0.05 0.689615321116064 0.8162986124682431\n",
      "\t 0.1 0.689615321116064 0.8162986124682431\n",
      "\t 0.2 0.689615321116064 0.8162986124682431\n",
      "\t 0.4 0.6894502228826151 0.8161110567992961\n",
      "\t 0.6 0.6752517748060096 0.8029650405689672\n",
      "\t 0.8 0.4416377744758131 0.32414068745004\n",
      "Analytics method: TCTC_tc\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE TCTC\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6882945352484728 0.8153725796988069\n",
      "\t 0.01 0.6882945352484728 0.8153725796988069\n",
      "\t 0.05 0.6882945352484728 0.8153725796988069\n",
      "\t 0.1 0.6882945352484728 0.8153725796988069\n",
      "\t 0.2 0.6882945352484728 0.8153725796988069\n",
      "\t 0.4 0.6858180617467393 0.8134862295403312\n",
      "\t 0.6 0.6564305761928347 0.7743194881249322\n",
      "\t 0.8 0.34389962027406307 0.09845735027223232\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6901106158164108 0.816573829766442\n",
      "\t 0.8 0.6757470695063563 0.7692126909518214\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.4914974409773815 0.427296392711045\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 1\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.3298662704309064 0.06106870229007634\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6901106158164108 0.816573829766442\n",
      "\t 0.8 0.6760772659732541 0.7704726251754798\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.4914974409773815 0.427296392711045\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 2\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.3298662704309064 0.06106870229007634\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6901106158164108 0.816573829766442\n",
      "\t 0.8 0.6760772659732541 0.7704726251754798\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.4914974409773815 0.427296392711045\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 3\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.3298662704309064 0.06106870229007634\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 10\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6915964999174509 0.8176849502244778\n",
      "\t 0.6 0.6901106158164108 0.816573829766442\n",
      "\t 0.8 0.6760772659732541 0.7704726251754798\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 20\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6906059105167575 0.8169921875\n",
      "\t 0.01 0.6906059105167575 0.8169921875\n",
      "\t 0.05 0.6906059105167575 0.8169921875\n",
      "\t 0.1 0.6906059105167575 0.8169921875\n",
      "\t 0.2 0.6906059105167575 0.8169921875\n",
      "\t 0.4 0.6904408122833086 0.816876648110167\n",
      "\t 0.6 0.6772329536073964 0.8071801952855312\n",
      "\t 0.8 0.4914974409773815 0.427296392711045\n",
      "Analytics method: CFCF_cf\n",
      "N-gram max length: 4\n",
      "Upper cut-off percentage: 30\n",
      "ANALYSE CFCF\n",
      "\tThreshold, accuracy, F1 score:\n",
      "\t 0.0 0.6889549281822684 0.8158357771260998\n",
      "\t 0.01 0.6889549281822684 0.8158357771260998\n",
      "\t 0.05 0.6889549281822684 0.8158357771260998\n",
      "\t 0.1 0.6889549281822684 0.8158357771260998\n",
      "\t 0.2 0.6889549281822684 0.8158357771260998\n",
      "\t 0.4 0.6877992405481261 0.8149525393874156\n",
      "\t 0.6 0.6595674426283639 0.7882087099424816\n",
      "\t 0.8 0.3298662704309064 0.06106870229007634\n"
     ]
    }
   ],
   "source": [
    "for analytics_method in ['normalize', 'normalize_uniq', 'TF-IDF', 'frequency', 'TFTF_tf', 'TCTC_tc', 'CFCF_cf']:\n",
    "    for ngram_max in range (1, 5):\n",
    "        for upper_cut_off_percentage in [10, 20, 30]:\n",
    "            analyse_dataset(analytics_method, ngram_max, upper_cut_off_percentage, df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
