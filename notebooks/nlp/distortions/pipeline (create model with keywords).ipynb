{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pygents.aigents_api import tokenize_re\n",
    "from pygents.util import dictcount\n",
    "from pygents.aigents_api import build_ngrams\n",
    "from collections import defaultdict\n",
    "\n",
    "from pygents.aigents_api import TextMetrics\n",
    "\n",
    "\n",
    "def language_metrics(metrics_list):\n",
    "    metrics = {}\n",
    "    for m in metrics_list:\n",
    "        metrics[m] = './data/corpora/English/distortions/our model/keywods for our model/' + m + '.txt'\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient Question</th>\n",
       "      <th>Distorted part</th>\n",
       "      <th>Dominant Distortion</th>\n",
       "      <th>Secondary Distortion (Optional)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm such a failure I never do anything right.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody likes me because I'm not interesting.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't try new things because I'll just mess...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My boss didn't say 'good morning' she must be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My friend didn't invite me to the party I mus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>Since then whenever my mother is out alone I b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>My family hate him but they didn’t met him at ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3524</th>\n",
       "      <td>However I am not happy at the least only half ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>Now I am at university my peers around me all ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>He claims he’s severely depressed and has outb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3527 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Patient Question  Distorted part  \\\n",
       "0         I'm such a failure I never do anything right.             NaN   \n",
       "1          Nobody likes me because I'm not interesting.             NaN   \n",
       "2      I can't try new things because I'll just mess...             NaN   \n",
       "3      My boss didn't say 'good morning' she must be...             NaN   \n",
       "4      My friend didn't invite me to the party I mus...             NaN   \n",
       "...                                                 ...             ...   \n",
       "3522  Since then whenever my mother is out alone I b...             NaN   \n",
       "3523  My family hate him but they didn’t met him at ...             NaN   \n",
       "3524  However I am not happy at the least only half ...             NaN   \n",
       "3525  Now I am at university my peers around me all ...             NaN   \n",
       "3526  He claims he’s severely depressed and has outb...             NaN   \n",
       "\n",
       "     Dominant Distortion  Secondary Distortion (Optional)  \n",
       "0             Distortion                              NaN  \n",
       "1             Distortion                              NaN  \n",
       "2             Distortion                              NaN  \n",
       "3             Distortion                              NaN  \n",
       "4             Distortion                              NaN  \n",
       "...                  ...                              ...  \n",
       "3522          Distortion                              NaN  \n",
       "3523          Distortion                              NaN  \n",
       "3524          Distortion                              NaN  \n",
       "3525          Distortion                              NaN  \n",
       "3526          Distortion                              NaN  \n",
       "\n",
       "[3527 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(r\"./data/corpora/English/distortions/sagarikashreevastava/test.csv\")\n",
    "#df = pd.read_csv(r\"./notebooks/nlp/sentiment/first dataset dominant and secondary distortions.csv\")\n",
    "#df = pd.read_csv(r\"./data/corpora/English/distortions/our model/first dataset dominant and secondary distortions.csv\")\n",
    "#df = df.drop('Id_Number', axis=1) # delete columnb with id \n",
    "binary_dataset_file_path = \"./data/corpora/English/distortions/halilbabacan/raw_Cognitive_distortions.csv\"\n",
    "df = pd.read_csv(binary_dataset_file_path)\n",
    "df.insert(1, \"Distorted part\", value = np.nan)\n",
    "df.insert(3, \"Secondary Distortion (Optional)\", value = np.nan)\n",
    "df = df.rename(columns={\n",
    "    \"Text\": \"Patient Question\",\n",
    "    \"Label\": \"Dominant Distortion\"\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(test_df, distortions_labels):\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    # Get the results using our model\n",
    "    \n",
    "    l = TextMetrics(language_metrics(distortions_labels),debug=False)\n",
    "    metric_to_index_and_column = {\n",
    "        label: (i, label) for i, label in enumerate(distortions_labels)\n",
    "    }\n",
    "    columns_values = np.zeros((len(test_df), len(metric_to_index_and_column)))\n",
    "    texts = test_df['Patient Question'] \n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        metrics = l.get_sentiment_words(t)\n",
    "        for metric_name, metric in metrics.items():\n",
    "            index, _ = metric_to_index_and_column[metric_name]\n",
    "            columns_values[i][index] = metric\n",
    "\n",
    "    for _, (index, column_name) in metric_to_index_and_column.items():\n",
    "        our_column_name = f\"Our {column_name}\"\n",
    "        their_column_name = f\"Their {column_name}\"\n",
    "        df[our_column_name] = pd.Series(columns_values[:, index])\n",
    "        df[their_column_name] = 0  # Fill \"Their\" columns with zeros\n",
    "\n",
    "    # Update \"Their\" columns based on Dominant and Secondary Distortions\n",
    "    for i, row in df.iterrows():\n",
    "        dominant_distortion = row.get('Dominant Distortion', None)\n",
    "        secondary_distortion = row.get('Secondary Distortion (Optional)', None)\n",
    "        \n",
    "        for _, (_, column_name) in metric_to_index_and_column.items():\n",
    "            their_column_name = f\"Their {column_name}\"\n",
    "            if dominant_distortion == column_name or secondary_distortion == column_name:\n",
    "                df.at[i, their_column_name] = 1\n",
    "\n",
    "        # Update \"Their Distortion\" column\n",
    "        their_distortion_column = \"Their Distortion\"\n",
    "        if dominant_distortion != \"No Distortion\":\n",
    "            df.at[i, their_distortion_column] = 1\n",
    "\n",
    "    # Calculate F1 scores for thresholds\n",
    "    thresholds = np.arange(0, 1.1, 0.1)  # Thresholds from 0 to 1 with step 0.1\n",
    "    accuracy_scores = pd.DataFrame(index=distortions_labels, columns=thresholds)\n",
    "\n",
    "    for column_name in distortions_labels:\n",
    "        our_column_name = f\"Our {column_name}\"\n",
    "        their_column_name = f\"Their {column_name}\"\n",
    "        for threshold in thresholds:\n",
    "            # Binarize \"Our\" column based on the threshold\n",
    "            our_binary = (df[our_column_name] >= threshold).astype(int)\n",
    "            their_binary = df[their_column_name]\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            #f1 = f1_score(their_binary, our_binary, zero_division=0)\n",
    "            #f1_scores.at[column_name, threshold] = f1\n",
    "\n",
    "            # Calculate accuracy score\n",
    "            accuracy = (our_binary == their_binary).mean()\n",
    "            accuracy_scores.at[column_name, threshold] = accuracy\n",
    "            \n",
    "\n",
    "    # Display the accuracy scores as a heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Format thresholds for better readability\n",
    "    formatted_thresholds = [f\"{th:.1f}\" for th in thresholds]\n",
    "    accuracy_scores.columns = formatted_thresholds\n",
    "\n",
    "    sns.heatmap(accuracy_scores.astype(float), annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 13}, vmin=0, vmax=0.8)\n",
    "    plt.title(\"Accuracy Heatmap\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Cognitive Distortion\")\n",
    "\n",
    "    # Adjust the x-axis labels to be horizontal\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "    # Save the plot\n",
    "    output_dir = \"C:/Users/kzvau/Desktop/работа нгу\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"f1_scores_heatmap.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    threshold_05 = accuracy_scores['0.5'].astype(float)\n",
    "    average_accuracy_05 = threshold_05.mean()\n",
    "    return average_accuracy_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_normalized_ngrams(ngram_max, df):\n",
    "    print('ANALYSE NORMALIZE')\n",
    "    # Analyze CFCF_cf values for n-grams for each cognitive distortion (see https://github.com/aigents/pygents/blob/main/notebooks/nlp/distortions/distortions_preliminary_data_checks.ipynb)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "    distortions = defaultdict(int) # Counting the number of documents for each distortion\n",
    "    n_gram_distortions = defaultdict(lambda: defaultdict(int))  # Distortions per n-gram\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        dictcount(distortions, primary_distortion)\n",
    "        if secondary_distortion:\n",
    "            dictcount(distortions, secondary_distortion)\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Generate and count n-grams\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "            # Count unique n-grams\n",
    "            uniq_n_grams = set(n_grams)\n",
    "            for uniq_n_gram in uniq_n_grams:\n",
    "                dictcount(unique_n_gram_dicts[primary_distortion], uniq_n_gram)\n",
    "                dictcount(n_gram_distortions[uniq_n_gram], primary_distortion)\n",
    "                if secondary_distortion:\n",
    "                    dictcount(unique_n_gram_dicts[secondary_distortion], uniq_n_gram)\n",
    "                    dictcount(n_gram_distortions[uniq_n_gram], secondary_distortion)\n",
    "\n",
    "    \n",
    "    # Normalizing distortion-specific counts by total counts\n",
    "    norm_n_gram_dicts = {}\n",
    "    for n_gram_dict in n_gram_dicts:\n",
    "        norm_n_gram_dict = {}\n",
    "        norm_n_gram_dicts[n_gram_dict] = norm_n_gram_dict\n",
    "        dic = n_gram_dicts[n_gram_dict]\n",
    "        for n_gram in dic:\n",
    "            if len(n_gram) <= ngram_max:\n",
    "                norm_n_gram_dict[n_gram] = float( dic[n_gram] ) / all_n_grams[n_gram]\n",
    "\n",
    "    # Normalize uniq counts \n",
    "    norm_uniq_n_gram_dicts = {}\n",
    "    for uniq_n_gram_dict in unique_n_gram_dicts:\n",
    "        norm_uniq_n_gram_dict = {}\n",
    "        norm_uniq_n_gram_dicts[uniq_n_gram_dict] = norm_uniq_n_gram_dict\n",
    "        dic = unique_n_gram_dicts[uniq_n_gram_dict]\n",
    "        # Normalize uniq Document counts of N-grams by distortion by Documents count by Distortion\n",
    "        for n_gram in dic:\n",
    "            if len(n_gram) <= ngram_max:\n",
    "                norm_uniq_n_gram_dict[n_gram] = float( dic[n_gram] ) / distortions[uniq_n_gram_dict] / len(n_gram_distortions[n_gram])\n",
    "\n",
    "    return norm_uniq_n_gram_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_frequency(ngram_max, df):\n",
    "    print('ANALYSE FREQUENCY')\n",
    "    # Analyze the frequency of n-grams for each cognitive distortion\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0]\n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Generation and counting of n-grams (from 1 to 4)\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams) # Increment the counter for the corresponding secondary distortion (if present)\n",
    "\n",
    "    return n_gram_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TF_IDF(ngram_max, df):\n",
    "    print('ANALYSE TF-IDF')\n",
    "    # Analyze TF-IDF values for n-grams for each cognitive distortion\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of documents in which each n-gram appears\n",
    "\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0]\n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Generate n-grams and update the document counters where they appear\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams) # Increment the counter for the corresponding secondary distortion (if present)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # TF-IDF Calculation\n",
    "    tfidf_dicts = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), analyze the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            tf = count / sum(ngram_dict.values())  # Frequency of the n-gram in the text (TF): TF = (Number of occurrences of the given n-gram for the specific cognitive distortion) / (Total number of occurrences of all other n-grams for the same cognitive distortion)\n",
    "            idf = math.log(total_docs / (1 + doc_counts[n_gram]))  # Inverse Document Frequency (IDF): IDF = Total number of documents / Number of documents containing the given n-gram\n",
    "            tfidf_dicts[distortion][n_gram] = tf * idf  # TF-IDF\n",
    "\n",
    "    return tfidf_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TFTF_tf(ngram_max, df):\n",
    "    print('ANALYSE TFTF')\n",
    "    # Analyze TFTF_tf values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[primary_distortion][n_gram] += 1\n",
    "            if secondary_distortion:\n",
    "                unique_n_gram_dicts[secondary_distortion][n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # Calculation of the TFTF_tf\n",
    "    tftf_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # TFTF: Mutual relevance of features and text (how important a given n-gram is for describing the text, looking for specific n-grams within the text)\n",
    "            tf = count / sum(ngram_dict.values()) # Frequency of the n-gram in the text (TF): TF = (Number of times the given n-gram appears for the specific cognitive distortion) / (Number of times all other n-grams appear for the same cognitive distortion)\n",
    "            ft = doc_counts[n_gram] / total_docs # The number of texts in which the current n-gram appears / the total number of texts in the dataset\n",
    "            Ft = sum(ngram_dict.values())\n",
    "            tftf = (tf ** 2) / (ft*Ft if ft*Ft > 0 else 1)\n",
    "\n",
    "            tftf_results[distortion][n_gram] = tftf\n",
    "\n",
    "    return tftf_results\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TCTC_tc(ngram_max, df):\n",
    "    print('ANALYSE TCTC')\n",
    "    # Analyze TCTC_tc values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[primary_distortion][n_gram] += 1\n",
    "            if secondary_distortion:\n",
    "                unique_n_gram_dicts[secondary_distortion][n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # Calculation of the TCTC_tc\n",
    "    tctc_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # TCTC: Mutual relevance of categories and text (how the given n-gram is distributed across all texts of a given category)\n",
    "            ct = unique_n_gram_dicts[distortion][n_gram] # The number of texts in which the current n-gram (n_gram) is associated with the current distortion (distortion)\n",
    "            tctc = (ct ** 2) / (doc_counts[n_gram] * total_docs) # In the denominator: the number of texts containing the given n-gram, regardless of distortion * the total number of texts\n",
    "\n",
    "            tctc_results[distortion][n_gram] = tctc\n",
    "\n",
    "    return tctc_results\n",
    "\n",
    "\n",
    "\n",
    "def analyse_CFCF_cf(ngram_max, df):\n",
    "    print('ANALYSE CFCF')\n",
    "    # Analyze CFCF_cf values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = tokenize_re(text)\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[primary_distortion][n_gram] += 1\n",
    "            if secondary_distortion:\n",
    "                unique_n_gram_dicts[secondary_distortion][n_gram] += 1\n",
    "\n",
    "    # Calculation of the CFCF_cf\n",
    "    cfcf_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # CFCF: Mutual relevance of features and categories (how characteristic the given n-gram is for a particular category)\n",
    "            cf = unique_n_gram_dicts[distortion][n_gram]\n",
    "            fc = sum(unique_n_gram_dicts[distortion].values()) # The total number of texts for all n-grams associated with the given distortion\n",
    "            cfcf = (cf ** 2) / (cf * fc if cf * fc > 0 else 1)\n",
    "\n",
    "            cfcf_results[distortion][n_gram] = cfcf\n",
    "\n",
    "    return cfcf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_df, test_df, analytics_method, ngram_max, treshold):\n",
    "    print('CREATE MODEL')\n",
    "    # Choose a method for analysis\n",
    "    if analytics_method == 'frequency':\n",
    "        model_ngram = analyse_frequency(ngram_max, train_df)\n",
    "    elif analytics_method == 'TF-IDF':\n",
    "        model_ngram = analyse_TF_IDF(ngram_max, train_df)\n",
    "    elif analytics_method == 'TFTF_tf':\n",
    "        model_ngram = analyse_TFTF_tf(ngram_max, train_df)\n",
    "    elif analytics_method == 'TCTC_tc':\n",
    "        model_ngram = analyse_TCTC_tc(ngram_max, train_df)\n",
    "    elif analytics_method == 'CFCF_cf':\n",
    "        model_ngram = analyse_CFCF_cf(ngram_max, train_df)\n",
    "    elif analytics_method == 'normalize':\n",
    "        model_ngram = analyse_normalized_ngrams(ngram_max, train_df)\n",
    "\n",
    "    # Filter out values below the threshold\n",
    "    filtered_model_ngram = {}\n",
    "    for distortion, ngram_dict in model_ngram.items():\n",
    "        # Find the maximum metric value for the current distortion\n",
    "        max_value = max(ngram_dict.values()) if ngram_dict else 0\n",
    "        threshold_value = max_value * (treshold / 100)\n",
    "\n",
    "        # Filter n-grams that meet or exceed the threshold value\n",
    "        filtered_model_ngram[distortion] = {\n",
    "            ngram: metric for ngram, metric in ngram_dict.items() if metric >= threshold_value\n",
    "        }\n",
    "\n",
    "    # Create .txt files for each distortion\n",
    "    output_dir = \"./data/corpora/English/distortions/our model/keywods for our model\"\n",
    "    distortion_file_path = f\"{output_dir}/Distortion.txt\"\n",
    "    distortions_labels = []\n",
    "\n",
    "    with open(distortion_file_path, \"w\", encoding=\"utf-8\") as distortion_file:\n",
    "        distortions_labels.append(\"Distortion\")\n",
    "        for distortion, ngrams in filtered_model_ngram.items():\n",
    "            file_path = f\"{output_dir}/{distortion}.txt\"\n",
    "            distortions_labels.append(distortion)\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for ngram in ngrams.keys():\n",
    "                    f.write(\" \".join(ngram) + \"\\n\")\n",
    "                    if distortion != \"No Distortion\":\n",
    "                            distortion_file.write(\" \".join(ngram) + \"\\n\")\n",
    "\n",
    "    accuracy = model_comparison(test_df, list(set(distortions_labels)))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for creating a model based on hyperparameters. \n",
    "# The analytics_method can take values such as frequency, TF-IDF, TFTF_tf, TCTC_tc, CFCF_cf. \n",
    "# ngram_max is the number corresponding to the maximum length of n-grams we will identify. \n",
    "# threshold contains the desired cutoff values (in %) for filtering out irrelevant keywords for each cognitive distortion. \n",
    "# split_number reflects the number of parts the dataset will be split into for training and recognition.\n",
    "\n",
    "def analyse_dataset(analytics_method, ngram_max, treshold, num_parts, df):\n",
    "    print('ANALYSE DATASET')\n",
    "\n",
    "    # Split the dataset into num_parts parts\n",
    "    parts = []\n",
    "    part_size = len(df) // num_parts\n",
    "    \n",
    "    for i in range(num_parts):\n",
    "        start_idx = i * part_size\n",
    "        end_idx = start_idx + part_size if i < num_parts - 1 else len(df)\n",
    "        parts.append(df.iloc[start_idx:end_idx])\n",
    "\n",
    "    # Generate combinations for training and testing\n",
    "    combinations = [] # A dictionary of (train, test) pairs\n",
    "    for i in range(num_parts):\n",
    "        test_set = parts[i]\n",
    "        train_set = pd.concat([parts[j] for j in range(num_parts) if j != i])\n",
    "        combinations.append((train_set, test_set))\n",
    "        # Create a model and comparing our model with markup\n",
    "        accuracy = create_model(combinations[i][0], combinations[i][1], analytics_method, ngram_max, treshold)\n",
    "        print(f\"The average accuracy value for a threshold of 0.5 across all distortions for combination {i+1}: {accuracy}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DATASET\n",
      "CREATE MODEL\n",
      "ANALYSE NORMALIZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzvau\\AppData\\Local\\Temp\\ipykernel_5688\\39694277.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = row[1] if pd.notna(row[1]) else row[0]\n",
      "C:\\Users\\kzvau\\AppData\\Local\\Temp\\ipykernel_5688\\39694277.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
      "C:\\Users\\kzvau\\AppData\\Local\\Temp\\ipykernel_5688\\39694277.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPARISON\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x98 in position 1158: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43manalyse_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormalize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 26\u001b[0m, in \u001b[0;36manalyse_dataset\u001b[1;34m(analytics_method, ngram_max, treshold, num_parts, df)\u001b[0m\n\u001b[0;32m     24\u001b[0m combinations\u001b[38;5;241m.\u001b[39mappend((train_set, test_set))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create a model and comparing our model with markup\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombinations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombinations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalytics_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe average accuracy value for a threshold of 0.5 across all distortions for combination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 45\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(train_df, test_df, analytics_method, ngram_max, treshold)\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m distortion \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Distortion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     43\u001b[0m                         distortion_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdistortions_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m, in \u001b[0;36mmodel_comparison\u001b[1;34m(test_df, distortions_labels)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL COMPARISON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get the results using our model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mTextMetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistortions_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m metric_to_index_and_column \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     label: (i, label) \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(distortions_labels)\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m      9\u001b[0m columns_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(test_df), \u001b[38;5;28mlen\u001b[39m(metric_to_index_and_column)))\n",
      "File \u001b[1;32mc:\\Users\\kzvau\\Desktop\\работа нгу\\pygents\\pygents\\aigents_api.py:364\u001b[0m, in \u001b[0;36mTextMetrics.__init__\u001b[1;34m(self, metrics, metric_logarithmic, tokenize_chars, scrub, debug)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;66;03m#TODO use unsupervised tokenization!!!!\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chars: \u001b[38;5;66;03m# for chinese\u001b[39;00m\n\u001b[0;32m    366\u001b[0m         ngrams \u001b[38;5;241m=\u001b[39m split_patterns(ngrams)\n",
      "File \u001b[1;32mc:\\Users\\kzvau\\Desktop\\работа нгу\\pygents\\pygents\\aigents_api.py:266\u001b[0m, in \u001b[0;36mPygentsSentiment.to_set\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(arg)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(arg) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m null\n",
      "File \u001b[1;32mc:\\Users\\kzvau\\Desktop\\работа нгу\\pygents\\pygents\\aigents_api.py:217\u001b[0m, in \u001b[0;36mload_ngrams\u001b[1;34m(file, debug)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 217\u001b[0m         lines \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m ngrams \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(l\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lines \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(l) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(ngrams)\n",
      "File \u001b[1;32mc:\\Users\\kzvau\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\encodings\\cp1251.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x98 in position 1158: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "analyse_dataset('normalize', 4, 20, 3, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
