{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd61e07f",
   "metadata": {},
   "source": [
    "# Russian Tokenization Experiments - PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6301bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * \n",
    "\n",
    "lex_en = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\"\n",
    "lex_ru = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_russian.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2282ae",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9558d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tunaisafish.catisamammal\"\n",
    "expected = ['tuna', 'is', 'a', 'fish', '.', 'cat', 'ia', 'a', 'mammal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5342ad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'isa', 'fish', '.', 'cati', 'sama', 'mma', 'l']\n",
      "2.9807663087309058 0.35294117647058826\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b484a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'un', 'a', 'is', 'a', 'f', 'is', 'h', '.', 'c', 'a', 't', 'is', 'a', 'm', 'a', 'm', 'm', 'a', 'l']\n",
      "5.729628877142061 0.2758620689655173\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_en, sortmode=1)\n",
    "tokens, weight = lt1.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33af0ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'is', 'af', 'is', 'h', '.', 'cat', 'is', 'ama', 'mm', 'al']\n",
      "4.544507159677375 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_en, sortmode=2)\n",
    "tokens, weight = lt2.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5841a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '.', ' ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tuna is a fish. Cat is a mammal\"\n",
    "expected = tokenize_split_with_delimiters_and_quotes(text)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00030ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '. ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n",
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '.', ' ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n",
      "2.923531848929005 0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0, cased = True)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(expected)\n",
    "print(weight,calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479c537",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d62bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"расцветалияблониигруши,поплылитуманынадрекой\"\n",
    "expected = ['расцветали', 'яблони', 'игруши', ',', 'поплыли', 'туманы', 'над', 'рекой']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79c51a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['расцвета', 'лия', 'бл', 'они', 'игру', 'ши', ',', 'поплыли', 'туманы', 'над', 'рекой']\n",
      "4.034137991553761 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8125a105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ра', 'с', 'цвет', 'али', 'яблони', 'игр', 'уши', ',', 'по', 'плыли', 'ту', 'ма', 'ны', 'на', 'др', 'е', 'ко', 'й']\n",
      "5.526441728891956 0.15384615384615383\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_ru, sortmode=1)\n",
    "tokens, weight = lt1.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d55380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['расцвета', 'ли', 'яблони', 'игру', 'ши', ',', 'поплыли', 'туман', 'ы', 'над', 'рекой']\n",
      "4.027652477481268 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_ru, sortmode=2)\n",
    "tokens, weight = lt2.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd22998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.734799829588847"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.fulldict['туманы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667eb94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.053785038134658"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.fulldict['авва']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4059884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Расцветали', ' ', 'яблони', ' ', 'и', ' ', 'груши', ',', ' ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n"
     ]
    }
   ],
   "source": [
    "text = \"Расцветали яблони и груши, поплыли туманы над рекой\"\n",
    "expected = tokenize_split_with_delimiters_and_quotes(text)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5677e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Расцвета', 'ли', ' ', 'яблони', ' и ', 'груши', ', ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n",
      "['Расцветали', ' ', 'яблони', ' ', 'и', ' ', 'груши', ',', ' ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n",
      "2.667408660328346 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0, cased = True)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(expected)\n",
    "print(weight,calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6be6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "280e8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO compute score with account to number of letters in token AND / OR log of frequency?\n",
    "#TODO build alternative graphs and score them (by SOME scoring function)!?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c872f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
