{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8f149c",
   "metadata": {},
   "source": [
    "# Russian Tokenization Experiments - PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62e5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * \n",
    "\n",
    "lex_en = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\"\n",
    "lex_ru = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_russian.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388b3c5",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad06a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tunaisafish.catisamammal\"\n",
    "expected = ['tuna', 'is', 'a', 'fish', '.', 'cat', 'ia', 'a', 'mammal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efc61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'isa', 'fish', '.', 'cati', 'sama', 'mma', 'l']\n",
      "2.9807663087309058 0.35294117647058826\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e2021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'un', 'a', 'is', 'a', 'f', 'is', 'h', '.', 'c', 'a', 't', 'is', 'a', 'm', 'a', 'm', 'm', 'a', 'l']\n",
      "5.729628877142061 0.2758620689655173\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_en, sortmode=1)\n",
    "tokens, weight = lt1.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98623e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'is', 'af', 'is', 'h', '.', 'cat', 'is', 'ama', 'mm', 'al']\n",
      "4.544507159677375 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_en, sortmode=2)\n",
    "tokens, weight = lt2.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c8650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '.', ' ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tuna is a fish. Cat is a mammal\"\n",
    "expected = tokenize_split_with_delimiters_and_quotes(text)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29bafbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '. ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n",
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '.', ' ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n",
      "2.923531848929005 0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0, cased = True)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(expected)\n",
    "print(weight,calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29326c",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89700499",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"расцветалияблониигруши,поплылитуманынадрекой\"\n",
    "expected = ['расцветали', 'яблони', 'игруши', ',', 'поплыли', 'туманы', 'над', 'рекой']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a78dc262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['расцвета', 'лия', 'бл', 'они', 'игру', 'ши', ',', 'поплыли', 'туманы', 'над', 'рекой']\n",
      "4.034137991553761 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75b362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ра', 'с', 'цвет', 'али', 'яблони', 'игр', 'уши', ',', 'по', 'плыли', 'ту', 'ма', 'ны', 'на', 'др', 'е', 'ко', 'й']\n",
      "5.526441728891956 0.15384615384615383\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_ru, sortmode=1)\n",
    "tokens, weight = lt1.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62d8f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['расцвета', 'ли', 'яблони', 'игру', 'ши', ',', 'поплыли', 'туман', 'ы', 'над', 'рекой']\n",
      "4.027652477481268 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_ru, sortmode=2)\n",
    "tokens, weight = lt2.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb7d3c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.734799829588847"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.fulldict['туманы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ce69297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.053785038134658"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.fulldict['авва']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa57237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Расцветали', ' ', 'яблони', ' ', 'и', ' ', 'груши', ',', ' ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n"
     ]
    }
   ],
   "source": [
    "text = \"Расцветали яблони и груши, поплыли туманы над рекой\"\n",
    "expected = tokenize_split_with_delimiters_and_quotes(text)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02a6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Расцвета', 'ли', ' ', 'яблони', ' и ', 'груши', ', ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n",
      "['Расцветали', ' ', 'яблони', ' ', 'и', ' ', 'груши', ',', ' ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n",
      "2.667408660328346 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0, cased = True)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(expected)\n",
    "print(weight,calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d202421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.850033257689769 True False\n"
     ]
    }
   ],
   "source": [
    "print(lt0.fulldict['расцвета'],'расцвета' in lt0.fulldict,'расцветали' in lt0.fulldict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37caa8",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac27d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/yishn/chinese-tokenizer\n",
    "# This tokenizer uses a simple greedy algorithm: It always looks for the longest word in the CC-CEDICT dictionary that matches the input, one at a time.\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83983bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akolonin/Documents/aigents/pygents/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (3,4,8,9,12,13,17,18,22,23,28,29,111,112,127,128) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48644\n",
      "['中东', '马队', '门徒', '申讨', '曲']\n"
     ]
    }
   ],
   "source": [
    "path = '../../nlp/corpora/Chinese/'\n",
    "cld_df = pd.read_csv(os.path.join(path,'lexicon/chineselexicaldatabase2.1.txt'))\n",
    "wordlist = list(cld_df['Word'])\n",
    "print(len(wordlist))\n",
    "print(wordlist[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca02758",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhlt0 = LexiconIndexedTokenizer(lexicon = wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03f92768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中东', '马队', '门徒', '申讨', '曲']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhlt0.tokenize('中东马队门徒申讨曲')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc514ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/lb/1m7gbdp17h578qq48pbbtxf40000gn/T/jieba.cache\n",
      "Loading model cost 0.945 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['狗', '、', '猫', '、', '老鼠', '和', '猪', '都', '是', '哺乳动物']\n",
      "['狗', '、', '猫', '、', '老鼠', '和', '猪', '都', '是', '哺乳', '动物']\n",
      "0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "#'Dogs, cats, mice and pigs are all mammals'\n",
    "expected = [r[0] for r in jieba.tokenize('狗、猫、老鼠和猪都是哺乳动物')]\n",
    "tokens = zhlt0.tokenize('狗、猫、老鼠和猪都是哺乳动物')\n",
    "print(expected)\n",
    "print(tokens)\n",
    "print(calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "072f8e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['苹果树', '和', '梨树', '开花', '，', '雾气', '飘过', '河面', '。']\n",
      "['苹果树', '和', '梨树', '开花', '，', '雾气', '飘', '过', '河面', '。']\n",
      "0.8421052631578948\n"
     ]
    }
   ],
   "source": [
    "#'Цвели яблони и груши, над рекой плыл туман.'\n",
    "expected = [r[0] for r in jieba.tokenize('苹果树和梨树开花，雾气飘过河面。')]\n",
    "tokens = zhlt0.tokenize('苹果树和梨树开花，雾气飘过河面。')\n",
    "print(expected)\n",
    "print(tokens)\n",
    "print(calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103d028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1156427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de736c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO compute score with account to number of letters in token AND / OR log of frequency?\n",
    "#TODO build alternative graphs and score them (by SOME scoring function)!?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbddccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
