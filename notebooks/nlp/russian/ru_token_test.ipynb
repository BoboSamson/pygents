{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214748e3",
   "metadata": {},
   "source": [
    "# Russian Tokenization Experiments - PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e588a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * \n",
    "\n",
    "lex_en = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\"\n",
    "lex_ru = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_russian.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2fee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygents.text import tokenize_with_sorted_lexicon\n",
    "\n",
    "class LexiconTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, name=None, lexicon=None, cased=False, url=None, debug=False):\n",
    "        Tokenizer.__init__(self,debug=debug)\n",
    "        self.name = name\n",
    "        if not lexicon is None: \n",
    "            self.alex = list(lexicon) #copy\n",
    "        else:\n",
    "            lex_lines = url_lines(url)\n",
    "            self.alex = [re.split('\\t| |,|;|\\n|\\r',line)[0] for line in lex_lines] #load from url\n",
    "            # TODO load from file\n",
    "        self.compile()\n",
    "        self.cased = cased\n",
    "\n",
    "    def compile(self):\n",
    "        self.alex.sort(key=len,reverse=True) #precompile\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        return tokenize_with_sorted_lexicon(self.alex,text,cased=self.cased)\n",
    "\n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"tunaisafish.catisamammal\"))==\"['tuna', 'is', 'a', 'fish', '.', 'cat', 'is', 'a', 'mammal']\"    \n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.Cat', 'is', 'a', 'mammal']\"\n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal'],cased=True).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.', 'Cat', 'is', 'a', 'mammal']\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ef7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixed_match_from_list(lst,text):\n",
    "    for item in lst:\n",
    "        if text.startswith(item[0]):\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "def prefixed_match(prefixed_dict,text):\n",
    "    letter = text[0]\n",
    "    if not letter in prefixed_dict:\n",
    "        return None\n",
    "    return prefixed_match_from_list(prefixed_dict[letter],text)\n",
    "\n",
    "def tokenize_with_prexied_sorted_lexicon(prefixed_dict,text,cased=False):\n",
    "    original = text\n",
    "    if cased: #if need to spend time on lowercasing non-lowercased text\n",
    "        text = text.lower()\n",
    "    tokens = []\n",
    "    start = 0\n",
    "    cur = 0\n",
    "    length = len(text)\n",
    "    sum_weight = 0\n",
    "    while cur < length:\n",
    "        subtext = text[cur:]\n",
    "        word_weight = prefixed_match(prefixed_dict,subtext)\n",
    "        #print(al)\n",
    "        if not word_weight is None:\n",
    "            word_len = len(word_weight[0])\n",
    "            if start < cur:\n",
    "                tokens.append(original[start:cur])\n",
    "            tokens.append(original[cur:cur+word_len])\n",
    "            sum_weight += word_weight[1]\n",
    "            cur += word_len\n",
    "            start = cur\n",
    "        else:\n",
    "            cur += 1\n",
    "            #print('yo')\n",
    "    if start < cur:\n",
    "        tokens.append(original[start:cur])\n",
    "        #print(original[start:cur])\n",
    "    return tokens, sum_weight\n",
    "\n",
    "def tabbed_line2tuple(line,log=True):\n",
    "    lst = re.split('\\t| |,|;|\\n|\\r',line)\n",
    "    if len(lst) > 1:\n",
    "        return (lst[0],float(lst[1]) if not log else math.log10(1+float(lst[1])))\n",
    "    else:\n",
    "        return (lst[0],1.0)\n",
    "\n",
    "def weightedlist2dict(lst,lower=False): # (key,weight) -> sum weigts by keys, keys may be lowercased\n",
    "    dic = {}\n",
    "    for item in lst:\n",
    "        dictcount(dic,item[0].lower() if lower else item[0],item[1])\n",
    "    return dic\n",
    "\n",
    "class LexiconIndexedTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, name=None, lexicon=None, cased=False, debug=False, url=None, sortmode=0):\n",
    "        Tokenizer.__init__(self,debug=debug)\n",
    "        self.name = name\n",
    "        if not lexicon is None: \n",
    "            self.freqlist = [(word,1.0) for word in lexicon] #copy\n",
    "        else:\n",
    "            lex_lines = url_lines(url)\n",
    "            self.freqlist = [tabbed_line2tuple(line) for line in lex_lines] #load from url\n",
    "            # TODO load from file\n",
    "        self.sortmode = sortmode\n",
    "        self.compile()\n",
    "        self.cased = cased\n",
    "\n",
    "    def compile(self):\n",
    "        self.dict = {}\n",
    "        self.fulldict = weightedlist2dict(self.freqlist,lower=True) # save for debugging only!?\n",
    "        for key in self.fulldict:\n",
    "            value = self.fulldict[key]\n",
    "            if len(key) > 0:\n",
    "                letter = key[0]\n",
    "                if not letter in self.dict:\n",
    "                    self.dict[letter] = set()\n",
    "                self.dict[letter].add((key,value))\n",
    "        #print(self.dict['f'])\n",
    "        for key in self.dict:\n",
    "            lst = list(self.dict[key])\n",
    "            if self.sortmode == 0:\n",
    "                lst.sort(key=lambda s: len(s[0]), reverse=True)\n",
    "            elif self.sortmode == 1:\n",
    "                lst.sort(key=lambda s: s[1], reverse=True)\n",
    "            else:\n",
    "                lst.sort(key=lambda s: math.log10(s[1])*len(s[0]), reverse=True)\n",
    "            self.dict[key] = lst\n",
    "        #print(self.dict['f'])\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        tokens, weight = tokenize_with_prexied_sorted_lexicon(self.dict,text,cased=self.cased)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_weight(self,text):\n",
    "        tokens, weight = tokenize_with_prexied_sorted_lexicon(self.dict,text,cased=self.cased)\n",
    "        length = len(tokens)\n",
    "        return tokens, 0 if length == 0 else weight / length \n",
    "\n",
    "assert str(LexiconIndexedTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"tunaisafish.catisamammal\"))==\"['tuna', 'is', 'a', 'fish', '.', 'cat', 'is', 'a', 'mammal']\"    \n",
    "assert str(LexiconIndexedTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.Cat', 'is', 'a', 'mammal']\"\n",
    "assert str(LexiconIndexedTokenizer(lexicon=['tuna','is','fish','cat','mammal'],cased=True).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.', 'Cat', 'is', 'a', 'mammal']\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107af82f",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d84240",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tunaisafish.catisamammal\"\n",
    "expected = ['tuna', 'is', 'a', 'fish', '.', 'cat', 'ia', 'a', 'mammal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da7dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'isa', 'fish', '.', 'cati', 'sama', 'mma', 'l']\n",
      "2.9807663087309058 0.35294117647058826\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1618f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'un', 'a', 'is', 'a', 'f', 'is', 'h', '.', 'c', 'a', 't', 'is', 'a', 'm', 'a', 'm', 'm', 'a', 'l']\n",
      "5.729628877142061 0.2758620689655173\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_en, sortmode=1)\n",
    "tokens, weight = lt1.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b186ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'is', 'af', 'is', 'h', '.', 'cat', 'is', 'ama', 'mm', 'al']\n",
      "4.544507159677375 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_en, sortmode=2)\n",
    "tokens, weight = lt2.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe43fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '.', ' ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tuna is a fish. Cat is a mammal\"\n",
    "expected = tokenize_split_with_delimiters_and_quotes(text)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97382da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '. ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n",
      "['Tuna', ' ', 'is', ' ', 'a', ' ', 'fish', '.', ' ', 'Cat', ' ', 'is', ' ', 'a', ' ', 'mammal']\n",
      "2.923531848929005 0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0, cased = True)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(expected)\n",
    "print(weight,calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6089d5d",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d71a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"расцветалияблониигруши,поплылитуманынадрекой\"\n",
    "expected = ['расцветали', 'яблони', 'игруши', ',', 'поплыли', 'туманы', 'над', 'рекой']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea704dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['расцвета', 'лия', 'бл', 'они', 'игру', 'ши', ',', 'поплыли', 'туманы', 'над', 'рекой']\n",
      "4.034137991553761 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38883ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ра', 'с', 'цвет', 'али', 'яблони', 'игр', 'уши', ',', 'по', 'плыли', 'ту', 'ма', 'ны', 'на', 'др', 'е', 'ко', 'й']\n",
      "5.526441728891956 0.15384615384615383\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_ru, sortmode=1)\n",
    "tokens, weight = lt1.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7b913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['расцвета', 'ли', 'яблони', 'игру', 'ши', ',', 'поплыли', 'туман', 'ы', 'над', 'рекой']\n",
      "4.027652477481268 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_ru, sortmode=2)\n",
    "tokens, weight = lt2.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(weight,calc_f1(expected,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a41886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.734799829588847"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.fulldict['туманы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25e9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.053785038134658"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.fulldict['авва']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "124d8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Расцветали', ' ', 'яблони', ' ', 'и', ' ', 'груши', ',', ' ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n"
     ]
    }
   ],
   "source": [
    "text = \"Расцветали яблони и груши, поплыли туманы над рекой\"\n",
    "expected = tokenize_split_with_delimiters_and_quotes(text)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6307e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Расцвета', 'ли', ' ', 'яблони', ' и ', 'груши', ', ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n",
      "['Расцветали', ' ', 'яблони', ' ', 'и', ' ', 'груши', ',', ' ', 'поплыли', ' ', 'туманы', ' ', 'над', ' ', 'рекой']\n",
      "2.667408660328346 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0, cased = True)\n",
    "tokens, weight = lt0.tokenize_weight(text)\n",
    "print(tokens)\n",
    "print(expected)\n",
    "print(weight,calc_f1(expected,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3daaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a9048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO compute score with account to number of letters in token AND / OR log of frequency?\n",
    "#TODO build alternative graphs and score them!?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f7b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
