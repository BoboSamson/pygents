{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f91131c",
   "metadata": {},
   "source": [
    "# Russian Tokenization Experiments - PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f3285961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * \n",
    "\n",
    "lex_en = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_english.txt\"\n",
    "lex_ru = \"https://raw.githubusercontent.com/aigents/aigents-java/master/lexicon_russian.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "16e41788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygents.text import tokenize_with_sorted_lexicon\n",
    "\n",
    "class LexiconTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, name=None, lexicon=None, cased=False, url=None, debug=False):\n",
    "        Tokenizer.__init__(self,debug=debug)\n",
    "        self.name = name\n",
    "        if not lexicon is None: \n",
    "            self.alex = list(lexicon) #copy\n",
    "        else:\n",
    "            lex_lines = url_lines(url)\n",
    "            self.alex = [re.split('\\t| |,|;|\\n|\\r',line)[0] for line in lex_lines] #load from url\n",
    "            # TODO load from file\n",
    "        self.compile()\n",
    "        self.cased = cased\n",
    "\n",
    "    def compile(self):\n",
    "        self.alex.sort(key=len,reverse=True) #precompile\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        return tokenize_with_sorted_lexicon(self.alex,text,cased=self.cased)\n",
    "\n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"tunaisafish.catisamammal\"))==\"['tuna', 'is', 'a', 'fish', '.', 'cat', 'is', 'a', 'mammal']\"    \n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.Cat', 'is', 'a', 'mammal']\"\n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal'],cased=True).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.', 'Cat', 'is', 'a', 'mammal']\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "516dfa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixed_match_from_list(lst,text):\n",
    "    for item in lst:\n",
    "        if text.startswith(item[0]):\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "def prefixed_match(prefixed_dict,text):\n",
    "    letter = text[0]\n",
    "    if not letter in prefixed_dict:\n",
    "        return None\n",
    "    return prefixed_match_from_list(prefixed_dict[letter],text)\n",
    "\n",
    "def tokenize_with_prexied_sorted_lexicon(prefixed_dict,text,cased=False):\n",
    "    original = text\n",
    "    if cased: #if need to spend time on lowercasing non-lowercased text\n",
    "        text = text.lower()\n",
    "    tokens = []\n",
    "    start = 0\n",
    "    cur = 0\n",
    "    length = len(text)\n",
    "    sum_weight = 0\n",
    "    while cur < length:\n",
    "        subtext = text[cur:]\n",
    "        word_weight = prefixed_match(prefixed_dict,subtext)\n",
    "        #print(al)\n",
    "        if not word_weight is None:\n",
    "            word_len = len(word_weight[0])\n",
    "            if start < cur:\n",
    "                tokens.append(original[start:cur])\n",
    "            tokens.append(original[cur:cur+word_len])\n",
    "            sum_weight += word_weight[1]\n",
    "            cur += word_len\n",
    "            start = cur\n",
    "        else:\n",
    "            cur += 1\n",
    "            #print('yo')\n",
    "    if start < cur:\n",
    "        tokens.append(original[start:cur])\n",
    "        #print(original[start:cur])\n",
    "    return tokens, sum_weight\n",
    "\n",
    "def tabbed_line2tuple(line):\n",
    "    lst = re.split('\\t| |,|;|\\n|\\r',line)\n",
    "    if len(lst) > 1:\n",
    "        return (lst[0],float(lst[1]))\n",
    "    else:\n",
    "        return (lst[0],1.0)\n",
    "\n",
    "class LexiconIndexedTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, name=None, lexicon=None, cased=False, debug=False, url=None, sortmode=0):\n",
    "        Tokenizer.__init__(self,debug=debug)\n",
    "        self.name = name\n",
    "        if not lexicon is None: \n",
    "            self.alex = [(word,1.0) for word in lexicon] #copy\n",
    "        else:\n",
    "            lex_lines = url_lines(url)\n",
    "            self.alex = [tabbed_line2tuple(line) for line in lex_lines] #load from url\n",
    "            # TODO load from file\n",
    "        self.sortmode = sortmode\n",
    "        self.compile()\n",
    "        self.cased = cased\n",
    "\n",
    "    def compile(self):\n",
    "        self.dict = {}\n",
    "        self.fulldict = dict(self.alex) # for debugging only!?\n",
    "        for entry in self.alex:\n",
    "            word = entry[0]\n",
    "            if len(word) > 0:\n",
    "                letter = word[0]\n",
    "                if not letter in self.dict:\n",
    "                    self.dict[letter] = set()\n",
    "                self.dict[letter].add(entry)\n",
    "        #print(self.dict['f'])\n",
    "        for key in self.dict:\n",
    "            lst = list(self.dict[key])\n",
    "            if self.sortmode == 0:\n",
    "                lst.sort(key=lambda s: len(s[0]), reverse=True)\n",
    "            elif self.sortmode == 1:\n",
    "                lst.sort(key=lambda s: s[1], reverse=True)\n",
    "            else:\n",
    "                lst.sort(key=lambda s: math.log10(s[1])*len(s[0]), reverse=True)\n",
    "            self.dict[key] = lst\n",
    "        #print(self.dict['f'])\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        tokens, weight = tokenize_with_prexied_sorted_lexicon(self.dict,text,cased=self.cased)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_weight(self,text):\n",
    "        return tokenize_with_prexied_sorted_lexicon(self.dict,text,cased=self.cased)\n",
    "\n",
    "assert str(LexiconIndexedTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"tunaisafish.catisamammal\"))==\"['tuna', 'is', 'a', 'fish', '.', 'cat', 'is', 'a', 'mammal']\"    \n",
    "assert str(LexiconIndexedTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.Cat', 'is', 'a', 'mammal']\"\n",
    "assert str(LexiconIndexedTokenizer(lexicon=['tuna','is','fish','cat','mammal'],cased=True).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.', 'Cat', 'is', 'a', 'mammal']\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31e43f",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "17f502e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['tuna', 'isa', 'fish', '.', 'cati', 'sama', 'mma', 'l'], 182006.0)\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_en, sortmode=0)\n",
    "print(lt0.tokenize_weight(\"tunaisafish.catisamammal\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ebed7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['t', 'un', 'a', 'is', 'a', 'f', 'is', 'h', '.', 'c', 'a', 't', 'is', 'a', 'm', 'a', 'm', 'm', 'a', 'l'], 118294882.0)\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_en, sortmode=1)\n",
    "print(lt1.tokenize_weight(\"tunaisafish.catisamammal\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5cac062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuna', 'is', 'af', 'is', 'h', '.', 'cat', 'is', 'am', 'am', 'mal']\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_en, sortmode=2)\n",
    "print(lt2.tokenize(\"tunaisafish.catisamammal\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b028a0",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ce6c7e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['расцвета', 'ли', 'яблони', 'игру', 'ши,', 'поплыли', 'туманы', 'над', 'рекой'], 350286.0)\n"
     ]
    }
   ],
   "source": [
    "lt0 = LexiconIndexedTokenizer(url = lex_ru, sortmode=0)\n",
    "print(lt0.tokenize_weight(\"расцветалияблониигруши,поплылитуманынадрекой\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d3343cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['расцвета', 'ли', 'яблони', 'игру', 'ши,', 'по', 'плыли', 'ту', 'ма', 'ны', 'на', 'др', 'е', 'ко', 'й'], 4034388.0)\n"
     ]
    }
   ],
   "source": [
    "lt1 = LexiconIndexedTokenizer(url = lex_ru, sortmode=1)\n",
    "print(lt1.tokenize_weight(\"расцветалияблониигруши,поплылитуманынадрекой\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ac46b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['расцвета', 'ли', 'яблони', 'игру', 'ши,', 'поплыли', 'туман', 'ы', 'над', 'рекой'], 354136.0)\n"
     ]
    }
   ],
   "source": [
    "lt2 = LexiconIndexedTokenizer(url = lex_ru, sortmode=2)\n",
    "print(lt2.tokenize_weight(\"расцветалияблониигруши,поплылитуманынадрекой\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3fa604a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ый', 292.0)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt2.dict['ы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd0207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO compute score dividing by number of tokens?\n",
    "#TODO compute score with account to number of letters in token AND / OR log of frequency?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
