{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924e0601",
   "metadata": {},
   "source": [
    "# Russian Tokenization Experiments - PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc9a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "from pygents.util import * \n",
    "from pygents.text import * \n",
    "from pygents.plot import * \n",
    "from pygents.token import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c598e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygents.text import tokenize_with_sorted_lexicon\n",
    "\n",
    "class LexiconTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, name=None, lexicon=None, cased=False, debug=False):\n",
    "        Tokenizer.__init__(self,debug=debug)\n",
    "        self.name = name\n",
    "        self.alex = list(lexicon) #precompile\n",
    "        self.alex.sort(key=len,reverse=True)\n",
    "        self.cased = cased\n",
    " \n",
    "    def tokenize(self,text):\n",
    "        return tokenize_with_sorted_lexicon(self.alex,text,cased=self.cased)\n",
    "\n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"tunaisafish.catisamammal\"))==\"['tuna', 'is', 'a', 'fish', '.', 'cat', 'is', 'a', 'mammal']\"    \n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal']).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.Cat', 'is', 'a', 'mammal']\"\n",
    "assert str(LexiconTokenizer(lexicon=['tuna','is','fish','cat','mammal'],cased=True).tokenize(\"Tunaisafish.Catisamammal\"))==\"['Tuna', 'is', 'a', 'fish', '.', 'Cat', 'is', 'a', 'mammal']\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3d6c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuna', 'is', 'a', 'fish', '.', 'cat', 'is', 'a', 'mammal']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_lexicon(['tuna','is','fish','cat','mammal'],\"tunaisafish.catisamammal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1a60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30db15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73587f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
