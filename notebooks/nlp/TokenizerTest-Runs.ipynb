{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad1c4f5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2655800/\n",
    "- https://lena-voita.github.io/nlp_course/language_modeling.html\n",
    "- https://en.wikipedia.org/wiki/Perplexity\n",
    "- https://github.com/singnet/language-learning/issues/255\n",
    "- https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7\n",
    "\n",
    "\n",
    "- https://github.com/natasha/razdel - razdel tries to mimic segmentation of these 4 datasets: SynTagRus, OpenCorpora, GICRYA and RNC. \n",
    "- https://www.kaggle.com/c/text-normalization-challenge-english-language\n",
    "- https://www.kaggle.com/c/text-normalization-challenge-russian-language\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb71b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd[:cwd.find('pygents')+7]\n",
    "if project_path not in sys.path: sys.path.append(project_path)\n",
    "os.chdir(project_path) \n",
    "\n",
    "#from importlib import reload  # Python 3.4+\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#force reimport\n",
    "if 'pygents.util' in sys.modules:\n",
    "    del sys.modules['pygents.util']\n",
    "if 'pygents.text' in sys.modules:\n",
    "    del sys.modules['pygents.text']\n",
    "if 'pygents.plot' in sys.modules:\n",
    "    del sys.modules['pygents.plot']\n",
    "if 'pygents.token' in sys.modules:\n",
    "    del sys.modules['pygents.token']\n",
    "\n",
    "\n",
    "from pygents.token import *\n",
    "from pygents.text import *\n",
    "from pygents.util import *\n",
    "from pygents.plot import plot_bars, plot_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d73c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10967135\n",
      "33960499\n"
     ]
    }
   ],
   "source": [
    "brown_chars = FreedomTokenizer(name='data/models/brown_nolines_chars_7a',max_n=7,mode='chars',debug=False)\n",
    "print(brown_chars.count_params())\n",
    "\n",
    "brown_grams = FreedomTokenizer(name='data/models/brown_nolines_grams_7a',max_n=7,mode='grams',debug=False)\n",
    "print(brown_grams.count_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e190e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19810\n"
     ]
    }
   ],
   "source": [
    "brown_text_lines = url_text_lines(\"http://www.sls.hawaii.edu/bley-vroman/brown_nolines.txt\")\n",
    "print(len(brown_text_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8a5e53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_texts = text_lines_sample(brown_text_lines,10,[\" \",\"#\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8834fa1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_forward_metric(model,text,forw,nlist,threshold=0.5,debug=False):\n",
    "    tokens = []\n",
    "    token = ''\n",
    "    df = profile_freedoms_avg_df(model,text,[forw],nlist)\n",
    "    length = len(df)\n",
    "    for i in range(length):\n",
    "        brk_forw = True if df.loc[i][forw] >= threshold else False\n",
    "        token += df.loc[i]['gram']\n",
    "        if debug:\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(df.loc[i]['gram'],'+' if brk_forw else '',round(df.loc[i][back],2),round(df.loc[i][forw],2),token))\n",
    "        if len(token) > 0 and brk_forw:\n",
    "            tokens.append(token)\n",
    "            token = ''\n",
    "    if len(token) > 0:\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def evaluate_tokenizer(model,texts,forw,back,nlist,threshold,spaces=False,debug=False):\n",
    "    f1_avg = 0\n",
    "    for text in texts:\n",
    "        tokens = tokenize_with_opposite_metrics(model,text,forw,back,nlist,threshold=threshold) if back is not None else tokenize_with_forward_metric(model,text,forw,nlist,threshold=threshold)\n",
    "        tokens_ref = tokenize_split_with_delimiters_and_quotes(text)\n",
    "        if not spaces:\n",
    "            remove_all(tokens,' ')\n",
    "            remove_all(tokens_ref,' ')\n",
    "        f1 = calc_f1(tokens_ref,tokens) \n",
    "        f1_avg += f1\n",
    "        if debug:\n",
    "            print(f1)\n",
    "            print(text)\n",
    "            print(calc_diff(tokens,tokens_ref))\n",
    "            print(str(tokens_ref))\n",
    "            print(str(tokens))\n",
    "            print()\n",
    "    print(\"{}\\t{}\\t{}\".format(nlist,threshold,round(f1_avg/len(texts),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e23f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\tthres.\tF1\n",
      "[1]\t0.2\t0.59\n",
      "[1]\t0.3\t0.62\n",
      "[1]\t0.4\t0.57\n",
      "[1]\t0.5\t0.56\n",
      "[1]\t0.6\t0.56\n",
      "[1]\t0.7\t0.78\n",
      "[1]\t0.8\t0.78\n",
      "[1, 2]\t0.2\t0.4\n",
      "[1, 2]\t0.3\t0.61\n",
      "[1, 2]\t0.4\t0.91\n",
      "[1, 2]\t0.5\t0.9\n",
      "[1, 2]\t0.6\t0.79\n",
      "[1, 2]\t0.7\t0.72\n",
      "[1, 2]\t0.8\t0.67\n",
      "[2, 3]\t0.2\t0.49\n",
      "[2, 3]\t0.3\t0.66\n",
      "[2, 3]\t0.4\t0.78\n",
      "[2, 3]\t0.5\t0.75\n",
      "[2, 3]\t0.6\t0.65\n",
      "[2, 3]\t0.7\t0.43\n",
      "[2, 3]\t0.8\t0.2\n",
      "[1, 2, 3]\t0.2\t0.56\n",
      "[1, 2, 3]\t0.3\t0.79\n",
      "[1, 2, 3]\t0.4\t0.84\n",
      "[1, 2, 3]\t0.5\t0.85\n",
      "[1, 2, 3]\t0.6\t0.7\n",
      "[1, 2, 3]\t0.7\t0.55\n",
      "[1, 2, 3]\t0.8\t0.32\n",
      "[1, 2, 3, 4]\t0.2\t0.69\n",
      "[1, 2, 3, 4]\t0.3\t0.82\n",
      "[1, 2, 3, 4]\t0.4\t0.78\n",
      "[1, 2, 3, 4]\t0.5\t0.7\n",
      "[1, 2, 3, 4]\t0.6\t0.51\n",
      "[1, 2, 3, 4]\t0.7\t0.25\n",
      "[1, 2, 3, 4]\t0.8\t0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"N\\tthres.\\tF1\")\n",
    "for nlist in [[1],[1,2],[2,3],[1,2,3],[1,2,3,4]]:\n",
    "    for threshold in [0.2,0.3,0.4,0.5,0.6,0.7,0.8]: \n",
    "        evaluate_tokenizer(brown_chars.model,test_texts,'ddf-','ddf+',nlist,threshold,spaces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f5bb46e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\tthres.\tF1\n",
      "[1]\t0.2\t0.59\n",
      "[1]\t0.3\t0.62\n",
      "[1]\t0.4\t0.57\n",
      "[1]\t0.5\t0.56\n",
      "[1]\t0.6\t0.56\n",
      "[1]\t0.7\t0.78\n",
      "[1]\t0.8\t0.78\n",
      "[1, 2]\t0.2\t0.38\n",
      "[1, 2]\t0.3\t0.46\n",
      "[1, 2]\t0.4\t0.46\n",
      "[1, 2]\t0.5\t0.53\n",
      "[1, 2]\t0.6\t0.48\n",
      "[1, 2]\t0.7\t0.28\n",
      "[1, 2]\t0.8\t0.05\n",
      "[2, 3]\t0.2\t0.53\n",
      "[2, 3]\t0.3\t0.38\n",
      "[2, 3]\t0.4\t0.24\n",
      "[2, 3]\t0.5\t0.11\n",
      "[2, 3]\t0.6\t0.05\n",
      "[2, 3]\t0.7\t0.03\n",
      "[2, 3]\t0.8\t0.02\n",
      "[1, 2, 3]\t0.2\t0.53\n",
      "[1, 2, 3]\t0.3\t0.39\n",
      "[1, 2, 3]\t0.4\t0.24\n",
      "[1, 2, 3]\t0.5\t0.11\n",
      "[1, 2, 3]\t0.6\t0.05\n",
      "[1, 2, 3]\t0.7\t0.03\n",
      "[1, 2, 3]\t0.8\t0.02\n",
      "[1, 2, 3, 4]\t0.2\t0.29\n",
      "[1, 2, 3, 4]\t0.3\t0.17\n",
      "[1, 2, 3, 4]\t0.4\t0.09\n",
      "[1, 2, 3, 4]\t0.5\t0.03\n",
      "[1, 2, 3, 4]\t0.6\t0.02\n",
      "[1, 2, 3, 4]\t0.7\t0.02\n",
      "[1, 2, 3, 4]\t0.8\t0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"N\\tthres.\\tF1\")\n",
    "for nlist in [[1],[1,2],[2,3],[1,2,3],[1,2,3,4]]:\n",
    "    for threshold in [0.2,0.3,0.4,0.5,0.6,0.7,0.8]: \n",
    "        evaluate_tokenizer(brown_grams.model,test_texts,'ddf-','ddf+',nlist,threshold,spaces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9bfba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\tthres.\tF1\n",
      "[1]\t0.2\t0.61\n",
      "[1]\t0.3\t0.57\n",
      "[1]\t0.4\t0.7\n",
      "[1]\t0.5\t0.37\n",
      "[1]\t0.6\t0.15\n",
      "[1]\t0.7\t0.07\n",
      "[1]\t0.8\t0.0\n",
      "[1, 2]\t0.2\t0.71\n",
      "[1, 2]\t0.3\t0.85\n",
      "[1, 2]\t0.4\t0.88\n",
      "[1, 2]\t0.5\t0.66\n",
      "[1, 2]\t0.6\t0.26\n",
      "[1, 2]\t0.7\t0.06\n",
      "[1, 2]\t0.8\t0.01\n",
      "[2, 3]\t0.2\t0.7\n",
      "[2, 3]\t0.3\t0.78\n",
      "[2, 3]\t0.4\t0.66\n",
      "[2, 3]\t0.5\t0.4\n",
      "[2, 3]\t0.6\t0.13\n",
      "[2, 3]\t0.7\t0.03\n",
      "[2, 3]\t0.8\t0.01\n",
      "[1, 2, 3]\t0.2\t0.75\n",
      "[1, 2, 3]\t0.3\t0.87\n",
      "[1, 2, 3]\t0.4\t0.75\n",
      "[1, 2, 3]\t0.5\t0.43\n",
      "[1, 2, 3]\t0.6\t0.16\n",
      "[1, 2, 3]\t0.7\t0.05\n",
      "[1, 2, 3]\t0.8\t0.0\n",
      "[1, 2, 3, 4]\t0.2\t0.78\n",
      "[1, 2, 3, 4]\t0.3\t0.82\n",
      "[1, 2, 3, 4]\t0.4\t0.68\n",
      "[1, 2, 3, 4]\t0.5\t0.42\n",
      "[1, 2, 3, 4]\t0.6\t0.16\n",
      "[1, 2, 3, 4]\t0.7\t0.04\n",
      "[1, 2, 3, 4]\t0.8\t0.01\n"
     ]
    }
   ],
   "source": [
    "# just use one parameter \n",
    "print(\"N\\tthres.\\tF1\")\n",
    "for nlist in [[1],[1,2],[2,3],[1,2,3],[1,2,3,4]]:\n",
    "    for threshold in [0.2,0.3,0.4,0.5,0.6,0.7,0.8]: \n",
    "        evaluate_tokenizer(brown_chars.model,test_texts,'ddf+|ddf-',None,nlist,threshold,spaces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84053a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57004394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d73423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda23d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481f458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f0e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7307bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765cc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cb453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
