{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "\n",
    "from util import dictcount\n",
    "from a_api import TextMetrics, build_ngrams, load_ngrams, tokenize_re, punct\n",
    "\n",
    "punct = punct + \"”“–&•\" \n",
    "\n",
    "model_family = 'multiclass_view_cleaned'\n",
    "if not os.path.exists('../../data/models/distortions/overfitting_combined/'+model_family+'/'):\n",
    "    os.makedirs('../../data/models/distortions/overfitting_combined/'+model_family+'/')\n",
    "\n",
    "grand_t0 = dt.datetime.now()\n",
    "\n",
    "def language_metrics(metrics_list):\n",
    "    metrics = {}\n",
    "    for m in metrics_list:\n",
    "        metrics[m] = '../../data/models/distortions/overfitting_combined/'+model_family+'/' + m + '.txt'\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient Question</th>\n",
       "      <th>Distorted part</th>\n",
       "      <th>Dominant Distortion</th>\n",
       "      <th>Secondary Distortion (Optional)l</th>\n",
       "      <th>Secondary Distortion (Optional)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm such a failure I never do anything right.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nobody likes me because I'm not interesting.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't try new things because I'll just mess...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My boss didn't say 'good morning' she must be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My friend didn't invite me to the party I mus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>I’m a 21 year old female. I spent most of my l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>I am 21 female and have not had any friends fo...</td>\n",
       "      <td>Now I am at university my peers around me all ...</td>\n",
       "      <td>Overgeneralization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>From the U.S.: My brother is 19 years old and ...</td>\n",
       "      <td>He claims he’s severely depressed and has outb...</td>\n",
       "      <td>Mental filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mind Reading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>From the U.S.: I am a 21 year old woman who ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>I recently moved out on my ex-roommate because...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Distortion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6057 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Patient Question  \\\n",
       "0         I'm such a failure I never do anything right.   \n",
       "1          Nobody likes me because I'm not interesting.   \n",
       "2      I can't try new things because I'll just mess...   \n",
       "3      My boss didn't say 'good morning' she must be...   \n",
       "4      My friend didn't invite me to the party I mus...   \n",
       "...                                                 ...   \n",
       "6052  I’m a 21 year old female. I spent most of my l...   \n",
       "6053  I am 21 female and have not had any friends fo...   \n",
       "6054  From the U.S.: My brother is 19 years old and ...   \n",
       "6055  From the U.S.: I am a 21 year old woman who ha...   \n",
       "6056  I recently moved out on my ex-roommate because...   \n",
       "\n",
       "                                         Distorted part Dominant Distortion  \\\n",
       "0                                                   NaN          Distortion   \n",
       "1                                                   NaN          Distortion   \n",
       "2                                                   NaN          Distortion   \n",
       "3                                                   NaN          Distortion   \n",
       "4                                                   NaN          Distortion   \n",
       "...                                                 ...                 ...   \n",
       "6052                                                NaN       No Distortion   \n",
       "6053  Now I am at university my peers around me all ...  Overgeneralization   \n",
       "6054  He claims he’s severely depressed and has outb...       Mental filter   \n",
       "6055                                                NaN       No Distortion   \n",
       "6056                                                NaN       No Distortion   \n",
       "\n",
       "      Secondary Distortion (Optional)l Secondary Distortion (Optional)  \n",
       "0                                  NaN                             NaN  \n",
       "1                                  NaN                             NaN  \n",
       "2                                  NaN                             NaN  \n",
       "3                                  NaN                             NaN  \n",
       "4                                  NaN                             NaN  \n",
       "...                                ...                             ...  \n",
       "6052                               NaN                             NaN  \n",
       "6053                               NaN                             NaN  \n",
       "6054                               NaN                    Mind Reading  \n",
       "6055                               NaN                             NaN  \n",
       "6056                               NaN                             NaN  \n",
       "\n",
       "[6057 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset_file_path = \"../../data/corpora/English/distortions/halilbabacan/raw_Cognitive_distortions.csv\" \n",
    "\n",
    "import kagglehub\n",
    "multiclass_dataset_path = kagglehub.dataset_download(\"sagarikashreevastava/cognitive-distortion-detetction-dataset\")\n",
    "multiclass_dataset_file_path = multiclass_dataset_path + \"/Annotated_data.csv\"\n",
    "\n",
    "df1 = pd.read_csv(binary_dataset_file_path)\n",
    "df1 = df1.rename(columns={'Text': 'Patient Question', 'Label': 'Dominant Distortion'})\n",
    "df1.insert(1, \"Distorted part\", value = np.nan)\n",
    "df1.insert(3, \"Secondary Distortion (Optional)l\", value = np.nan)\n",
    "\n",
    "df2 = pd.read_csv(multiclass_dataset_file_path) \n",
    "df2 = df2.drop('Id_Number', axis=1) # delete columnb with id \n",
    "\n",
    "df3 = pd.concat([df1, df2], ignore_index=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_normalized_ngrams(ngram_max, df, analytics_method):\n",
    "    distortions = defaultdict(int)\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    \n",
    "    uniq_n_gram_dicts = defaultdict(lambda: defaultdict(int)) # Counts of uniq N-grams by Distortion\n",
    "    uniq_all_n_grams = defaultdict(int)  # A general dictionary for all n-grams uniq by text\n",
    "    n_gram_distortions = defaultdict(lambda: defaultdict(int)) # Counts of distortiions by N-gram\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row.iloc[3] if pd.notna(row.iloc[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        dictcount(distortions,primary_distortion)\n",
    "        if secondary_distortion:\n",
    "            dictcount(distortions,secondary_distortion)\n",
    "        \n",
    "        # Text tokenization\n",
    "        tokens = [t for t in tokenize_re(text) if not (t in punct or t.isnumeric())]\n",
    "\n",
    "        # Generation and counting of n-grams (from 1 to 4)\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams) # Increment the counter for the corresponding secondary distortion (if present)\n",
    "\n",
    "            uniq_n_grams = set(n_grams)\n",
    "            for uniq_n_gram in uniq_n_grams:\n",
    "                dictcount(uniq_n_gram_dicts[primary_distortion], uniq_n_gram)\n",
    "                dictcount(uniq_all_n_grams, uniq_n_gram)\n",
    "                dictcount(n_gram_distortions[uniq_n_gram],primary_distortion)\n",
    "                if secondary_distortion:\n",
    "                    dictcount(uniq_n_gram_dicts[secondary_distortion], uniq_n_gram)\n",
    "                    dictcount(n_gram_distortions[uniq_n_gram],secondary_distortion)\n",
    "                \n",
    "    # Normalizing distortion-specific counts by total counts\n",
    "    norm_n_gram_dicts = {}\n",
    "    for n_gram_dict in n_gram_dicts:\n",
    "        norm_n_gram_dict = {}\n",
    "        norm_n_gram_dicts[n_gram_dict] = norm_n_gram_dict\n",
    "        dic = n_gram_dicts[n_gram_dict]\n",
    "        for n_gram in dic:\n",
    "            if len(n_gram) <= ngram_max:\n",
    "                norm_n_gram_dict[n_gram] = float( dic[n_gram] ) / all_n_grams[n_gram]\n",
    "\n",
    "    # Normalize uniq counts \n",
    "    norm_uniq_n_gram_dicts = {}\n",
    "    for uniq_n_gram_dict in uniq_n_gram_dicts:\n",
    "        norm_uniq_n_gram_dict = {}\n",
    "        norm_uniq_n_gram_dicts[uniq_n_gram_dict] = norm_uniq_n_gram_dict\n",
    "        dic = uniq_n_gram_dicts[uniq_n_gram_dict]\n",
    "        nonuniq_dic = n_gram_dicts[n_gram_dict]\n",
    "        # Normalize uniq Document counts of N-grams by distortion by Documents count by Distortion\n",
    "        for n_gram in dic:\n",
    "            if len(n_gram) <= ngram_max:\n",
    "                norm_uniq_n_gram_dict[n_gram] = float( dic[n_gram] ) / distortions[uniq_n_gram_dict] / len(n_gram_distortions[n_gram])\n",
    "\n",
    "    if analytics_method == 'normalize':\n",
    "        return norm_n_gram_dicts\n",
    "    else:\n",
    "        return norm_uniq_n_gram_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_frequency(ngram_max, df):\n",
    "    # Analyze the frequency of n-grams for each cognitive distortion\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0]\n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = [t for t in tokenize_re(text) if not (t in punct or t.isnumeric())]\n",
    "\n",
    "        # Generation and counting of n-grams (from 1 to 4)\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams) # Increment the counter for the corresponding secondary distortion (if present)\n",
    "\n",
    "    return n_gram_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TF_IDF(ngram_max, df):\n",
    "    # Analyze TF-IDF values for n-grams for each cognitive distortion\n",
    "\n",
    "    # Creating dictionaries for counting n-grams\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of documents in which each n-gram appears\n",
    "\n",
    "\n",
    "    # Loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0]\n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = [t for t in tokenize_re(text) if not (t in punct or t.isnumeric())]\n",
    "\n",
    "        # Generate n-grams and update the document counters where they appear\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)  # Increment the counter for the corresponding primary distortion\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams) # Increment the counter for the corresponding secondary distortion (if present)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # TF-IDF Calculation\n",
    "    tfidf_dicts = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), analyze the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            tf = count / sum(ngram_dict.values())  # Frequency of the n-gram in the text (TF): TF = (Number of occurrences of the given n-gram for the specific cognitive distortion) / (Total number of occurrences of all other n-grams for the same cognitive distortion)\n",
    "            idf = math.log(total_docs / (1 + doc_counts[n_gram]))  # Inverse Document Frequency (IDF): IDF = Total number of documents / Number of documents containing the given n-gram\n",
    "            tfidf_dicts[distortion][n_gram] = tf * idf  # TF-IDF\n",
    "\n",
    "    return tfidf_dicts\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TFTF_tf(ngram_max, df):\n",
    "    # Analyze TFTF_tf values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = [t for t in tokenize_re(text) if not (t in punct or t.isnumeric())]\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[primary_distortion][n_gram] += 1\n",
    "            if secondary_distortion:\n",
    "                unique_n_gram_dicts[secondary_distortion][n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # Calculation of the TFTF_tf\n",
    "    tftf_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # TFTF: Mutual relevance of features and text (how important a given n-gram is for describing the text, looking for specific n-grams within the text)\n",
    "            tf = count / sum(ngram_dict.values()) # Frequency of the n-gram in the text (TF): TF = (Number of times the given n-gram appears for the specific cognitive distortion) / (Number of times all other n-grams appear for the same cognitive distortion)\n",
    "            ft = doc_counts[n_gram] / total_docs # The number of texts in which the current n-gram appears / the total number of texts in the dataset\n",
    "            Ft = sum(ngram_dict.values())\n",
    "            tftf = (tf ** 2) / (ft*Ft if ft*Ft > 0 else 1)\n",
    "\n",
    "            tftf_results[distortion][n_gram] = tftf\n",
    "\n",
    "    return tftf_results\n",
    "\n",
    "\n",
    "\n",
    "def analyse_TCTC_tc(ngram_max, df):\n",
    "    # Analyze TCTC_tc values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = [t for t in tokenize_re(text) if not (t in punct or t.isnumeric())]\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[primary_distortion][n_gram] += 1\n",
    "            if secondary_distortion:\n",
    "                unique_n_gram_dicts[secondary_distortion][n_gram] += 1\n",
    "\n",
    "    # The total number of texts\n",
    "    total_docs = len(df)\n",
    "\n",
    "    # Calculation of the TCTC_tc\n",
    "    tctc_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # TCTC: Mutual relevance of categories and text (how the given n-gram is distributed across all texts of a given category)\n",
    "            ct = unique_n_gram_dicts[distortion][n_gram] # The number of texts in which the current n-gram (n_gram) is associated with the current distortion (distortion)\n",
    "            tctc = (ct ** 2) / (doc_counts[n_gram] * total_docs) # In the denominator: the number of texts containing the given n-gram, regardless of distortion * the total number of texts\n",
    "\n",
    "            tctc_results[distortion][n_gram] = tctc\n",
    "\n",
    "    return tctc_results\n",
    "\n",
    "\n",
    "\n",
    "def analyse_CFCF_cf(ngram_max, df):\n",
    "    # Analyze CFCF_cf values for n-grams for each cognitive distortion (see http://webstructor.net/papers/Kolonin-HP-ACA-IC-text.pdf)\n",
    "\n",
    "    # Dictionaries for metrics\n",
    "    n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-n-gram-n_gram_frequency)\n",
    "    all_n_grams = defaultdict(int)  # A general dictionary for all n-grams\n",
    "    unique_n_gram_dicts = defaultdict(lambda: defaultdict(int))  # A dictionary for each distortion (distortion-unique_n-gram-n_gram_frequency)\n",
    "\n",
    "    # Counting documents for n-grams\n",
    "    doc_counts = defaultdict(int)  # The number of texts in which each n-gram appears\n",
    "\n",
    "    # The main loop through the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Text identification: first, check the 2nd column; if NaN, take the text from the 1st column\n",
    "        text = row[1] if pd.notna(row[1]) else row[0] \n",
    "        primary_distortion = row[2]  # The primary cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row[3] if pd.notna(row[3]) else None  # The secondary distortion from the 4th column, if present\n",
    "\n",
    "        # Text tokenization\n",
    "        tokens = [t for t in tokenize_re(text) if not (t in punct or t.isnumeric())]\n",
    "\n",
    "        # Look at unique n-grams and increment the document counters\n",
    "        unique_ngrams = set()\n",
    "        for n in range(1, ngram_max + 1):\n",
    "            n_grams = build_ngrams(tokens, n)\n",
    "            unique_ngrams.update(n_grams)\n",
    "            dictcount(all_n_grams, n_grams)\n",
    "            dictcount(n_gram_dicts[primary_distortion], n_grams)\n",
    "            if secondary_distortion:\n",
    "                dictcount(n_gram_dicts[secondary_distortion], n_grams)\n",
    "\n",
    "        for n_gram in unique_ngrams:\n",
    "            doc_counts[n_gram] += 1\n",
    "            unique_n_gram_dicts[primary_distortion][n_gram] += 1\n",
    "            if secondary_distortion:\n",
    "                unique_n_gram_dicts[secondary_distortion][n_gram] += 1\n",
    "\n",
    "    # Calculation of the CFCF_cf\n",
    "    cfcf_results = defaultdict(dict)\n",
    "    for distortion, ngram_dict in n_gram_dicts.items(): # For each distortion (distortion), examine the n-grams (ngram_dict)\n",
    "        for n_gram, count in ngram_dict.items(): # For each n-gram (n_gram), check its frequency of occurrence (count) for the given distortion\n",
    "            # CFCF: Mutual relevance of features and categories (how characteristic the given n-gram is for a particular category)\n",
    "            cf = unique_n_gram_dicts[distortion][n_gram]\n",
    "            fc = sum(unique_n_gram_dicts[distortion].values()) # The total number of texts for all n-grams associated with the given distortion\n",
    "            cfcf = (cf ** 2) / (cf * fc if cf * fc > 0 else 1)\n",
    "\n",
    "            cfcf_results[distortion][n_gram] = cfcf\n",
    "\n",
    "    return cfcf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_inclusion(model_ngram, ngram_inclusion_threshold):\n",
    "    filtered_model_ngram = {}\n",
    "    for distortion, ngram_dict in model_ngram.items():\n",
    "        # Find the maximum metric value for the current distortion\n",
    "        max_value = max(ngram_dict.values()) if ngram_dict else 0\n",
    "        threshold_value = max_value * (ngram_inclusion_threshold / 100)\n",
    "\n",
    "        # Filter n-grams that meet or exceed the threshold value\n",
    "        filtered_model_ngram[distortion] = {\n",
    "            ngram: metric for ngram, metric in ngram_dict.items() if metric >= threshold_value\n",
    "        }\n",
    "    return filtered_model_ngram\n",
    "\n",
    "\n",
    "def create_model_files(filtered_model_ngram):\n",
    "    # Create .txt files for each distortion\n",
    "    output_dir = \"../../data/models/distortions/overfitting_combined/\"+model_family\n",
    "    distortion_file_path = f\"{output_dir}/All_distortions.txt\"\n",
    "    distortions_labels = []\n",
    "\n",
    "    with open(distortion_file_path, \"w\", encoding=\"utf-8\") as distortion_file:\n",
    "        for distortion, ngrams in filtered_model_ngram.items():\n",
    "            distortion_ = distortion.replace(\" \", \"_\")\n",
    "            file_path = f\"{output_dir}/{distortion_}.txt\"\n",
    "            sorted_ngrams = sorted(ngrams.items(), key=lambda x: x[1], reverse=True)\n",
    "            distortions_labels.append(distortion_)\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for ngram, metric_value in sorted_ngrams:\n",
    "                    ngram_str = ' '.join(ngram)\n",
    "                    f.write(f\"{ngram_str}\\t{metric_value}\\n\")\n",
    "                    if distortion != \"No Distortion\":\n",
    "                            distortion_file.write(f\"{ngram_str}\\t{metric_value}\\n\")\n",
    "    \n",
    "    old_distortion_file = f\"{output_dir}/Distortion.txt\"\n",
    "    os.remove(old_distortion_file)\n",
    "    os.rename(distortion_file_path, old_distortion_file)\n",
    "\n",
    "    return (list(set(distortions_labels) - {'No_Distortion'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_from_counts(true_positive, true_negative, false_positive, false_negative):\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    return 2 * precision * recall / (precision + recall) if precision > 0 or recall > 0 else 0 \n",
    "\n",
    "def evaluate_df_counts(df,evaluator,threshold, tm, debug=False):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    for _, row in df.iterrows():\n",
    "        # Text definition: first, check the 2nd column; if NaN, take the text from the 1st column.\n",
    "        text = row.iloc[1] if pd.notna(row.iloc[1]) else row.iloc[0]\n",
    "        primary_distortion = row.iloc[2]  # The main cognitive distortion from the 3rd column\n",
    "        secondary_distortion = row.iloc[3] if pd.notna(row.iloc[3]) else None  # The secondary distortion from the 4th column, if it exists\n",
    "        ground_distortion = False if primary_distortion == 'No Distortion' else True\n",
    "                       \n",
    "        our_distortion = evaluator(text,threshold, tm)\n",
    "        \n",
    "        # https://en.wikipedia.org/wiki/F-score\n",
    "        if ground_distortion == True and our_distortion == True:\n",
    "            true_positive += 1\n",
    "        if ground_distortion == False and our_distortion == True:\n",
    "            false_positive += 1\n",
    "        if ground_distortion == False and our_distortion == False:\n",
    "            true_negative += 1\n",
    "        if ground_distortion == True and our_distortion == False:\n",
    "            false_negative += 1\n",
    "\n",
    "        if debug:\n",
    "            print(ground_distortion,our_distortion,text[:20],metrics)\n",
    "\n",
    "    return true_positive, true_negative, false_positive, false_negative\n",
    "\n",
    "def evaluate_df(df,evaluator,threshold,tm, debug=False):\n",
    "    true_positive, true_negative, false_positive, false_negative = evaluate_df_counts(df,evaluator,threshold,tm,debug)\n",
    "    return f1_from_counts(true_positive, true_negative, false_positive, false_negative)\n",
    "\n",
    "def evaluate_df_acc_f1(df,evaluator,threshold,tm,debug=False):\n",
    "    true_positive, true_negative, false_positive, false_negative = evaluate_df_counts(df,evaluator,threshold,tm,debug)\n",
    "    return (true_positive + true_negative) / len(df), f1_from_counts(true_positive, true_negative, false_positive, false_negative) \n",
    "\n",
    "def our_evaluator_any(text,threshold, tm):\n",
    "    metrics = tm.get_sentiment_words(text)\n",
    "    for m in metrics:\n",
    "        if metrics[m] > threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def our_evaluator_avg(text,threshold, tm):\n",
    "    metrics = tm.get_sentiment_words(text)\n",
    "    l = list(metrics.values())\n",
    "    avg = sum(l) / len(l) if  len(l) > 0 else 0\n",
    "    if avg > threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_plot(row_labels, col_labels, matrix, absmax, title = None, vmin = None, vmax = None, dpi = None, titlefontsize = None, width = 20):\n",
    "    plt.rcParams[\"figure.figsize\"] = (width,len(row_labels)/4)\n",
    "    if not dpi is None:\n",
    "        plt.rcParams[\"figure.dpi\"] = dpi\n",
    "    p = sns.heatmap(matrix, xticklabels=col_labels, yticklabels=row_labels, \n",
    "                    vmin = -absmax if vmin is None else vmin, \n",
    "                    vmax = absmax if vmax is None else vmax, \n",
    "                    cmap='RdYlGn', annot=True)\n",
    "    if title is not None:\n",
    "        if titlefontsize is None:\n",
    "            titlefontsize = 32 if len(title) < 50 else round(32 * 50 / len(title))\n",
    "        p.set_title(title,fontsize = titlefontsize)\n",
    "    plt.show()\n",
    "\n",
    "def analyse_dataset(analytics_method, ngram_max, ngram_inclusion_threshold, df, print_or_plot):\n",
    "\n",
    "    if print_or_plot == 'print_results':\n",
    "        print('\\nAnalytics method:', analytics_method)\n",
    "        print('N-gram max length:', ngram_max)\n",
    "        print('N-gram inclusion threshold:', ngram_inclusion_threshold)\n",
    "\n",
    "    if analytics_method == 'frequency':\n",
    "        model_ngram = analyse_frequency(ngram_max, df)\n",
    "    elif analytics_method == 'TF-IDF':\n",
    "        model_ngram = analyse_TF_IDF(ngram_max, df)\n",
    "    elif analytics_method == 'TFTF_tf':\n",
    "        model_ngram = analyse_TFTF_tf(ngram_max, df)\n",
    "    elif analytics_method == 'TCTC_tc':\n",
    "        model_ngram = analyse_TCTC_tc(ngram_max, df)\n",
    "    elif analytics_method == 'CFCF_cf':\n",
    "        model_ngram = analyse_CFCF_cf(ngram_max, df)\n",
    "    elif analytics_method == 'normalize' or analytics_method == 'normalize_uniq':\n",
    "        model_ngram = analyse_normalized_ngrams(ngram_max, df, analytics_method)\n",
    "\n",
    "    # Filter out values below the threshold\n",
    "    filtered_model_ngram = ngrams_inclusion(model_ngram, ngram_inclusion_threshold)\n",
    "\n",
    "    # Create .txt files for each distortion\n",
    "    distortions_labels = create_model_files(filtered_model_ngram)\n",
    "    tm = TextMetrics(language_metrics(distortions_labels), encoding = \"utf-8\", debug=False)\n",
    "\n",
    "    if print_or_plot == 'print_results':\n",
    "        print('\\tAny distortion (threshold, accuracy, F1 score):')\n",
    "    any_res_acc = {}\n",
    "    any_res = {}\n",
    "    for threshold in [0.0,0.01,0.05,0.1,0.2,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "        acc, f1 = evaluate_df_acc_f1(df3,our_evaluator_any,threshold, tm)\n",
    "        any_res_acc[threshold] = acc\n",
    "        any_res[threshold] = f1\n",
    "        if print_or_plot == 'print_results':\n",
    "            print('\\t', threshold, acc, f1)\n",
    "\n",
    "    min_acc = min(any_res_acc.values())\n",
    "    max_acc = max(any_res_acc.values())\n",
    "    avg_acc = sum(any_res_acc.values()) / len(any_res_acc)\n",
    "\n",
    "    min_f1 = min(any_res.values())\n",
    "    max_f1 = max(any_res.values())\n",
    "    avg_f1 = sum(any_res.values()) / len(any_res)\n",
    "\n",
    "    if print_or_plot == 'print_results':\n",
    "        print(f\"\\n\\tAccuracy: min={min_acc}, max={max_acc}, avg={avg_acc}\")\n",
    "        print(f\"\\tF1 Score: min={min_f1}, max={max_f1}, avg={avg_f1}\")\n",
    "\n",
    "    avg_res_acc = {}\n",
    "    avg_res = {}\n",
    "    if print_or_plot == 'print_results':\n",
    "        print('\\n\\tAverage distortion (threshold, accuracy, F1 score):')\n",
    "    for threshold in [0.0,0.01,0.05,0.1,0.2,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "        acc, f1 = evaluate_df_acc_f1(df3,our_evaluator_avg,threshold, tm)\n",
    "        avg_res_acc[threshold] = acc\n",
    "        avg_res[threshold] = f1\n",
    "        if print_or_plot == 'print_results':\n",
    "            print('\\t', threshold, acc, f1)\n",
    "\n",
    "    min_acc = min(avg_res_acc.values())\n",
    "    max_acc = max(avg_res_acc.values())\n",
    "    avg_acc = sum(avg_res_acc.values()) / len(any_res_acc)\n",
    "\n",
    "    min_f1 = min(avg_res.values())\n",
    "    max_f1 = max(avg_res.values())\n",
    "    avg_f1 = sum(avg_res.values()) / len(any_res)\n",
    "\n",
    "    if print_or_plot == 'print_results':\n",
    "        print(f\"\\n\\tAccuracy: min={min_acc}, max={max_acc}, avg={avg_acc}\")\n",
    "        print(f\"\\tF1 Score: min={min_f1}, max={max_f1}, avg={avg_f1}\")\n",
    "\n",
    "    return any_res_acc, avg_res_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "N-gram inclusion threshold: 10\n",
      "\tAny distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6919266963843487 0.8179156908665105\n",
      "\t 0.4 0.6919266963843487 0.8179156908665105\n",
      "\t 0.5 0.6919266963843487 0.8179156908665105\n",
      "\t 0.6 0.6919266963843487 0.8179156908665105\n",
      "\t 0.7 0.6919266963843487 0.8179156908665105\n",
      "\t 0.8 0.6919266963843487 0.8179156908665105\n",
      "\t 0.9 0.7076110285619944 0.8252245139642752\n",
      "\n",
      "\tAccuracy: min=0.6919266963843487, max=0.7076110285619944, avg=0.6933525447641348\n",
      "\tF1 Score: min=0.8179156908665105, max=0.8252245139642752, avg=0.8185801293299438\n",
      "\n",
      "\tAverage distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6919266963843487 0.8179156908665105\n",
      "\t 0.01 0.6919266963843487 0.8179156908665105\n",
      "\t 0.05 0.6919266963843487 0.8179156908665105\n",
      "\t 0.1 0.6919266963843487 0.8179156908665105\n",
      "\t 0.2 0.6922568928512465 0.8180753464766739\n",
      "\t 0.4 0.8875681030212976 0.9204903677758319\n",
      "\t 0.5 0.7964338781575037 0.8312576980977144\n",
      "\t 0.6 0.6283638765065214 0.6345185906803053\n",
      "\t 0.7 0.4910021462770348 0.4190691539476164\n",
      "\t 0.8 0.3954102691101205 0.22546531302876482\n",
      "\t 0.9 0.3815420175004127 0.1933677863910422\n",
      "\n",
      "\tAccuracy: min=0.3815420175004127, max=0.8875681030212976, avg=0.6400258153601395\n",
      "\tF1 Score: min=0.1933677863910422, max=0.9204903677758319, avg=0.6649006381694538\n",
      "\n",
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "N-gram inclusion threshold: 20\n",
      "\tAny distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.6915964999174509 0.8176849502244778\n",
      "\t 0.4 0.6917615981508998 0.8177647632991704\n",
      "\t 0.5 0.6940729734191844 0.8187775061124695\n",
      "\t 0.6 0.7087667161961367 0.8255882934546174\n",
      "\t 0.7 0.8025425127951131 0.8725218503517373\n",
      "\t 0.8 0.8406802047218095 0.8756603530472876\n",
      "\t 0.9 0.5174178636288592 0.46494600036609923\n",
      "\n",
      "\tAccuracy: min=0.5174178636288592, max=0.8406802047218095, avg=0.7012022153181143\n",
      "\tF1 Score: min=0.46494600036609923, max=0.8756603530472876, avg=0.7966985016139791\n",
      "\n",
      "\tAverage distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6915964999174509 0.8176849502244778\n",
      "\t 0.01 0.6915964999174509 0.8176849502244778\n",
      "\t 0.05 0.6915964999174509 0.8176849502244778\n",
      "\t 0.1 0.6915964999174509 0.8176849502244778\n",
      "\t 0.2 0.7019976886247317 0.8227437886673867\n",
      "\t 0.4 0.8378735347531782 0.8911066755378133\n",
      "\t 0.5 0.8157503714710252 0.8602204408817635\n",
      "\t 0.6 0.6769027571404986 0.7188622324378681\n",
      "\t 0.7 0.5542347696879644 0.5478901540522438\n",
      "\t 0.8 0.5033845137857025 0.4462444771723122\n",
      "\t 0.9 0.4452699356116889 0.3312101910828025\n",
      "\n",
      "\tAccuracy: min=0.4452699356116889, max=0.8378735347531782, avg=0.6637999609767812\n",
      "\tF1 Score: min=0.3312101910828025, max=0.8911066755378133, avg=0.7171834327936456\n",
      "\n",
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "N-gram inclusion threshold: 30\n",
      "\tAny distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6772329536073964 0.8052206834711567\n",
      "\t 0.01 0.6772329536073964 0.8052206834711567\n",
      "\t 0.05 0.6772329536073964 0.8052206834711567\n",
      "\t 0.1 0.6772329536073964 0.8052206834711567\n",
      "\t 0.2 0.7008420009905894 0.8168587022437842\n",
      "\t 0.4 0.8420009905894007 0.8883183568677792\n",
      "\t 0.5 0.8198778273072478 0.8534586971121558\n",
      "\t 0.6 0.6351329040779263 0.6423948220064726\n",
      "\t 0.7 0.5012382367508668 0.43648573027420257\n",
      "\t 0.8 0.4556711243189698 0.35162241887905604\n",
      "\t 0.9 0.34257883440647185 0.095\n",
      "\n",
      "\tAccuracy: min=0.34257883440647185, max=0.8420009905894007, avg=0.6369339757155507\n",
      "\tF1 Score: min=0.095, max=0.8883183568677792, avg=0.6640928601152797\n",
      "\n",
      "\tAverage distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6772329536073964 0.8052206834711567\n",
      "\t 0.01 0.6772329536073964 0.8052206834711567\n",
      "\t 0.05 0.6772329536073964 0.8052206834711567\n",
      "\t 0.1 0.6772329536073964 0.8052206834711567\n",
      "\t 0.2 0.7378240052831435 0.8357807652533609\n",
      "\t 0.4 0.8254911672445104 0.8623877099336023\n",
      "\t 0.5 0.6701337295690936 0.6892690513219284\n",
      "\t 0.6 0.536239062242034 0.496324188631881\n",
      "\t 0.7 0.47036486709592207 0.3799768071124855\n",
      "\t 0.8 0.4432887568103021 0.3269461077844311\n",
      "\t 0.9 0.3392768697374938 0.08630136986301369\n",
      "\n",
      "\tAccuracy: min=0.3392768697374938, max=0.8254911672445104, avg=0.6119591156738259\n",
      "\tF1 Score: min=0.08630136986301369, max=0.8623877099336023, avg=0.6270789757986663\n",
      "\n",
      "Analytics method: normalize\n",
      "N-gram max length: 1\n",
      "N-gram inclusion threshold: 40\n",
      "\tAny distortion (threshold, accuracy, F1 score):\n",
      "\t 0.0 0.6645203896318309 0.7631149452086735\n",
      "\t 0.01 0.6645203896318309 0.7631149452086735\n",
      "\t 0.05 0.6666666666666666 0.7642732049036778\n",
      "\t 0.1 0.6844972758791481 0.7740333451578575\n",
      "\t 0.2 0.7700181608056794 0.8219353189313563\n",
      "\t 0.4 0.664355291398382 0.6804966210906805\n",
      "\t 0.5 0.5491167244510484 0.5167227039462042\n",
      "\t 0.6 0.4901766551097903 0.41669814884775214\n",
      "\t 0.7 0.45368994551758296 0.34772324068598465\n",
      "\t 0.8 0.3703153376258874 0.165061295971979\n",
      "\t 0.9 0.30873369654944693 0.001907032181168057\n",
      "\n",
      "\tAccuracy: min=0.30873369654944693, max=0.7700181608056794, avg=0.5715100484788449\n",
      "\tF1 Score: min=0.001907032181168057, max=0.8219353189313563, avg=0.546825527466728\n",
      "\n",
      "\tAverage distortion (threshold, accuracy, F1 score):\n"
     ]
    }
   ],
   "source": [
    "for analytics_method in ['normalize', 'normalize_uniq', 'TF-IDF', 'frequency', 'TFTF_tf', 'TCTC_tc', 'CFCF_cf']:\n",
    "    for ngram_max in range (1, 5):\n",
    "        for ngram_inclusion_threshold in [10, 20, 30, 40, 50]:\n",
    "            acc_data_any, acc_data_avg = analyse_dataset(analytics_method, ngram_max, ngram_inclusion_threshold, df3, 'print_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "acc_data_any, acc_data_avg = analyse_dataset('normalize', 4, 50, df3, 'print_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_methods = ['normalize', 'normalize_uniq', 'TF-IDF', 'frequency', 'TFTF_tf', 'TCTC_tc', 'CFCF_cf']\n",
    "thresholds = [0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "ngram_max_values = [1, 2, 3, 4]\n",
    "ngram_inclusion_thresholds = [100, 90, 80, 70, 60, 50, 40, 30, 20, 10]\n",
    "\n",
    "# 1) analytics_method - threshold\n",
    "acc_analytics_threshold_any = np.zeros((len(analytics_methods), len(thresholds)))\n",
    "acc_analytics_threshold_avg = np.zeros((len(analytics_methods), len(thresholds)))\n",
    "\n",
    "for i, analytics_method in enumerate(analytics_methods):\n",
    "    acc_data_any, acc_data_avg = analyse_dataset(analytics_method, 4, 50, df3, 'plot_results')\n",
    "    for j, threshold in enumerate(thresholds):\n",
    "        acc_analytics_threshold_any[i, j] = acc_data_any[threshold]\n",
    "        acc_analytics_threshold_avg[i, j] = acc_data_avg[threshold]\n",
    "\n",
    "matrix_plot(analytics_methods, thresholds, acc_analytics_threshold_any, 1.0, title = 'Аnalytics method - threshold: any distortion (accuracy). N-gram max len = 4. N-gram inclusion threshold (%) = 50', \n",
    "            vmin = 1.0-(1.0-0.8)*2, vmax = 1.0, titlefontsize = 10, dpi = 200, width = 10)\n",
    "\n",
    "matrix_plot(analytics_methods, thresholds, acc_analytics_threshold_avg, 1.0, title = 'Аnalytics method - threshold: average distortion (accuracy). N-gram max len = 4. N-gram inclusion threshold (%) = 50', \n",
    "            vmin = 1.0-(1.0-0.8)*2, vmax = 1.0, titlefontsize = 10, dpi = 200, width = 10)\n",
    "\n",
    "\n",
    "# 2) ngram_max - threshold\n",
    "acc_ngrammax_threshold_any = np.zeros((len(ngram_max_values), len(thresholds)))\n",
    "acc_ngrammax_threshold_avg = np.zeros((len(ngram_max_values), len(thresholds)))\n",
    "\n",
    "for i, ngram_max in enumerate(ngram_max_values):\n",
    "    acc_data_any, acc_data_avg = analyse_dataset('normalize', ngram_max, 50, df3, 'plot_results')\n",
    "    for j, threshold in enumerate(thresholds):\n",
    "        acc_ngrammax_threshold_any[i, j] = acc_data_any[threshold]\n",
    "        acc_ngrammax_threshold_avg[i, j] = acc_data_avg[threshold]\n",
    "\n",
    "matrix_plot(ngram_max_values, thresholds, acc_ngrammax_threshold_any, 1.0, title = 'N-gram max len - threshold: any distortion (accuracy). Аnalytics method = normalize. N-gram inclusion threshold (%) = 50', \n",
    "            vmin = 1.0-(1.0-0.8)*2, vmax = 1.0, titlefontsize = 10, dpi = 200, width = 10)\n",
    "\n",
    "matrix_plot(ngram_max_values, thresholds, acc_ngrammax_threshold_avg, 1.0, title = 'N-gram max len - threshold: average distortion (accuracy). Аnalytics method = normalize. N-gram inclusion threshold (%) = 50', \n",
    "            vmin = 1.0-(1.0-0.8)*2, vmax = 1.0, titlefontsize = 10, dpi = 200, width = 10)\n",
    "\n",
    "\n",
    "# 3) ngram_inclusion_threshold - threshold\n",
    "acc_ngram_inclusion_threshold_any = np.zeros((len(ngram_inclusion_thresholds), len(thresholds)))\n",
    "acc_ngram_inclusion_threshold_avg = np.zeros((len(ngram_inclusion_thresholds), len(thresholds)))\n",
    "\n",
    "for i, ngram_inclusion_threshold in enumerate(ngram_inclusion_thresholds):\n",
    "    acc_data_any, acc_data_avg = analyse_dataset('normalize', 4, ngram_inclusion_threshold, df3, 'plot_results')\n",
    "    for j, threshold in enumerate(thresholds):\n",
    "        acc_ngram_inclusion_threshold_any[i, j] = acc_data_any[threshold]\n",
    "        acc_ngram_inclusion_threshold_avg[i, j] = acc_data_avg[threshold]\n",
    "\n",
    "matrix_plot(ngram_inclusion_thresholds, thresholds, acc_ngram_inclusion_threshold_any, 1.0, title = 'N-gram inclusion threshold (%) - threshold: any distortion (accuracy). Аnalytics method = normalize. N-gram max len = 4', \n",
    "            vmin = 1.0-(1.0-0.8)*2, vmax = 1.0, titlefontsize = 10, dpi = 200, width = 10)\n",
    "\n",
    "matrix_plot(ngram_inclusion_thresholds, thresholds, acc_ngram_inclusion_threshold_avg, 1.0, title = 'N-gram inclusion threshold (%) - threshold: average distortion (accuracy). Аnalytics method = normalize. N-gram max len = 4', \n",
    "            vmin = 1.0-(1.0-0.8)*2, vmax = 1.0, titlefontsize = 10, dpi = 200, width = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_t1 = dt.datetime.now()\n",
    "grand_delta = grand_t1 - grand_t0\n",
    "str(grand_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
